{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Elysia","text":"<p>Elysia is an agentic platform designed to use tools in a decision tree. A decision agent decides which tools to use dynamically based on its environment and context. You can use custom tools or use the pre-built tools designed to retrieve your data in a Weaviate cluster.</p> <p>See the basic example to get started right away! Or if you want to make your own tools and customise Elysia, see how to easily add your own tools.</p>"},{"location":"#get-started","title":"Get Started","text":"<p>To use Elysia, you need to either set up your models and API keys in your <code>.env</code> file, or specify them in the config. See the setup page to get started.</p>"},{"location":"#mcp-integration","title":"MCP Integration","text":"<p>Elysia now supports MCP (Model Context Protocol) server integration! Connect external tool servers and dynamically load tools via configuration. See the MCP documentation to get started.</p> <p>Elysia can be used very simply: <pre><code>from elysia import tool, Tree\n\ntree = Tree()\n\n@tool(tree=tree)\nasync def add(x: int, y: int) -&gt; int:\n    return x + y\n\ntree(\"What is the sum of 9009 and 6006?\")\n</code></pre></p> <p>Elysia is pre-configured to be capable of connecting to and interacting with your Weaviate clusters! <pre><code>from elysia import Tree\ntree = Tree()\nresponse, objects = tree(\n    \"What are the 10 most expensive items in the Ecommerce collection?\",\n    collection_names = [\"Ecommerce\"]\n)\n</code></pre> This will use the built-in open source query tool or aggregate tool to interact with your Weaviate collections. To get started connecting to Weaviate, see the setting up page.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install elysia-ai\n</code></pre> <p>Then, either run the app via</p> <pre><code>elysia start\n</code></pre> <p>(Additionally you can use <code>--port XXXX</code> to run from a different port, default is 8000). Or import Elysia with python and use it directly there.</p>"},{"location":"#usage","title":"Usage","text":"<p>Elysia is free, open source, and available to anyone.</p> <p>Unlike other agent-based packages, Elysia is pre-configured to run a wide range of tools and has a lot of capabilities straight away. For example, you could just call Elysia on your Weaviate collections and it will immediately and dynamically search your data, using custom queries with filters or aggregations.</p> <p>Or you could customise Elysia to your liking, create your own custom tools and add them to the Elysia decision tree.</p> <p>To use Elysia to search your data, you need to either have a Weaviate cloud cluster or a locally running Weaviate instance (or you can define your own custom tool to search another data source!).</p> <p>Sign up to Weaviate Cloud! A 14 day sandbox cluster is free.</p> <p>For more information on signing up to Weaviate Cloud, click here. </p> <p>From your Weaviate cloud cluster, you can upload data via a CSV on the cloud console, or you can upload via the Weaviate APIs.</p>"},{"location":"#about","title":"About","text":"<p>Check out the Github Repositories for the backend and the frontend</p> <ul> <li> <p>elysia (backend)</p> </li> <li> <p>elysia-frontend (frontend)</p> </li> </ul> <p>Elysia was developed by Edward Schmuhl (frontend) and Danny Williams (backend). Check out our socials below:</p> <ul> <li> <p>Edward's Linkedin</p> </li> <li> <p>Danny's Linkedin</p> </li> </ul> <p>Documentation built with mkdocs.</p>"},{"location":"RAG-DEV/","title":"RAG Development Guide for Elysia","text":"<p>This document provides comprehensive guidance for developers looking to contribute to and customize Elysia's RAG (Retrieval-Augmented Generation) system. Elysia is built with modularity in mind, allowing for extensive customization of its core RAG components.</p>"},{"location":"RAG-DEV/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Architecture Overview</li> <li>Chunking and Indexing Mechanisms</li> <li>Retrieval System Customization</li> <li>Continuous Chat Memory</li> <li>Development Setup</li> <li>Testing Guidelines</li> <li>Contributing Guidelines</li> </ol>"},{"location":"RAG-DEV/#architecture-overview","title":"Architecture Overview","text":"<p>Elysia's RAG system consists of several key components:</p> <ul> <li>Chunking &amp; Indexing: Document preprocessing and chunk creation (<code>elysia/tools/retrieval/chunk.py</code>)</li> <li>Retrieval System: Vector search and hybrid retrieval (<code>elysia/tools/retrieval/query.py</code>)</li> <li>Memory Management: Conversation context and environment state (<code>elysia/tree/objects.py</code>)</li> <li>Preprocessing: Collection analysis and metadata generation (<code>elysia/preprocessing/collection.py</code>)</li> <li>Vector Database Integration: Primarily Weaviate with extensible architecture</li> </ul> <p>The system follows a tree-based decision making approach where agents communicate through structured environments and can utilize various tools for retrieval, aggregation, and generation tasks.</p>"},{"location":"RAG-DEV/#chunking-and-indexing-mechanisms","title":"Chunking and Indexing Mechanisms","text":""},{"location":"RAG-DEV/#overview","title":"Overview","text":"<p>The chunking system is responsible for breaking down large documents into smaller, manageable pieces that can be effectively vectorized and retrieved. Elysia supports multiple chunking strategies and creates parallel collections in Weaviate for optimized retrieval.</p>"},{"location":"RAG-DEV/#key-components","title":"Key Components","text":""},{"location":"RAG-DEV/#1-chunker-class-elysiatoolsretrievalchunkpy","title":"1. Chunker Class (<code>elysia/tools/retrieval/chunk.py</code>)","text":"<p>The <code>Chunker</code> class provides the core chunking functionality:</p> <pre><code>from elysia.tools.retrieval.chunk import Chunker\n\n# Initialize chunker with different strategies\nchunker = Chunker(\n    chunking_strategy=\"sentences\",  # or \"fixed\" for token-based\n    num_tokens=256,                 # for token-based chunking\n    num_sentences=5                 # for sentence-based chunking\n)\n\n# Chunk a document\ndocument = \"Your long document text here...\"\nchunks, spans = chunker.chunk(document)\n</code></pre>"},{"location":"RAG-DEV/#2-chunking-strategies","title":"2. Chunking Strategies","text":"<p>Sentence-based Chunking: <pre><code>def chunk_by_sentences(\n    self,\n    document: str,\n    num_sentences: int | None = None,\n    overlap_sentences: int = 1,\n) -&gt; tuple[list[str], list[tuple[int, int]]]:\n    \"\"\"\n    Chunks document by sentences using spaCy for sentence detection.\n    Returns chunks and their character span annotations.\n    \"\"\"\n</code></pre></p> <p>Token-based Chunking: <pre><code>def chunk_by_tokens(\n    self, \n    document: str, \n    num_tokens: int | None = None, \n    overlap_tokens: int = 32\n) -&gt; tuple[list[str], list[tuple[int, int]]]:\n    \"\"\"\n    Chunks document by tokens with configurable overlap.\n    Uses spaCy for tokenization.\n    \"\"\"\n</code></pre></p>"},{"location":"RAG-DEV/#customizing-chunking","title":"Customizing Chunking","text":""},{"location":"RAG-DEV/#adding-new-chunking-strategies","title":"Adding New Chunking Strategies","text":"<p>To add a new chunking strategy:</p> <ol> <li>Extend the Chunker class:</li> </ol> <pre><code>class CustomChunker(Chunker):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Add custom initialization\n\n    def chunk_by_paragraphs(\n        self, \n        document: str, \n        max_paragraph_size: int = 1000\n    ) -&gt; tuple[list[str], list[tuple[int, int]]]:\n        \"\"\"Custom paragraph-based chunking strategy.\"\"\"\n        paragraphs = document.split('\\n\\n')\n        chunks = []\n        spans = []\n        current_pos = 0\n\n        for paragraph in paragraphs:\n            if len(paragraph) &lt;= max_paragraph_size:\n                chunks.append(paragraph)\n                spans.append((current_pos, current_pos + len(paragraph)))\n            else:\n                # Split large paragraphs using existing token method\n                sub_chunks, sub_spans = self.chunk_by_tokens(\n                    paragraph, num_tokens=max_paragraph_size//4\n                )\n                for chunk, (start, end) in zip(sub_chunks, sub_spans):\n                    chunks.append(chunk)\n                    spans.append((current_pos + start, current_pos + end))\n\n            current_pos += len(paragraph) + 2  # Account for \\n\\n\n\n        return chunks, spans\n\n    def chunk(self, document: str) -&gt; tuple[list[str], list[tuple[int, int]]]:\n        if self.chunking_strategy == \"paragraphs\":\n            return self.chunk_by_paragraphs(document)\n        else:\n            return super().chunk(document)\n</code></pre> <ol> <li>Update the AsyncCollectionChunker:</li> </ol> <pre><code>class CustomAsyncCollectionChunker(AsyncCollectionChunker):\n    def __init__(self, collection_name: str, chunking_strategy: str = \"paragraphs\"):\n        super().__init__(collection_name)\n        self.chunker = CustomChunker(chunking_strategy, num_sentences=5)\n</code></pre>"},{"location":"RAG-DEV/#customizing-vectorization","title":"Customizing Vectorization","text":"<p>The chunking system automatically inherits vectorization settings from the parent collection. To customize vectorization for chunks:</p> <pre><code>async def get_custom_vectoriser(\n    self, content_field: str, client: WeaviateAsyncClient\n) -&gt; _VectorConfigCreate:\n    \"\"\"Custom vectorizer configuration for chunks.\"\"\"\n    return Configure.Vectors.text2vec_openai(\n        model=\"text-embedding-3-large\",\n        dimensions=3072,\n        source_properties=[content_field],\n        vector_index_config=Configure.VectorIndex.hnsw(\n            distance_metric=\"cosine\",\n            ef_construction=256,\n            max_connections=32,\n            quantizer=Configure.VectorIndex.Quantizer.pq(\n                training_limit=50000,\n                segments=256\n            )\n        )\n    )\n</code></pre>"},{"location":"RAG-DEV/#chunk-size-optimization","title":"Chunk Size Optimization","text":"<p>The system automatically determines when chunking is needed based on document size:</p> <pre><code>def _evaluate_needs_chunking(\n    self,\n    display_type: str,\n    query_type: str,\n    schema: dict,\n    threshold: int = 400,  # Customize this threshold\n) -&gt; bool:\n    content_field, content_len = self._evaluate_content_field(schema[\"fields\"])\n\n    return (\n        content_field is not None\n        and content_len &gt; threshold\n        and query_type != \"filter_only\"\n        and display_type == \"document\"\n    )\n</code></pre> <p>To customize the threshold or add conditions:</p> <pre><code>class CustomQuery(Query):\n    def _evaluate_needs_chunking(self, display_type, query_type, schema, threshold=600):\n        # Custom logic for determining chunking needs\n        content_field, content_len = self._evaluate_content_field(schema[\"fields\"])\n\n        # Custom conditions\n        if display_type == \"conversation\":\n            return content_len &gt; 200  # Lower threshold for conversations\n        elif schema.get(\"collection_type\") == \"technical_docs\":\n            return content_len &gt; 800  # Higher threshold for technical docs\n\n        return super()._evaluate_needs_chunking(display_type, query_type, schema, threshold)\n</code></pre>"},{"location":"RAG-DEV/#best-practices-for-chunking","title":"Best Practices for Chunking","text":"<ol> <li>Choose appropriate chunk sizes: </li> <li>For semantic search: 200-500 tokens</li> <li>For question answering: 100-300 tokens</li> <li> <p>For summarization: 500-1000 tokens</p> </li> <li> <p>Use overlap: Include 10-20% overlap between chunks to maintain context</p> </li> <li> <p>Preserve structure: Respect document boundaries (paragraphs, sections) when possible</p> </li> <li> <p>Test performance: Monitor retrieval quality with different chunking strategies</p> </li> </ol>"},{"location":"RAG-DEV/#retrieval-system-customization","title":"Retrieval System Customization","text":""},{"location":"RAG-DEV/#overview_1","title":"Overview","text":"<p>Elysia's retrieval system supports hybrid search combining semantic, keyword, and filtered search capabilities. The system is designed to work primarily with Weaviate but can be extended to support other vector databases.</p>"},{"location":"RAG-DEV/#core-retrieval-components","title":"Core Retrieval Components","text":""},{"location":"RAG-DEV/#1-query-tool-elysiatoolsretrievalquerypy","title":"1. Query Tool (<code>elysia/tools/retrieval/query.py</code>)","text":"<p>The main retrieval interface that handles: - Hybrid search (semantic + keyword) - Multiple collection querying - Dynamic query generation via LLMs - Result formatting and display</p>"},{"location":"RAG-DEV/#2-query-generation","title":"2. Query Generation","text":"<p>The system uses DSPy for dynamic query generation:</p> <pre><code>from elysia.tools.retrieval.prompt_templates import QueryCreatorPrompt\nfrom elysia.util.elysia_chain_of_thought import ElysiaChainOfThought\n\nquery_generator = ElysiaChainOfThought(\n    QueryCreatorPrompt,\n    tree_data=tree_data,\n    environment=True,\n    collection_schemas=True,\n    tasks_completed=True,\n    message_update=True,\n    collection_names=collection_names,\n)\n</code></pre>"},{"location":"RAG-DEV/#supporting-different-vector-databases","title":"Supporting Different Vector Databases","text":"<p>While Elysia is primarily built for Weaviate, you can extend it to support other vector databases:</p>"},{"location":"RAG-DEV/#1-create-a-database-adapter","title":"1. Create a Database Adapter","text":"<pre><code># elysia/util/adapters/base.py\nfrom abc import ABC, abstractmethod\nfrom typing import Any, Dict, List, Tuple\n\nclass VectorDatabaseAdapter(ABC):\n    \"\"\"Base adapter for vector database integration.\"\"\"\n\n    @abstractmethod\n    async def connect(self, **kwargs) -&gt; Any:\n        \"\"\"Establish connection to the database.\"\"\"\n        pass\n\n    @abstractmethod\n    async def create_collection(self, name: str, schema: Dict) -&gt; Any:\n        \"\"\"Create a new collection/index.\"\"\"\n        pass\n\n    @abstractmethod\n    async def insert_documents(self, collection: str, documents: List[Dict]) -&gt; None:\n        \"\"\"Insert documents into collection.\"\"\"\n        pass\n\n    @abstractmethod\n    async def search(\n        self, \n        collection: str, \n        query_vector: List[float] = None,\n        query_text: str = None,\n        filters: Dict = None,\n        limit: int = 10\n    ) -&gt; List[Dict]:\n        \"\"\"Perform search operation.\"\"\"\n        pass\n\n    @abstractmethod\n    async def hybrid_search(\n        self,\n        collection: str,\n        query_text: str,\n        alpha: float = 0.5,\n        limit: int = 10\n    ) -&gt; List[Dict]:\n        \"\"\"Perform hybrid search (semantic + keyword).\"\"\"\n        pass\n</code></pre>"},{"location":"RAG-DEV/#2-implement-specific-adapters","title":"2. Implement Specific Adapters","text":"<p>Pinecone Adapter Example:</p> <pre><code># elysia/util/adapters/pinecone_adapter.py\nimport pinecone\nfrom pinecone import Pinecone\nfrom .base import VectorDatabaseAdapter\n\nclass PineconeAdapter(VectorDatabaseAdapter):\n    def __init__(self, api_key: str, environment: str):\n        self.api_key = api_key\n        self.environment = environment\n        self.client = None\n\n    async def connect(self, **kwargs):\n        self.client = Pinecone(api_key=self.api_key)\n        return self.client\n\n    async def create_collection(self, name: str, schema: Dict):\n        return self.client.create_index(\n            name=name,\n            dimension=schema.get('dimension', 1536),\n            metric=schema.get('metric', 'cosine'),\n            spec=pinecone.ServerlessSpec(\n                cloud='aws',\n                region=self.environment\n            )\n        )\n\n    async def insert_documents(self, collection: str, documents: List[Dict]):\n        index = self.client.Index(collection)\n        vectors = [\n            (doc['id'], doc['vector'], doc.get('metadata', {}))\n            for doc in documents\n        ]\n        index.upsert(vectors=vectors)\n\n    async def search(self, collection: str, query_vector: List[float] = None, \n                    query_text: str = None, filters: Dict = None, limit: int = 10):\n        index = self.client.Index(collection)\n\n        if query_vector:\n            results = index.query(\n                vector=query_vector,\n                filter=filters,\n                top_k=limit,\n                include_metadata=True\n            )\n            return [\n                {\n                    'id': match.id,\n                    'score': match.score,\n                    'metadata': match.metadata\n                }\n                for match in results.matches\n            ]\n        else:\n            raise NotImplementedError(\"Text-only search requires embedding generation\")\n\n    async def hybrid_search(self, collection: str, query_text: str, \n                           alpha: float = 0.5, limit: int = 10):\n        # Implement hybrid search logic\n        # This would require combining dense and sparse vectors\n        raise NotImplementedError(\"Hybrid search not yet implemented for Pinecone\")\n</code></pre> <p>Qdrant Adapter Example:</p> <pre><code># elysia/util/adapters/qdrant_adapter.py\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PointStruct\nfrom .base import VectorDatabaseAdapter\n\nclass QdrantAdapter(VectorDatabaseAdapter):\n    def __init__(self, host: str = \"localhost\", port: int = 6333):\n        self.host = host\n        self.port = port\n        self.client = None\n\n    async def connect(self, **kwargs):\n        self.client = QdrantClient(host=self.host, port=self.port)\n        return self.client\n\n    async def create_collection(self, name: str, schema: Dict):\n        return self.client.create_collection(\n            collection_name=name,\n            vectors_config=VectorParams(\n                size=schema.get('dimension', 1536),\n                distance=Distance.COSINE\n            )\n        )\n\n    async def insert_documents(self, collection: str, documents: List[Dict]):\n        points = [\n            PointStruct(\n                id=doc['id'],\n                vector=doc['vector'],\n                payload=doc.get('metadata', {})\n            )\n            for doc in documents\n        ]\n        self.client.upsert(collection_name=collection, points=points)\n\n    async def search(self, collection: str, query_vector: List[float] = None,\n                    query_text: str = None, filters: Dict = None, limit: int = 10):\n        results = self.client.search(\n            collection_name=collection,\n            query_vector=query_vector,\n            query_filter=filters,\n            limit=limit,\n            with_payload=True\n        )\n\n        return [\n            {\n                'id': result.id,\n                'score': result.score,\n                'metadata': result.payload\n            }\n            for result in results\n        ]\n\n    async def hybrid_search(self, collection: str, query_text: str,\n                           alpha: float = 0.5, limit: int = 10):\n        # Implement using Qdrant's hybrid search capabilities\n        # This would use both dense and sparse vectors\n        raise NotImplementedError(\"Hybrid search implementation needed\")\n</code></pre>"},{"location":"RAG-DEV/#3-integrate-adapters-with-elysia","title":"3. Integrate Adapters with Elysia","text":"<p>Create a database manager that handles different adapters:</p> <pre><code># elysia/util/database_manager.py\nfrom typing import Optional, Union\nfrom .adapters.base import VectorDatabaseAdapter\nfrom .adapters.pinecone_adapter import PineconeAdapter\nfrom .adapters.qdrant_adapter import QdrantAdapter\nfrom .client import ClientManager  # Existing Weaviate client\n\nclass DatabaseManager:\n    def __init__(self, db_type: str = \"weaviate\", **kwargs):\n        self.db_type = db_type\n        self.adapter: Optional[VectorDatabaseAdapter] = None\n        self.weaviate_client: Optional[ClientManager] = None\n\n        if db_type == \"weaviate\":\n            self.weaviate_client = ClientManager(**kwargs)\n        elif db_type == \"pinecone\":\n            self.adapter = PineconeAdapter(**kwargs)\n        elif db_type == \"qdrant\":\n            self.adapter = QdrantAdapter(**kwargs)\n        else:\n            raise ValueError(f\"Unsupported database type: {db_type}\")\n\n    async def connect(self):\n        if self.adapter:\n            return await self.adapter.connect()\n        elif self.weaviate_client:\n            return self.weaviate_client.connect_to_async_client()\n\n    async def search(self, **kwargs):\n        if self.adapter:\n            return await self.adapter.search(**kwargs)\n        elif self.weaviate_client:\n            # Use existing Weaviate search logic\n            return await self._weaviate_search(**kwargs)\n\n    async def _weaviate_search(self, **kwargs):\n        # Implement Weaviate search using existing logic\n        pass\n</code></pre>"},{"location":"RAG-DEV/#4-custom-query-execution","title":"4. Custom Query Execution","text":"<p>Extend the query execution to support different databases:</p> <pre><code># elysia/tools/retrieval/multi_db_query.py\nfrom elysia.tools.retrieval.query import Query\nfrom elysia.util.database_manager import DatabaseManager\n\nclass MultiDBQuery(Query):\n    def __init__(self, db_type: str = \"weaviate\", **kwargs):\n        super().__init__(**kwargs)\n        self.db_manager = DatabaseManager(db_type=db_type, **kwargs)\n\n    async def execute_search(self, query_output, **kwargs):\n        \"\"\"Execute search across different database types.\"\"\"\n        if self.db_manager.db_type == \"weaviate\":\n            # Use existing Weaviate logic\n            return await super().execute_weaviate_query(query_output, **kwargs)\n        else:\n            # Use adapter for other databases\n            return await self._execute_adapter_search(query_output, **kwargs)\n\n    async def _execute_adapter_search(self, query_output, **kwargs):\n        await self.db_manager.connect()\n\n        results = []\n        for collection in query_output.target_collections:\n            collection_results = await self.db_manager.search(\n                collection=collection,\n                query_text=query_output.search_query,\n                limit=query_output.limit,\n                # Convert filters to database-specific format\n                filters=self._convert_filters(query_output.filters)\n            )\n            results.extend(collection_results)\n\n        return results\n\n    def _convert_filters(self, weaviate_filters):\n        \"\"\"Convert Weaviate filters to database-specific format.\"\"\"\n        # Implement conversion logic based on self.db_manager.db_type\n        pass\n</code></pre>"},{"location":"RAG-DEV/#advanced-retrieval-techniques","title":"Advanced Retrieval Techniques","text":""},{"location":"RAG-DEV/#1-custom-similarity-functions","title":"1. Custom Similarity Functions","text":"<pre><code>def custom_similarity_search(\n    query_embedding: List[float],\n    document_embeddings: List[List[float]],\n    metadata: List[Dict],\n    top_k: int = 10\n) -&gt; List[Tuple[float, Dict]]:\n    \"\"\"Custom similarity computation with business logic.\"\"\"\n\n    similarities = []\n    for i, doc_embedding in enumerate(document_embeddings):\n        # Custom similarity computation\n        base_similarity = cosine_similarity(query_embedding, doc_embedding)\n\n        # Apply business logic modifications\n        doc_meta = metadata[i]\n\n        # Boost recent documents\n        recency_boost = calculate_recency_boost(doc_meta.get('timestamp'))\n\n        # Boost by document type\n        type_boost = get_type_boost(doc_meta.get('document_type'))\n\n        # User preference boost\n        user_boost = get_user_preference_boost(doc_meta, user_preferences)\n\n        final_score = base_similarity * recency_boost * type_boost * user_boost\n        similarities.append((final_score, doc_meta))\n\n    return sorted(similarities, key=lambda x: x[0], reverse=True)[:top_k]\n</code></pre>"},{"location":"RAG-DEV/#2-multi-vector-search","title":"2. Multi-Vector Search","text":"<pre><code>class MultiVectorRetriever:\n    \"\"\"Retrieve using multiple vector representations.\"\"\"\n\n    def __init__(self, vector_configs: List[Dict]):\n        self.vector_configs = vector_configs\n\n    async def multi_vector_search(\n        self,\n        query: str,\n        collection: str,\n        weights: List[float] = None\n    ) -&gt; List[Dict]:\n        \"\"\"Search using multiple vector representations and combine results.\"\"\"\n\n        if weights is None:\n            weights = [1.0 / len(self.vector_configs)] * len(self.vector_configs)\n\n        all_results = []\n\n        for i, config in enumerate(self.vector_configs):\n            # Generate query vector for this configuration\n            query_vector = await self._generate_vector(query, config)\n\n            # Search with this vector\n            results = await self._vector_search(\n                collection=collection,\n                query_vector=query_vector,\n                vector_name=config['name']\n            )\n\n            # Weight the results\n            for result in results:\n                result['weighted_score'] = result['score'] * weights[i]\n                result['vector_type'] = config['name']\n\n            all_results.extend(results)\n\n        # Combine and re-rank results\n        return self._combine_results(all_results)\n\n    def _combine_results(self, results: List[Dict]) -&gt; List[Dict]:\n        \"\"\"Combine results from multiple vectors using RRF or similar.\"\"\"\n        # Group by document ID\n        doc_groups = {}\n        for result in results:\n            doc_id = result['id']\n            if doc_id not in doc_groups:\n                doc_groups[doc_id] = []\n            doc_groups[doc_id].append(result)\n\n        # Apply Reciprocal Rank Fusion (RRF)\n        combined_results = []\n        for doc_id, doc_results in doc_groups.items():\n            rrf_score = sum(1 / (60 + rank) for rank, _ in enumerate(doc_results))\n            combined_results.append({\n                'id': doc_id,\n                'combined_score': rrf_score,\n                'individual_scores': doc_results,\n                'metadata': doc_results[0]['metadata']  # Assume same metadata\n            })\n\n        return sorted(combined_results, key=lambda x: x['combined_score'], reverse=True)\n</code></pre>"},{"location":"RAG-DEV/#3-contextual-retrieval","title":"3. Contextual Retrieval","text":"<pre><code>class ContextualRetriever:\n    \"\"\"Retriever that considers conversation context.\"\"\"\n\n    def __init__(self, base_retriever):\n        self.base_retriever = base_retriever\n\n    async def contextual_search(\n        self,\n        query: str,\n        conversation_history: List[Dict],\n        collection: str,\n        context_weight: float = 0.3\n    ) -&gt; List[Dict]:\n        \"\"\"Search considering conversation context.\"\"\"\n\n        # Extract context from conversation history\n        context = self._extract_context(conversation_history)\n\n        # Generate context-aware query\n        enhanced_query = await self._enhance_query_with_context(query, context)\n\n        # Perform standard search\n        results = await self.base_retriever.search(\n            query=enhanced_query,\n            collection=collection\n        )\n\n        # Re-rank results based on context relevance\n        return self._rerank_with_context(results, context, context_weight)\n\n    def _extract_context(self, conversation_history: List[Dict]) -&gt; Dict:\n        \"\"\"Extract relevant context from conversation history.\"\"\"\n        context = {\n            'entities': set(),\n            'topics': set(),\n            'previous_queries': [],\n            'user_preferences': {}\n        }\n\n        for turn in conversation_history[-5:]:  # Last 5 turns\n            # Extract entities\n            entities = extract_named_entities(turn.get('content', ''))\n            context['entities'].update(entities)\n\n            # Extract topics\n            topics = extract_topics(turn.get('content', ''))\n            context['topics'].update(topics)\n\n            # Store previous queries\n            if turn.get('type') == 'query':\n                context['previous_queries'].append(turn['content'])\n\n        return context\n\n    async def _enhance_query_with_context(self, query: str, context: Dict) -&gt; str:\n        \"\"\"Enhance query with context information.\"\"\"\n        # Use LLM to enhance query with context\n        enhancement_prompt = f\"\"\"\n        Original query: {query}\n\n        Conversation context:\n        - Recent entities mentioned: {', '.join(list(context['entities'])[:10])}\n        - Recent topics: {', '.join(list(context['topics'])[:5])}\n        - Previous queries: {', '.join(context['previous_queries'][-3:])}\n\n        Enhanced query (maintain original intent but add relevant context):\n        \"\"\"\n\n        # Use your LLM to enhance the query\n        # enhanced_query = await self.llm.generate(enhancement_prompt)\n        # For now, simple concatenation\n        enhanced_query = query\n        if context['entities']:\n            enhanced_query += f\" (related to: {', '.join(list(context['entities'])[:3])})\"\n\n        return enhanced_query\n</code></pre>"},{"location":"RAG-DEV/#continuous-chat-memory","title":"Continuous Chat Memory","text":""},{"location":"RAG-DEV/#overview_2","title":"Overview","text":"<p>Elysia's memory system manages conversation context and maintains state across interactions through the <code>Environment</code> class. This system enables continuity in conversations and allows agents to build upon previous interactions.</p>"},{"location":"RAG-DEV/#core-memory-components","title":"Core Memory Components","text":""},{"location":"RAG-DEV/#1-environment-class-elysiatreeobjectspy","title":"1. Environment Class (<code>elysia/tree/objects.py</code>)","text":"<p>The Environment class provides structured storage for all interaction data:</p> <pre><code>from elysia.tree.objects import Environment\n\n# Initialize environment\nenv = Environment()\n\n# Add retrieval results  \nenv.add_objects(\"query\", \"collection_name\", objects=[...], metadata={...})\n\n# Find previous results\nprevious_results = env.find(\"query\", \"collection_name\")\n</code></pre>"},{"location":"RAG-DEV/#2-memory-structure","title":"2. Memory Structure","text":"<p>The environment organizes data hierarchically:</p> <pre><code>{\n    \"tool_name\": {           # e.g., \"query\", \"aggregate\", \"summarize\"\n        \"result_name\": [     # e.g., collection name, task identifier\n            {\n                \"metadata\": {...},    # Query parameters, timestamps, etc.\n                \"objects\": [...]      # Retrieved/processed objects\n            }\n        ]\n    },\n    \"hidden_environment\": {...}  # Internal state not shown to LLM\n}\n</code></pre>"},{"location":"RAG-DEV/#customizing-memory-behavior","title":"Customizing Memory Behavior","text":""},{"location":"RAG-DEV/#1-persistent-memory-storage","title":"1. Persistent Memory Storage","text":"<p>Extend the Environment class to support persistent storage:</p> <pre><code># elysia/tree/persistent_environment.py\nimport json\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, Any, Optional\nfrom .objects import Environment\n\nclass PersistentEnvironment(Environment):\n    \"\"\"Environment with persistent storage capabilities.\"\"\"\n\n    def __init__(self, user_id: str, conversation_id: str, storage_backend: str = \"file\", **kwargs):\n        super().__init__(**kwargs)\n        self.user_id = user_id\n        self.conversation_id = conversation_id\n        self.storage_backend = storage_backend\n        self._storage = self._init_storage()\n\n    def _init_storage(self):\n        if self.storage_backend == \"file\":\n            return FileStorage(self.user_id, self.conversation_id)\n        elif self.storage_backend == \"redis\":\n            return RedisStorage(self.user_id, self.conversation_id)\n        elif self.storage_backend == \"weaviate\":\n            return WeaviateStorage(self.user_id, self.conversation_id)\n        else:\n            raise ValueError(f\"Unsupported storage backend: {self.storage_backend}\")\n\n    async def save_state(self):\n        \"\"\"Save current environment state to persistent storage.\"\"\"\n        state = {\n            'environment': self.environment,\n            'hidden_environment': self.hidden_environment,\n            'timestamp': datetime.utcnow().isoformat(),\n            'user_id': self.user_id,\n            'conversation_id': self.conversation_id\n        }\n        await self._storage.save(state)\n\n    async def load_state(self):\n        \"\"\"Load environment state from persistent storage.\"\"\"\n        state = await self._storage.load()\n        if state:\n            self.environment = state.get('environment', {})\n            self.hidden_environment = state.get('hidden_environment', {})\n            return True\n        return False\n\n    async def add_objects(self, tool_name: str, result_name: str, objects: list, metadata: dict):\n        \"\"\"Override to auto-save after adding objects.\"\"\"\n        super().add_objects(tool_name, result_name, objects, metadata)\n        await self.save_state()\n\nclass FileStorage:\n    \"\"\"File-based storage for conversation state.\"\"\"\n\n    def __init__(self, user_id: str, conversation_id: str, base_path: str = \"./conversations\"):\n        self.user_id = user_id\n        self.conversation_id = conversation_id\n        self.file_path = f\"{base_path}/{user_id}/{conversation_id}.json\"\n        os.makedirs(os.path.dirname(self.file_path), exist_ok=True)\n\n    async def save(self, state: Dict[str, Any]):\n        with open(self.file_path, 'w') as f:\n            json.dump(state, f, indent=2, default=str)\n\n    async def load(self) -&gt; Optional[Dict[str, Any]]:\n        try:\n            with open(self.file_path, 'r') as f:\n                return json.load(f)\n        except FileNotFoundError:\n            return None\n\nclass WeaviateStorage:\n    \"\"\"Weaviate-based storage for conversation state.\"\"\"\n\n    def __init__(self, user_id: str, conversation_id: str):\n        self.user_id = user_id\n        self.conversation_id = conversation_id\n        self.collection_name = \"ELYSIA_CONVERSATIONS\"\n\n    async def save(self, state: Dict[str, Any]):\n        # Implement Weaviate storage\n        from elysia.util.client import ClientManager\n\n        client_manager = ClientManager()\n        async with client_manager.connect_to_async_client() as client:\n            if not await client.collections.exists(self.collection_name):\n                await self._create_conversation_collection(client)\n\n            collection = client.collections.get(self.collection_name)\n\n            # Check if conversation exists\n            existing = await collection.query.fetch_objects(\n                filters=Filter.by_property(\"conversation_id\").equal(self.conversation_id),\n                limit=1\n            )\n\n            if existing.objects:\n                # Update existing conversation\n                await collection.data.update(\n                    uuid=existing.objects[0].uuid,\n                    properties={\"state\": json.dumps(state)}\n                )\n            else:\n                # Create new conversation\n                await collection.data.insert({\n                    \"user_id\": self.user_id,\n                    \"conversation_id\": self.conversation_id,\n                    \"state\": json.dumps(state),\n                    \"created_at\": datetime.utcnow().isoformat(),\n                    \"updated_at\": datetime.utcnow().isoformat()\n                })\n\n    async def load(self) -&gt; Optional[Dict[str, Any]]:\n        from elysia.util.client import ClientManager\n\n        client_manager = ClientManager()\n        async with client_manager.connect_to_async_client() as client:\n            if not await client.collections.exists(self.collection_name):\n                return None\n\n            collection = client.collections.get(self.collection_name)\n            results = await collection.query.fetch_objects(\n                filters=Filter.by_property(\"conversation_id\").equal(self.conversation_id),\n                limit=1\n            )\n\n            if results.objects:\n                state_str = results.objects[0].properties.get(\"state\")\n                if state_str:\n                    return json.loads(state_str)\n\n            return None\n\n    async def _create_conversation_collection(self, client):\n        \"\"\"Create the conversation storage collection.\"\"\"\n        await client.collections.create(\n            self.collection_name,\n            properties=[\n                Property(name=\"user_id\", data_type=DataType.TEXT),\n                Property(name=\"conversation_id\", data_type=DataType.TEXT),\n                Property(name=\"state\", data_type=DataType.TEXT),\n                Property(name=\"created_at\", data_type=DataType.DATE),\n                Property(name=\"updated_at\", data_type=DataType.DATE),\n            ],\n            vectorizer_config=Configure.Vectorizer.none()\n        )\n</code></pre>"},{"location":"RAG-DEV/#2-smart-memory-management","title":"2. Smart Memory Management","text":"<p>Implement intelligent memory management with relevance-based retention:</p> <pre><code>class SmartMemoryEnvironment(PersistentEnvironment):\n    \"\"\"Environment with intelligent memory management.\"\"\"\n\n    def __init__(self, max_memory_items: int = 100, relevance_threshold: float = 0.7, **kwargs):\n        super().__init__(**kwargs)\n        self.max_memory_items = max_memory_items\n        self.relevance_threshold = relevance_threshold\n        self.memory_scorer = MemoryScorer()\n\n    async def add_objects(self, tool_name: str, result_name: str, objects: list, metadata: dict):\n        \"\"\"Add results with automatic memory management.\"\"\"\n        # Add new objects  \n        super().add_objects(tool_name, result_name, objects, metadata)\n\n        # Check if memory cleanup is needed\n        total_items = self._count_total_items()\n        if total_items &gt; self.max_memory_items:\n            await self._cleanup_memory()\n\n        await self.save_state()\n\n    def _count_total_items(self) -&gt; int:\n        \"\"\"Count total items in environment.\"\"\"\n        count = 0\n        for tool_results in self.environment.values():\n            for result_list in tool_results.values():\n                count += len(result_list)\n        return count\n\n    async def _cleanup_memory(self):\n        \"\"\"Remove least relevant memories to stay under limit.\"\"\"\n        # Score all memory items\n        scored_items = []\n\n        for tool_name, tool_results in self.environment.items():\n            for result_name, result_list in tool_results.items():\n                for i, result in enumerate(result_list):\n                    score = await self.memory_scorer.score_memory_item(\n                        result, \n                        tool_name, \n                        result_name,\n                        self.environment\n                    )\n                    scored_items.append({\n                        'score': score,\n                        'tool_name': tool_name,\n                        'result_name': result_name,\n                        'index': i,\n                        'result': result\n                    })\n\n        # Sort by score (keep highest scoring items)\n        scored_items.sort(key=lambda x: x['score'], reverse=True)\n\n        # Keep only top items\n        items_to_keep = scored_items[:self.max_memory_items]\n\n        # Rebuild environment with only kept items\n        new_environment = {}\n        for item in items_to_keep:\n            tool_name = item['tool_name']\n            result_name = item['result_name']\n\n            if tool_name not in new_environment:\n                new_environment[tool_name] = {}\n            if result_name not in new_environment[tool_name]:\n                new_environment[tool_name][result_name] = []\n\n            new_environment[tool_name][result_name].append(item['result'])\n\n        self.environment = new_environment\n\nclass MemoryScorer:\n    \"\"\"Scores memory items for relevance and importance.\"\"\"\n\n    async def score_memory_item(\n        self, \n        memory_item: Dict, \n        tool_name: str, \n        result_name: str,\n        full_environment: Dict\n    ) -&gt; float:\n        \"\"\"Score a memory item for its importance/relevance.\"\"\"\n\n        score = 0.0\n        metadata = memory_item.get('metadata', {})\n\n        # Recency score (more recent = higher score)\n        recency_score = self._calculate_recency_score(metadata.get('timestamp'))\n        score += recency_score * 0.3\n\n        # Usage frequency score\n        usage_score = self._calculate_usage_score(memory_item, full_environment)\n        score += usage_score * 0.2\n\n        # Content relevance score\n        relevance_score = await self._calculate_relevance_score(memory_item, full_environment)\n        score += relevance_score * 0.3\n\n        # Tool importance score\n        tool_importance = self._get_tool_importance(tool_name)\n        score += tool_importance * 0.2\n\n        return min(score, 1.0)  # Cap at 1.0\n\n    def _calculate_recency_score(self, timestamp: str) -&gt; float:\n        \"\"\"Calculate score based on how recent the memory is.\"\"\"\n        if not timestamp:\n            return 0.0\n\n        try:\n            memory_time = datetime.fromisoformat(timestamp)\n            now = datetime.utcnow()\n            time_diff = (now - memory_time).total_seconds()\n\n            # Exponential decay over 7 days\n            decay_factor = 7 * 24 * 3600  # 7 days in seconds\n            return math.exp(-time_diff / decay_factor)\n        except:\n            return 0.0\n\n    def _calculate_usage_score(self, memory_item: Dict, full_environment: Dict) -&gt; float:\n        \"\"\"Calculate score based on how often this memory is referenced.\"\"\"\n        # Simple implementation: count how many times similar objects appear\n        objects = memory_item.get('objects', [])\n        if not objects:\n            return 0.0\n\n        # Count similar objects in environment\n        similar_count = 0\n        total_count = 0\n\n        for tool_results in full_environment.values():\n            for result_list in tool_results.values():\n                for result in result_list:\n                    total_count += 1\n                    if self._objects_similar(objects, result.get('objects', [])):\n                        similar_count += 1\n\n        return similar_count / max(total_count, 1) if total_count &gt; 0 else 0.0\n\n    async def _calculate_relevance_score(self, memory_item: Dict, full_environment: Dict) -&gt; float:\n        \"\"\"Calculate relevance score based on semantic similarity to recent queries.\"\"\"\n        # Extract recent query context\n        recent_queries = self._extract_recent_queries(full_environment)\n        if not recent_queries:\n            return 0.5  # Neutral score if no recent queries\n\n        # Calculate semantic similarity (simplified)\n        memory_text = self._extract_text_from_memory(memory_item)\n        query_text = \" \".join(recent_queries[-3:])  # Last 3 queries\n\n        # Use embedding similarity (placeholder implementation)\n        similarity = await self._calculate_semantic_similarity(memory_text, query_text)\n        return similarity\n\n    def _get_tool_importance(self, tool_name: str) -&gt; float:\n        \"\"\"Get importance score for different tools.\"\"\"\n        importance_map = {\n            'query': 0.9,      # Query results are very important\n            'aggregate': 0.7,   # Aggregation results are important\n            'summarize': 0.8,   # Summaries are important\n            'text': 0.6,       # Text responses are moderately important\n            'visualize': 0.5    # Visualizations are less critical\n        }\n        return importance_map.get(tool_name, 0.5)\n</code></pre>"},{"location":"RAG-DEV/#3-context-aware-memory-retrieval","title":"3. Context-Aware Memory Retrieval","text":"<pre><code>class ContextAwareEnvironment(SmartMemoryEnvironment):\n    \"\"\"Environment that provides context-aware memory retrieval.\"\"\"\n\n    async def get_relevant_context(\n        self, \n        current_query: str, \n        max_items: int = 10,\n        context_types: List[str] = None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Retrieve relevant context for the current query.\"\"\"\n\n        if context_types is None:\n            context_types = ['query', 'aggregate', 'summarize']\n\n        relevant_items = []\n\n        # Score all memory items for relevance to current query\n        for tool_name in context_types:\n            if tool_name in self.environment:\n                for result_name, result_list in self.environment[tool_name].items():\n                    for result in result_list:\n                        relevance_score = await self._calculate_query_relevance(\n                            current_query, result\n                        )\n                        if relevance_score &gt; 0.3:  # Relevance threshold\n                            relevant_items.append({\n                                'score': relevance_score,\n                                'tool_name': tool_name,\n                                'result_name': result_name,\n                                'result': result\n                            })\n\n        # Sort by relevance and return top items\n        relevant_items.sort(key=lambda x: x['score'], reverse=True)\n        top_items = relevant_items[:max_items]\n\n        # Format context for consumption\n        context = {\n            'relevant_retrievals': [],\n            'relevant_summaries': [],\n            'related_queries': [],\n            'entities_mentioned': set(),\n            'topics_covered': set()\n        }\n\n        for item in top_items:\n            tool_name = item['tool_name']\n            result = item['result']\n\n            if tool_name == 'query':\n                context['relevant_retrievals'].append({\n                    'collection': item['result_name'],\n                    'objects': result.get('objects', []),\n                    'metadata': result.get('metadata', {}),\n                    'relevance_score': item['score']\n                })\n            elif tool_name == 'summarize':\n                context['relevant_summaries'].append({\n                    'summary': result.get('objects', [{}])[0].get('summary', ''),\n                    'source': item['result_name'],\n                    'relevance_score': item['score']\n                })\n\n            # Extract entities and topics\n            text_content = self._extract_text_from_result(result)\n            entities = self._extract_entities(text_content)\n            topics = self._extract_topics(text_content)\n\n            context['entities_mentioned'].update(entities)\n            context['topics_covered'].update(topics)\n\n        # Convert sets to lists for JSON serialization\n        context['entities_mentioned'] = list(context['entities_mentioned'])\n        context['topics_covered'] = list(context['topics_covered'])\n\n        return context\n\n    async def _calculate_query_relevance(self, query: str, memory_result: Dict) -&gt; float:\n        \"\"\"Calculate how relevant a memory result is to the current query.\"\"\"\n\n        # Extract text from memory result\n        memory_text = self._extract_text_from_result(memory_result)\n\n        # Calculate semantic similarity\n        semantic_score = await self._calculate_semantic_similarity(query, memory_text)\n\n        # Calculate entity overlap\n        query_entities = self._extract_entities(query)\n        memory_entities = self._extract_entities(memory_text)\n        entity_overlap = len(query_entities &amp; memory_entities) / max(len(query_entities), 1)\n\n        # Calculate keyword overlap\n        query_keywords = set(query.lower().split())\n        memory_keywords = set(memory_text.lower().split())\n        keyword_overlap = len(query_keywords &amp; memory_keywords) / max(len(query_keywords), 1)\n\n        # Combine scores\n        relevance_score = (\n            semantic_score * 0.6 +\n            entity_overlap * 0.25 +\n            keyword_overlap * 0.15\n        )\n\n        return relevance_score\n</code></pre>"},{"location":"RAG-DEV/#4-memory-integration-with-tree","title":"4. Memory Integration with Tree","text":"<p>Integrate the enhanced memory system with the Tree class:</p> <pre><code># elysia/tree/memory_aware_tree.py\nfrom elysia.tree.tree import Tree\nfrom elysia.tree.persistent_environment import ContextAwareEnvironment\n\nclass MemoryAwareTree(Tree):\n    \"\"\"Tree with enhanced memory capabilities.\"\"\"\n\n    def __init__(self, memory_type: str = \"context_aware\", **kwargs):\n        super().__init__(**kwargs)\n\n        # Replace default environment with memory-aware version\n        if memory_type == \"context_aware\":\n            self.tree_data.environment = ContextAwareEnvironment(\n                user_id=self.user_id,\n                conversation_id=self.conversation_id,\n                **kwargs\n            )\n        elif memory_type == \"smart\":\n            self.tree_data.environment = SmartMemoryEnvironment(\n                user_id=self.user_id,\n                conversation_id=self.conversation_id,\n                **kwargs\n            )\n        elif memory_type == \"persistent\":\n            self.tree_data.environment = PersistentEnvironment(\n                user_id=self.user_id,\n                conversation_id=self.conversation_id,\n                **kwargs\n            )\n\n    async def __call__(self, user_prompt: str, **kwargs) -&gt; AsyncGenerator:\n        \"\"\"Enhanced call with memory context injection.\"\"\"\n\n        # Load previous conversation state\n        if hasattr(self.tree_data.environment, 'load_state'):\n            await self.tree_data.environment.load_state()\n\n        # Get relevant context for the current query\n        if hasattr(self.tree_data.environment, 'get_relevant_context'):\n            relevant_context = await self.tree_data.environment.get_relevant_context(\n                user_prompt, max_items=5\n            )\n\n            # Inject context into tree data for LLM consumption\n            self.tree_data.environment.hidden_environment['query_context'] = relevant_context\n\n        # Proceed with normal tree execution\n        async for result in super().__call__(user_prompt, **kwargs):\n            yield result\n\n        # Save updated state\n        if hasattr(self.tree_data.environment, 'save_state'):\n            await self.tree_data.environment.save_state()\n</code></pre>"},{"location":"RAG-DEV/#memory-best-practices","title":"Memory Best Practices","text":"<ol> <li>Selective Memory: Don't store everything - be selective about what gets remembered</li> <li>Context Relevance: Prioritize recent and relevant memories over older ones</li> <li>Memory Limits: Implement reasonable limits to prevent memory bloat</li> <li>Privacy Considerations: Be mindful of sensitive information in persistent storage</li> <li>Performance: Balance memory richness with retrieval performance</li> </ol>"},{"location":"RAG-DEV/#development-setup","title":"Development Setup","text":""},{"location":"RAG-DEV/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10-3.12</li> <li>Poetry or pip for dependency management</li> <li>Access to a Weaviate instance (local or cloud)</li> <li>API keys for LLM providers (OpenAI, OpenRouter, etc.)</li> </ul>"},{"location":"RAG-DEV/#installation-for-development","title":"Installation for Development","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/weaviate/elysia\ncd elysia\n</code></pre></p> </li> <li> <p>Set up virtual environment: <pre><code>python3.12 -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre></p> </li> <li> <p>Install dependencies: <pre><code>pip install -e .\npip install -e \".[dev]\"  # Include development dependencies\n</code></pre></p> </li> <li> <p>Set up environment variables: <pre><code>cp .env.example .env\n# Edit .env with your configuration\n</code></pre></p> </li> </ol> <p>Example <code>.env</code> configuration: <pre><code># Weaviate Configuration\nWCD_URL=https://your-cluster.weaviate.network\nWCD_API_KEY=your-weaviate-api-key\nWEAVIATE_IS_LOCAL=False\n\n# LLM Configuration\nOPENAI_API_KEY=your-openai-key\nOPENROUTER_API_KEY=your-openrouter-key\n\n# Development Settings\nLOGGING_LEVEL=DEBUG\nUSE_FEEDBACK=False\n</code></pre></p> <ol> <li>Install spaCy model: <pre><code>python -m spacy download en_core_web_sm\n</code></pre></li> </ol>"},{"location":"RAG-DEV/#directory-structure","title":"Directory Structure","text":"<pre><code>elysia/\n\u251c\u2500\u2500 elysia/                 # Main package\n\u2502   \u251c\u2500\u2500 api/               # FastAPI backend\n\u2502   \u251c\u2500\u2500 tools/             # RAG tools (query, aggregate, etc.)\n\u2502   \u2502   \u251c\u2500\u2500 retrieval/     # Retrieval-specific tools\n\u2502   \u2502   \u251c\u2500\u2500 postprocessing/ # Post-processing tools\n\u2502   \u2502   \u251c\u2500\u2500 text/          # Text generation tools\n\u2502   \u2502   \u2514\u2500\u2500 visualisation/ # Visualization tools\n\u2502   \u251c\u2500\u2500 tree/              # Decision tree logic\n\u2502   \u251c\u2500\u2500 preprocessing/     # Collection preprocessing\n\u2502   \u251c\u2500\u2500 util/              # Utility functions\n\u2502   \u2514\u2500\u2500 config.py          # Configuration management\n\u251c\u2500\u2500 tests/                 # Test suite\n\u251c\u2500\u2500 docs/                  # Documentation\n\u2514\u2500\u2500 examples/              # Example scripts\n</code></pre>"},{"location":"RAG-DEV/#configuration-management","title":"Configuration Management","text":"<p>Elysia uses a centralized configuration system:</p> <pre><code>from elysia.config import Settings, settings\n\n# Access current settings\nprint(settings.WCD_URL)\nprint(settings.OPENAI_API_KEY)\n\n# Create custom settings\ncustom_settings = Settings(\n    WCD_URL=\"http://localhost:8080\",\n    WEAVIATE_IS_LOCAL=True,\n    LOGGING_LEVEL=\"INFO\"\n)\n</code></pre>"},{"location":"RAG-DEV/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"RAG-DEV/#test-structure","title":"Test Structure","text":"<p>Tests are organized by functionality:</p> <pre><code>tests/\n\u251c\u2500\u2500 no_reqs/              # Tests that don't require external services\n\u2502   \u251c\u2500\u2500 test_chunking.py\n\u2502   \u251c\u2500\u2500 test_memory.py\n\u2502   \u2514\u2500\u2500 test_utils.py\n\u2514\u2500\u2500 requires_env/         # Tests requiring API keys/services\n    \u251c\u2500\u2500 test_retrieval.py\n    \u251c\u2500\u2500 test_preprocessing.py\n    \u2514\u2500\u2500 test_integration.py\n</code></pre>"},{"location":"RAG-DEV/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test category\npytest tests/no_reqs/\n\n# Run with coverage\npytest --cov=elysia --cov-report=html\n\n# Run specific test\npytest tests/no_reqs/test_chunking.py::test_sentence_chunking\n</code></pre>"},{"location":"RAG-DEV/#writing-tests","title":"Writing Tests","text":""},{"location":"RAG-DEV/#1-unit-tests-for-chunking","title":"1. Unit Tests for Chunking","text":"<pre><code># tests/no_reqs/test_chunking.py\nimport pytest\nfrom elysia.tools.retrieval.chunk import Chunker\n\nclass TestChunker:\n    def test_sentence_chunking(self):\n        chunker = Chunker(chunking_strategy=\"sentences\", num_sentences=2)\n        document = \"First sentence. Second sentence. Third sentence. Fourth sentence.\"\n\n        chunks, spans = chunker.chunk(document)\n\n        assert len(chunks) == 2\n        assert \"First sentence. Second sentence.\" in chunks[0]\n        assert \"Third sentence. Fourth sentence.\" in chunks[1]\n\n    def test_token_chunking(self):\n        chunker = Chunker(chunking_strategy=\"fixed\", num_tokens=5)\n        document = \"This is a test document with many tokens for testing chunking.\"\n\n        chunks, spans = chunker.chunk(document)\n\n        assert len(chunks) &gt; 1\n        assert all(len(chunk.split()) &lt;= 7 for chunk in chunks)  # Allow some overlap\n\n    def test_chunk_overlap(self):\n        chunker = Chunker(chunking_strategy=\"sentences\", num_sentences=3)\n        document = \"Sentence one. Sentence two. Sentence three. Sentence four. Sentence five.\"\n\n        chunks, spans = chunker.chunk_by_sentences(document, overlap_sentences=1)\n\n        # Check that chunks have expected overlap\n        assert len(chunks) &gt;= 2\n        # Verify overlap exists between consecutive chunks\n</code></pre>"},{"location":"RAG-DEV/#2-integration-tests-for-retrieval","title":"2. Integration Tests for Retrieval","text":"<pre><code># tests/requires_env/test_retrieval.py\nimport pytest\nimport asyncio\nfrom elysia.tools.retrieval.query import Query\nfrom elysia.tree.objects import TreeData, CollectionData\nfrom elysia.util.client import ClientManager\n\n@pytest.mark.asyncio\nclass TestRetrieval:\n    @pytest.fixture\n    async def setup_test_data(self):\n        # Setup test collection with sample data\n        client_manager = ClientManager()\n        # ... setup code\n        yield client_manager\n        # ... cleanup code\n\n    async def test_semantic_search(self, setup_test_data):\n        client_manager = setup_test_data\n\n        # Create test tree data\n        tree_data = TreeData(\n            user_prompt=\"Find information about AI\",\n            collection_data=CollectionData(collection_names=[\"test_collection\"])\n        )\n\n        # Execute query\n        query_tool = Query()\n        inputs = {\"collection_names\": [\"test_collection\"]}\n\n        results = []\n        async for result in query_tool(\n            tree_data=tree_data,\n            inputs=inputs,\n            base_lm=None,  # Mock LM\n            complex_lm=None,  # Mock LM\n            client_manager=client_manager\n        ):\n            results.append(result)\n\n        # Verify results\n        assert len(results) &gt; 0\n        # Add more specific assertions\n</code></pre>"},{"location":"RAG-DEV/#3-memory-system-tests","title":"3. Memory System Tests","text":"<pre><code># tests/no_reqs/test_memory.py\nimport pytest\nfrom elysia.tree.objects import Environment\n\nclass TestEnvironment:\n    def test_add_and_retrieve_results(self):\n        env = Environment()\n\n        # Add test results\n        test_objects = [{\"id\": 1, \"content\": \"test content\"}]\n        test_metadata = {\"query\": \"test query\", \"timestamp\": \"2024-01-01\"}\n\n        env.add_objects(\"query\", \"test_collection\", test_objects, test_metadata)\n\n        # Retrieve results  \n        results = env.find(\"query\", \"test_collection\")\n\n        assert len(results) == 1\n        assert results[0][\"objects\"] == test_objects\n        assert results[0][\"metadata\"] == test_metadata\n\n    def test_memory_search(self):\n        env = Environment()\n\n        # Add multiple results\n        env.add_objects(\"query\", \"collection1\", [{\"type\": \"document\"}], {\"topic\": \"AI\"})\n        env.add_objects(\"query\", \"collection2\", [{\"type\": \"article\"}], {\"topic\": \"ML\"})\n        env.add_objects(\"aggregate\", \"collection1\", [{\"count\": 10}], {\"operation\": \"count\"})\n\n        # Search by tool and result name\n        ai_results = env.find(\"query\", \"collection1\")\n        assert len(ai_results) == 1\n\n        # Get all query results from environment\n        query_results = []\n        if \"query\" in env.environment:\n            for result_name in env.environment[\"query\"]:\n                query_results.extend(env.find(\"query\", result_name))\n        assert len(query_results) == 2\n</code></pre>"},{"location":"RAG-DEV/#mocking-external-services","title":"Mocking External Services","text":"<p>For tests that interact with external services, use mocking:</p> <pre><code># tests/conftest.py\nimport pytest\nfrom unittest.mock import AsyncMock, MagicMock\n\n@pytest.fixture\ndef mock_weaviate_client():\n    client = AsyncMock()\n    client.collections.exists.return_value = True\n    client.collections.get.return_value.query.fetch_objects.return_value.objects = []\n    return client\n\n@pytest.fixture\ndef mock_llm():\n    llm = MagicMock()\n    llm.generate.return_value = \"Mocked LLM response\"\n    return llm\n</code></pre>"},{"location":"RAG-DEV/#contributing-guidelines","title":"Contributing Guidelines","text":""},{"location":"RAG-DEV/#code-style","title":"Code Style","text":"<p>Elysia follows Python best practices:</p> <ol> <li>PEP 8 compliance with line length of 88 characters (Black default)</li> <li>Type hints for all public functions and methods</li> <li>Docstrings in Google style for all public APIs</li> <li>Async/await for I/O operations</li> </ol>"},{"location":"RAG-DEV/#code-formatting","title":"Code Formatting","text":"<p>Use the provided development tools:</p> <pre><code># Format code\nblack elysia/ tests/\n\n# Sort imports\nisort elysia/ tests/\n\n# Type checking (if mypy is configured)\nmypy elysia/\n</code></pre>"},{"location":"RAG-DEV/#submitting-changes","title":"Submitting Changes","text":"<ol> <li> <p>Create a feature branch: <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make your changes following the guidelines above</p> </li> <li> <p>Add tests for new functionality</p> </li> <li> <p>Run the test suite: <pre><code>pytest\n</code></pre></p> </li> <li> <p>Update documentation if needed</p> </li> <li> <p>Commit with descriptive messages: <pre><code>git commit -m \"Add custom chunking strategy for PDF documents\"\n</code></pre></p> </li> <li> <p>Submit a pull request with:</p> </li> <li>Clear description of changes</li> <li>Test results</li> <li>Documentation updates</li> <li>Breaking change notes (if any)</li> </ol>"},{"location":"RAG-DEV/#common-contribution-areas","title":"Common Contribution Areas","text":"<ol> <li>New Chunking Strategies: Implement domain-specific chunking</li> <li>Vector Database Adapters: Add support for new vector databases</li> <li>Memory Enhancements: Improve memory management and context handling</li> <li>Retrieval Algorithms: Add new search and ranking algorithms</li> <li>Tool Extensions: Create new tools for specific use cases</li> <li>Performance Optimizations: Improve speed and memory usage</li> <li>Documentation: Improve guides and examples</li> </ol>"},{"location":"RAG-DEV/#performance-considerations","title":"Performance Considerations","text":"<p>When contributing, consider:</p> <ol> <li>Async Operations: Use async/await for I/O bound operations</li> <li>Memory Usage: Be mindful of memory consumption in long-running conversations</li> <li>Vectorization Costs: Consider the cost of generating embeddings</li> <li>Caching: Implement appropriate caching where beneficial</li> <li>Batch Operations: Use batch operations for multiple documents</li> </ol>"},{"location":"RAG-DEV/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: https://weaviate.github.io/elysia/</li> <li>GitHub Issues: https://github.com/weaviate/elysia/issues</li> <li>Discussions: Use GitHub Discussions for questions and ideas</li> </ul> <p>This guide provides a comprehensive foundation for contributing to Elysia's RAG system. The modular architecture makes it straightforward to customize and extend functionality while maintaining compatibility with the existing system.</p>"},{"location":"advanced_usage/","title":"Customising the Elysia Decision Tree","text":"<p>If you haven't followed the basic guide yet, check that out first before you go into more detail here.</p> <p>See the setup page for details on setting up models and API keys.</p>"},{"location":"advanced_usage/#changing-the-style-agent-description-or-end-goal","title":"Changing the Style, Agent Description or End Goal","text":"<p>You can initialise these parameters at the startup of the tree, for example <pre><code>from elysia import Tree\ntree = Tree(\n    style = \"...\",\n    agent_description = \"...\",\n    end_goal = \"...\"\n)\n</code></pre> Each of these options is described below.</p>"},{"location":"advanced_usage/#style","title":"Style","text":"<p>You can customise the style of Elysia's text responses by either initialising the decision tree with the <code>style</code> argument,  or creating a tree and then modifying the style later. <pre><code>tree = Tree()\ntree.change_style(\"Always speak in rhyming couplets.\")\n</code></pre></p> <p>You should see the changes in Elysia's textual responses! <pre><code>response, _ = tree(\"Hi Elysia, how are you?\")\nprint(response)\n</code></pre> <pre><code>## Hello there, it's Elly, I'm doing fine, I hope you are as well, in this rhyme!\n</code></pre></p>"},{"location":"advanced_usage/#agent-description","title":"Agent Description","text":"<p>The <code>agent_description</code> parameter in Elysia assigns the agent a unique identity, which you can use to customise the objective of the agent. </p> <p>Change this via <code>change_agent_description</code>, e.g. <pre><code>tree.change_agent_description(\"You are a travel agent, specialised in creating unique travel plans for customers that interact with you.\")\n</code></pre></p>"},{"location":"advanced_usage/#end-goal","title":"End Goal","text":"<p>The <code>end_goal</code> parameter describes to Elysia what criteria it will use to decide when the decision tree should end. Normally, this is something similar to 'When you have either achieved all the user has asked of you, or you can no longer do any actions that have not failed already, and you have no other options to explore.'. But this could be unique to your use case, for example, <pre><code>tree.change_end_goal(\"The user has been recommended a hotel, travel options as well as activities to do in the local area. Or, you have exhausted all options. Or, you have asked the user for more clarification about their request.\")\n</code></pre></p>"},{"location":"advanced_usage/#creating-a-title","title":"Creating a Title","text":"<p>You can run <code>tree.create_conversation_title()</code> to use the base LM in the tree to create a title for the conversation (which uses the conversation history as inspiration). This saves the title as the <code>tree.conversation_title</code> attribute, which you can also directly overwrite or access if needed. An async version of this method is available at <code>tree.create_conversation_title_async()</code>.</p>"},{"location":"advanced_usage/#loading-and-saving-trees","title":"Loading and Saving Trees","text":""},{"location":"advanced_usage/#locally","title":"Locally","text":"<p>You can export a decision tree to a JSON serialisable dictionary object via <code>tree.export_to_json()</code>. This saves certain aspects, including:</p> <ul> <li>The Tree Data, which includes:<ul> <li>The environment, including all retrieved objects from previous tool runs or otherwise</li> <li>The collection metadata, if processed</li> </ul> </li> <li>Class variables used to initialise the tree</li> <li>The config options, such as the settings (including the model choices) and the style, agent description and end goal.</li> <li>The branch initialisation, which will be re-run when the tree is loaded.</li> </ul> <p>The tree can be loaded from the dictionary via running the <code>Tree.import_from_json(json_data=...)</code> method, passing as the first argument <code>json_data</code> which is the output from <code>tree.export_to_json()</code>.</p> <p>Note that any custom tools or branches added to the decision tree are not saved and need to be manually re-added, in the same way that your tree was originally initialised.</p>"},{"location":"advanced_usage/#with-weaviate","title":"With Weaviate","text":"<p>Also included are two similar functions for saving and loading a decision tree within a Weaviate instance. To save a tree in a Weaviate instance, you can use <code>tree.export_to_weaviate(collection_name, client_manager)</code>. You can specify the collection that you will be saving to via the <code>collection_name</code> argument. You can specify the Weaviate cluster information by passing a <code>ClientManager</code>. If you do not provide a ClientManager, it will automatically create one from the specification set in the environment variables. It will save the tree according to the <code>conversation_id</code> used to initialise the tree (which is randomly generated via a UUID if not set).</p> <p>To load a tree from Weaviate, you can use the class method <code>Tree.import_from_weaviate(collection_name, conversation_id, client_manager)</code>. You must use the correct <code>conversation_id</code> to load a tree. If you do not know the conversation ID, you can view all available conversation IDs saved to a Weaviate collection via <pre><code>from elysia.tree.util import get_saved_trees_weaviate\nget_saved_trees_weaviate()\n</code></pre> Which will return a dictionary whose keys correspond to the available conversation IDs, and whose values are the titles as strings of the conversations (if one was created via <code>tree.create_conversation_title()</code>).</p> <p>Note that any custom tools or branches added to the decision tree are not saved and need to be manually re-added, in the same way that your tree was originally initialised.</p>"},{"location":"basic/","title":"Basic Example","text":"<p>Let's assume we have access to the following Weaviate collections:</p> <ul> <li> <p><code>ecommerce</code>: a fashion dataset with fields such as <code>price</code>, <code>category</code>, <code>description</code>, etc.</p> </li> <li> <p><code>ml_wikipedia</code>: a collection of long Wikipedia articles related to machine learning, with fields such as <code>categories</code>, <code>content</code>, <code>title</code> etc.</p> </li> <li> <p><code>Tickets</code>: Github issues for a fictional company</p> </li> </ul> <p>These Weaviate collections are those which we want to search over using Elysia.</p>"},{"location":"basic/#setup","title":"Setup","text":"<p>You need to specify what models you want to use, as well as any API keys. To set up the models, you can use <code>configure</code>. For example, if you want to use the GPT-4o family of models:</p> <pre><code>from elysia import configure\nconfigure(\n    base_model=\"gpt-4o-mini\",\n    base_provider=\"openai\",\n    complex_model=\"gpt-4o\",\n    complex_provider=\"openai\",\n    openai_api_key=\"sk-...\", # replace with your API key\n    wcd_url=\"...\",    # replace with your weaviate cloud url\n    wcd_api_key=\"...\" # replace with your weaviate cloud api key\n)\n</code></pre> <p>You need to specify both a <code>base_model</code> and a <code>complex_model</code>, as well as their providers. This hooks into LiteLLM through DSPy, so any LiteLLM supported models and providers will work. See the setup page for more details.</p> <p>Then for a collection to be accessible within Elysia, we need to preprocess it - so that the models are aware of the schemas and information about the collection.</p> <pre><code>from elysia import preprocess\npreprocess(\"Tickets\")\n</code></pre>"},{"location":"basic/#running-elysia","title":"Running Elysia","text":"<p>To run the Elysia decision tree, using the default setup, just call the <code>Tree</code> object!</p> <p><pre><code>from elysia import Tree\ntree = Tree()\nresponse, objects = tree(\"what were the 10 most recent Github issues?\")\n</code></pre> Elysia will dynamically run through the decision tree, choosing tools to use based on the decision agent LM based on the input. The decision tree returns the concatenation of all text responses from the models, as well as any retrieved objects (anything that was added to the environment during this call).</p> <p><pre><code>print(response)\n</code></pre> <pre><code>I will now query the Tickets collection to retrieve the 10 most recent issues for you. I applied a descending sort on the \"issue_created_at\" field to retrieve the 10 most recent issues. I will now summarize the 10 most recent Github issues for you. The latest tickets reflect ongoing discussions and developments within the Verba project. Notable entries include a closed issue regarding the use of specific model inputs, a report on a breaking change affecting the code chunker functionality, and requests for enhancements like custom JSON support and improved metadata handling during file uploads. The issues also highlight user concerns about application performance when processing large document uploads and the integration of external language models. Overall, these issues illustrate a dynamic environment with active contributions and feedback from the community.\n</code></pre> <pre><code>print(objects)\n</code></pre> <pre><code>[\n    [\n        {\n            'issue_id': 2843638219.0,\n            'issue_content': \"If you set OLLAMA_MODEL and OLLAMA_EMBED_MODEL they will be the ones suggested instead of the first on Ollama's list.\",\n            'issue_created_at': '2025-02-10T21:06:00Z',\n            'issue_labels': [],\n            'issue_url': 'https://github.com/weaviate/Verba/pull/372',\n            'issue_comments': 0.0,\n            'issue_title': 'use OLLAMA_MODEL OLLAMA_EMBED_MODEL as input suggestion when using Ollama',\n            'issue_author': 'dudanogueira',\n            'issue_updated_at': '2025-02-27T10:38:02Z',\n            'issue_state': 'closed',\n            'uuid': 'bc56b4b2fc6a541c94969721bd895a7c',\n            'ELYSIA_SUMMARY': ''\n        },\n    ...\n        {\n            'issue_id': 2845625123.0,\n            'issue_content': \"Pull request for feature #2.\",\n            'issue_created_at': '2025-01-4T22:16:05Z',\n            'issue_labels': [],\n            'issue_url': 'https://github.com/weaviate/Verba/pull/373',\n            'issue_comments': 2.0,\n            'issue_title': 'Feature PR #2',\n            'issue_author': 'thomashacker',\n            'issue_updated_at': '2025-01-15T11:06:08Z',\n            'issue_state': 'closed',\n            'uuid': '05dae4214e9050a59d4e9985892cdc10',\n            'ELYSIA_SUMMARY': ''\n        }\n    ]\n]\n</code></pre></p>"},{"location":"creating_tools/","title":"Creating a Tool","text":"<p>You can use the custom tool decorator within Elysia to very simply add a tool to the tree. For example:</p> <pre><code>from elysia import tool\n\n@tool\nasync def add(x: int, y: int) -&gt; int:\n    \"\"\"\n    Return the sum of two numbers.\n    \"\"\"\n    return x + y\n</code></pre> <p>The docstring of the function serves as the tool description, and it's important this is as detailed as possible. This tool can be added to an Elysia <code>Tree</code> via</p> <pre><code>from elysia import Tree\ntree = Tree()\ntree.add_tool(add)\n</code></pre> <p>Then when calling the tree, the decision agent should use the tool if it recognises it as the best tool for the task.</p> <p><pre><code>response, objects = tree(\"What is 1238213 + 1238213?\")\nprint(response)\n</code></pre> <pre><code>'I will calculate the sum for you using the add tool. The sum of 1238213 + 1238213 is 2476426.'\n</code></pre></p> <p>And this is all you need to do to add a tool to Elysia! Some things to note:</p> <ul> <li>Your tool must be an async function (must be defined via <code>async def</code> instead of <code>def</code>).</li> <li><code>tree.add_tool(add)</code> added this tool to the root decision node (at the base of the tree). If you are using a tree with multiple branches, you can specify which branch it is added to via <code>tree.add_tool(add, branch_id=...)</code> where <code>...</code> should be replaced with the <code>branch_id</code>.</li> <li>You can add a tool to the tree automatically via customising the decorator function, e.g.     <pre><code>@tool(tree=tree, branch_id=\"base\")\nasync def add(x: int, y: int) -&gt; int:\n    return x + y    \n</code></pre>     which will automatically add it to a pre-defined tree (<code>tree</code>) at branch ID <code>\"base\"</code>.</li> <li>Type hinting (e.g. declaring <code>x: int</code> and <code>y: int</code>) helps the LLM choose the correct input types to the function.</li> </ul>"},{"location":"creating_tools/#more-detail","title":"More Detail","text":"<p>Elysia works by adding objects to its internal environment. For example, when we called the <code>add</code> function above, it automatically added a <code>Result</code> type object to the Elysia tree environment. Any objects directly returned by the function will be added to the environment under a generic set of keys. To have more control over this, you can create your tool as an async generator function, which yields objects. For example, let's extend our basic calculator a bit further:</p> <pre><code>@tool\nasync def calculate_two_numbers(x: int, y: int):\n    \"\"\"\n    This function calculates the sum, product, and difference of two numbers.\n    \"\"\"\n    yield {\n        \"sum\": x + y,\n        \"product\": x * y,\n        \"difference\": x - y,\n    }\n    yield f\"I just performed some calculations on {x} and {y}.\"\n</code></pre> <p>This now returns two items to the decision tree, a string and a dictionary. There is no limit to the amount of objects you can yield. </p> <p>When a string is returned, it automatically becomes a response from Elysia, so it will be displayed to the user as if the agent is talking back to them. When any other type of item is yielded, it becomes a <code>Result</code> type object which means it becomes part of the tree's environment. Yielding or returning a dictionary means you can customise what specific object is added to the environment. Returning or yielding a list of dictionaries will add multiple objects to the <code>Result</code>. You can also return or yield one or more <code>Result</code> objects directly.</p>"},{"location":"creating_tools/#assigning-inputs","title":"Assigning Inputs","text":"<p>The decision agent LLM is responsible for choosing the correct inputs to the tool. Any inputs added to the declaration of your tool will be automatically chosen by the LLM. Let's extend the calculator even more:</p> <pre><code>from math import prod\n@tool\nasync def perform_mathematical_operations(numbers: list[int | float], operation: str = \"sum\"):\n    \"\"\"\n    This function calculates a mathematical operation on the `numbers` list.\n    The `numbers` input must be a list of integers or floats.\n    The `operation` input must be one of: \"sum\" or \"product\". These are the only options.\n    \"\"\"\n\n    if operation == \"sum\":\n        yield sum(numbers)\n    elif operation == \"product\":\n        yield prod(numbers)\n\n    yield f\"I just performed a {operation} on {numbers}.\"    \n</code></pre> <p>Now the LLM should choose the operation in addition to the numbers. We also extended it so that the values can be a list of integers or floats, not just two numbers. The default argument, indicated by <code>operation: str = \"sum\"</code>, give the decision agent awareness of what the default argument for that particular input is - and it is no longer a required input and can be ignored, in which case the default argument is used. Note how the tool description details the descriptions of each input. In more advanced tool construction, you can assign descriptions to each input separately.</p>"},{"location":"creating_tools/#advanced-features","title":"Advanced Features","text":"<p>If your tool may error, then you can return or yield a custom Elysia <code>Error</code> object which will not cause a halt in the execution of the program. Instead, the error message will be logged in the decision tree for which the decision agent can judge whether the error is avoidable on another run of the tool. For example, if our decision agent tries to choose the wrong <code>operation</code> in the above <code>perform_mathematical_operations</code> tool, we can do something like this: <pre><code>from elysia import Error\n@tool\nasync def perform_mathematical_operations(numbers: list[int | float], operation: str = \"sum\"):\n    \"\"\"\n    This function calculates a mathematical operation on the `numbers` list.\n    The `numbers` input must be a list of integers or floats.\n    The `operation` input must be one of: \"sum\" or \"product\". These are the only options.\n    \"\"\"\n\n    if operation == \"sum\":\n        yield sum(numbers)\n    elif operation == \"product\":\n        yield prod(numbers)\n    else:\n        # This will return an error back to the decision tree\n        yield Error(f\"You picked the input {operation}, but it was not in the available operations: 'sum' or 'product'\")\n        return # Then return out of the tool early\n\n    yield f\"I just performed a {operation} on {numbers}.\"    \n</code></pre></p> <p>Finally, tools can interact with Elysia's environment, LMs and the Weaviate client through specific inputs to the function. To use the <code>TreeData</code> class, you can use the argument <code>tree_data</code> in the function signature (for which you can access the Elysia environment). Likewise, for the base LM you can use <code>base_lm</code>, for the complex LM you can use <code>complex_lm</code> and for the Client Manager you can use <code>client_manager</code>. For example:</p> <pre><code>@tool\nasync def some_tool(\n    tree_data, base_lm, complex_lm, client_manager, # these inputs are automatically assigned as Elysia variables\n    x: str, y: int # these inputs are not assigned automatically and get assigned by the decision agent\n):\n    # do something\n    pass\n</code></pre> <p>All optional arguments you can pass to the <code>@tool</code> decorator are:</p> <ul> <li><code>tree</code> (<code>Tree</code>): the tree that you will automatically add the tool to.</li> <li><code>branch_id</code> (<code>str</code>): the ID of the branch on the tree to add the tool to.</li> <li><code>status</code> (<code>str</code>): a custom message to display whilst the tool is running.</li> <li><code>end</code> (<code>bool</code>): when <code>True</code>, this tool can be the end of the conversation if the decision agent decides it should end after the completion of this tool.</li> </ul>"},{"location":"creating_tools/#tool-discovery-and-initialization","title":"Tool Discovery and Initialization","text":""},{"location":"creating_tools/#default-tools-tree","title":"Default Tools (Tree)","text":"<p>Elysia automatically loads a set of default tools when a Tree is initialized. The specific tools loaded depend on the initialization mode:</p> <ul> <li><code>one_branch</code> (default): All tools are added to a single \"base\" branch</li> <li> <p>Query, Aggregate, CitedSummarizer, FakeTextResponse, Visualise, SummariseItems</p> </li> <li> <p><code>multi_branch</code>: Tools are organized into multiple branches</p> </li> <li>Base branch: CitedSummarizer, FakeTextResponse, Visualise</li> <li> <p>Search branch: Query, Aggregate, SummariseItems</p> </li> <li> <p><code>empty</code>: No tools are loaded by default (you must add them manually)</p> </li> </ul>"},{"location":"creating_tools/#adding-custom-tools-to-default-initialization","title":"Adding Custom Tools to Default Initialization","text":"<p>Custom tools (including MCP tools) are automatically discovered and can be added to trees. The system looks for tools defined in:</p> <ol> <li><code>elysia/api/custom_tools.py</code>: Import your tools here to make them discoverable</li> <li><code>elysia/tools/mcp/</code>: MCP tools configured in <code>elysia/mcp.json</code> are automatically loaded</li> </ol> <p>Example of adding a custom tool to <code>custom_tools.py</code>:</p> <pre><code># In elysia/api/custom_tools.py\nfrom elysia.tools.my_custom_tool import MyCustomTool\n\n# MCP tools are auto-discovered when servers are enabled in mcp.json\n</code></pre>"},{"location":"creating_tools/#tool-discovery-system","title":"Tool Discovery System","text":"<p>Elysia maintains a registry of all discovered tools in <code>elysia/config/discovered_tools.yaml</code>. This file categorizes tools by type:</p> <ul> <li><code>retrieval</code>: Query, Aggregate</li> <li><code>text</code>: CitedSummarizer, FakeTextResponse</li> <li><code>visualization</code>: Visualise, BasicLinearRegression</li> <li><code>postprocessing</code>: SummariseItems</li> <li><code>mcp</code>: Dynamically loaded MCP server tools</li> <li><code>other</code>: Uncategorized tools</li> </ul> <p>To regenerate this file after adding new tools:</p> <pre><code>from elysia.util import generate_tool_discovery_yaml\n\n# Generate the YAML file\ngenerate_tool_discovery_yaml('elysia/config/discovered_tools.yaml')\n</code></pre>"},{"location":"creating_tools/#mcp-tools-integration","title":"MCP Tools Integration","text":"<p>MCP (Model Context Protocol) tools are automatically discovered and loaded when you:</p> <ol> <li> <p>Configure MCP servers in <code>elysia/mcp.json</code>: <pre><code>{\n  \"servers\": [\n    {\n      \"name\": \"my_mcp_server\",\n      \"description\": \"My custom MCP server\",\n      \"server_script_path\": \"/path/to/server.py\",\n      \"enabled\": true\n    }\n  ]\n}\n</code></pre></p> </li> <li> <p>The system will:</p> </li> <li>Automatically create an Elysia Tool class for each enabled server</li> <li>Add it to the tree's root branch during initialization</li> <li>Make it available via the <code>/tools/available</code> API endpoint</li> </ol> <p>MCP tools are loaded after the base initialization, so they're added on top of the default tools for your chosen mode.</p>"},{"location":"creating_tools/#modular-tool-loading","title":"Modular Tool Loading","text":"<p>The tool initialization system is modular and defined in <code>elysia/tools/ui/default_tools.py</code>. You can customize which tools are loaded by:</p> <ol> <li>Modifying the default configurations in <code>DEFAULT_TOOL_CONFIGS</code></li> <li>Providing additional tool classes during initialization:</li> </ol> <pre><code>from elysia import Tree\nfrom elysia.tools.ui import load_default_tools_for_mode\nfrom my_tools import MyCustomTool\n\ntree = Tree(branch_initialisation=\"empty\")\nload_default_tools_for_mode(\n    tree, \n    \"one_branch\",\n    additional_tool_classes=[MyCustomTool]\n)\n</code></pre> <ol> <li>Using the programmatic API to add tools after initialization:</li> </ol> <pre><code>tree = Tree()\ntree.add_tool(MyCustomTool, branch_id=\"base\")\n</code></pre>"},{"location":"setting_up/","title":"Setting up Elysia","text":"<p>Elysia requires setting up your LMs and API keys for the decision tree functionality to work. Additionally, to use Elysia to its full potential (adaptively searching and retrieving Weaviate data), it requires a preprocessing step.</p> <p>Elysia can be configured in three different ways: via the configure function, by creating a Settings object, or by setting environment variables (in a <code>.env</code> file).</p>"},{"location":"setting_up/#model-setup","title":"Model Setup","text":"<p>Elysia uses two language models for different types of tasks;</p> <ul> <li>The base model is responsible for the decision agent, as well as any tools that specify its use.</li> <li>The complex model is used for more complex tasks, and is responsible for any tools that specify its use (such as the inbuilt query and aggregate tools).</li> </ul>"},{"location":"setting_up/#configuring-models","title":"Configuring Models","text":"<p>To configure different LMs as default for all functions within Elysia, you can use the global <code>configure</code> function. For example, to use the GPT family of models, you can set:</p> <p><pre><code>from elysia import configure\n\nconfigure(\n    base_model=\"gpt-4.1-mini\",\n    base_provider=\"openai\",\n    complex_model=\"gpt-4.1\",\n    complex_provider=\"openai\",\n    openai_api_key=\"...\" # replace with your API key\n)\n</code></pre> The <code>configure</code> function can be used to specify both the <code>base_model</code> and <code>complex_model</code>. Both require separately setting a provider; in this case <code>openai</code> Instead, you can create your own <code>Settings</code> object which can be passed to any of the Elysia functions that use LMs to have a separate settings instance for each initialisation. E.g.,</p> <p><pre><code>from elysia import Settings, Tree\nmy_settings = Settings()\ntree = Tree(settings=my_settings)\n</code></pre> This tree will use the <code>my_settings</code> object instead of the global one. If not specified, it will use the global settings. You can configure <code>my_settings</code> manually, either by <code>my_settings.configure(...)</code> (which takes exactly the same arguments as <code>configure</code>), or by using <code>my_settings.smart_setup()</code>, which uses recommended models based on the API keys and/or models set in the <code>.env</code> file, prioritising Gemini 2.0 Flash for the base and complex model. See the reference page for more details.</p> <p>The third alternative: you can set everything in advance via creating a <code>.env</code> file in the root directory of your working directory, including the models, providers, and api keys. For example:</p> <pre><code>BASE_MODEL=gpt-4.1-mini\nBASE_PROVIDER=openai\nCOMPLEX_MODEL=gpt-4.1\nCOMPLEX_PROVIDER=anthropic\nOPENAI_API_KEY=... # replace with your OpenAI API key\n</code></pre> <p>Then, the global <code>settings</code> object will always use these values, and the <code>smart_setup()</code> or <code>my_settings.smart_setup()</code> (local settings object) will use these models and providers instead of the recommended ones.</p>"},{"location":"setting_up/#local-model-integration-via-ollama","title":"Local Model Integration via Ollama","text":"<p>First, make sure your Ollama server is running either via the Ollama app or <code>ollama run &lt;model_name&gt;</code>. E.g., <code>ollama run gpt-oss:20b</code>, which we'll use in this example. Within Python, you can configure your model API base to your Ollama api endpoint (default to <code>http://localhost:11434</code>) via the <code>model_api_base</code> parameter of <code>configure</code>.</p> <pre><code>from elysia import configure\nconfigure(\n    base_provider=\"ollama\",\n    complex_provider=\"ollama\",\n    base_model=\"gpt-oss:20b\",\n    complex_model=\"gpt-oss:20b\",\n    model_api_base=\"http://localhost:11434\",\n)\n</code></pre> <p>On the app side, this is configurable via the 'Api Base URL' parameter in the Settings. Set both of your providers to <code>ollama</code>, and your base and complex model to whatever model you are currently hosting, and this should work out-of-the-box.</p> <p>Warning: Elysia uses a long context, quite long context, due to the nature of the collection schemas, environment and more being included in every prompt. So these models will run quite slowly. However, on the backend, you can configure this to be faster by disabling connection to your Weaviate cluster, if applicable, by removing your weaviate api key and url. There is an optional setting <pre><code>settings.configure(\n    base_use_reasoning=False,\n    complex_use_reasoning=False\n)\n</code></pre> which will remove chain of thought prompting for the base and complex model, respectively. Use this with caution though, as it will degrade accuracy significantly. Additionally, some smaller models struggle with the complex nature of multiple outputs in DSPy and Elysia, so you might encounter some errors. In testing, the <code>gpt-oss</code> models work relatively well.</p> <p>Note: Simplifying model outputs and reducing the context window size for local models is planned for a future version of Elysia. Stay tuned!</p>"},{"location":"setting_up/#weaviate-integration","title":"Weaviate Integration","text":""},{"location":"setting_up/#weaviate-cloud","title":"Weaviate Cloud","text":"<p>To use Elysia with Weaviate cloud, you need to specify your Weaviate cluster details. These can be set via the Weaviate Cluster URL (<code>WCD_URL</code>) and the Weaviate Cluster API Key (<code>WCD_API_KEY</code>). To set these values, you can use <code>configure</code> on the settings: <pre><code>from elysia import configure\nconfigure(\n    wcd_url=..., # replace with your WCD_URL\n    wcd_api_key=... # replace with your WCD_API_KEY\n)\n</code></pre> or by setting them as environment variables <pre><code>WCD_URL=... # replace with your WCD_URL\nWCD_API_KEY=... # replace with your WCD_API_KEY\n</code></pre></p> <p>You can sign up for a 14-day sandbox to Weaviate cloud for free.</p>"},{"location":"setting_up/#local-weaviate","title":"Local Weaviate","text":"<p>You can run Elysia with a locally running Weaviate (e.g. Docker), making Elysia able to be run with completely open source software. To do so, you only need to set your local Weaviate instance variables. Configure Elysia to use the local instance by setting in the <code>.env</code> file:</p> <pre><code>WEAVIATE_IS_LOCAL=True\n\n# URL can be just a host or full URL; defaults shown below\nWCD_URL=localhost            # or http://localhost:8080\nLOCAL_WEAVIATE_PORT=8080     # optional override\nLOCAL_WEAVIATE_GRPC_PORT=50051  # optional override\n\n# No API key required for local unless you enabled local auth\nWCD_API_KEY=\n</code></pre> <p>Or within Python via:</p> <pre><code>from elysia import configure\nconfigure(\n    weaviate_is_local=True,\n    wcd_url=\"http://localhost:8080\",  # or \"localhost\"\n    local_weaviate_port=8080,\n    local_weaviate_grpc_port=50051,\n)\n</code></pre> <p>Notes: - If <code>WEAVIATE_IS_LOCAL=True</code> and no URL is provided, Elysia defaults to <code>localhost</code> with ports shown above. - Local mode can work without an API key; if you enable auth locally, set <code>WCD_API_KEY</code> accordingly.</p> <p>The easiest way to set up a local Weaviate instance is via Docker, see here for detailed instructions.</p> <p>Additionally, you need to preprocess your collections for Elysia to use the built in Weaviate-based tools, see below for details.</p> <p>Note: Using a local Weaviate instance is experimental. If you run into any issues, please open a Github Issue!</p>"},{"location":"setting_up/#preprocessing-collections","title":"Preprocessing Collections","text":"<p>The <code>preprocess</code> function must be used on the Weaviate collections you plan to use within Elysia. </p> <pre><code>from elysia import preprocess\npreprocess(\"&lt;your_collection_name&gt;\")\n</code></pre> <p>Preprocessing does several things:</p> <ul> <li>Creates an LLM generated summary of the collection, including descriptions of the fields in the dataset.</li> <li>Creates 'mappings', so that fields in the collection can be mapped to frontend-specific fields. This enables the Elysia frontend app to display items from the collection when retrieved in the app.</li> <li>Calculates summary statistics, such as the mean, maximum and minimum values of number fields, as well as statistics for other fields.</li> <li>Collects other metadata such as any named vectors, what index types are used, if the inverted index is configured to index e.g. creation time.</li> </ul> <p>Since preprocessing uses LLM created summaries of the collections, you must configure your models in advance. See above for details.</p>"},{"location":"setting_up/#running-the-preprocessing-function","title":"Running the Preprocessing Function","text":"<p>You have access to two functions, <code>preprocess_async</code>, which must be awaited, and <code>preprocess</code>, which is a sync wrapper for its async sister. The basic arguments for either function are:</p> <ul> <li><code>collection_names</code> (list[str]): The names of the collections to preprocess.</li> <li><code>client_manager</code> (ClientManager): The client manager to use.     The ClientManager class is how Elysia interacts with Weaviate client.     If you are unsure of this, do not provide this argument, it will default to the Weaviate cluster you selected via the <code>Settings</code>, or via <code>configure</code>/environment variables.</li> </ul> <p>As well, the LLM requires a number of objects retrieved from the collection, at random, to help provide its summary. Since objects in collections vary greatly in token size (and hence LLM compute time/cost), you can adjust the following parameters to change how many objects are used for this sample.</p> <ul> <li><code>min_sample_size</code> (int): The minimum number of objects in the sample.</li> <li><code>max_sample_size</code> (int): The maximum number of objects to sample.</li> <li><code>num_sample_tokens</code> (int): The maximum number of tokens in the sample objects used to evaluate the summary.</li> </ul> <p>The <code>num_sample_tokens</code> parameter controls how many objects are actually used. Provided it is between <code>min_sample_size</code> and <code>max_sample_size</code>, the preprocessor will select the closest number of objects that are estimated to be in total <code>num_sample_tokens</code> tokens.</p> <p>Additionally, you have: - <code>settings</code> (Settings): The settings to use. - <code>force</code> (bool): Whether to force the preprocessor to run even if the collection already exists.</p>"},{"location":"setting_up/#additional-functions","title":"Additional Functions","text":"<p>You can also use <code>preprocessed_collection_exists</code>, which returns True/False if the collection has been preprocessed (and it can be accessed within the Weaviate cluster):</p> <p><pre><code>from elysia import preprocessed_collection_exists\npreprocessed_collection_exists(collection_name = ...)\n</code></pre> which returns True/False if the preprocess exists within this Weaviate cluster</p> <p>You can use <code>edit_preprocessed_collection</code> to update the values manually: <pre><code>from elysia import edit_preprocessed_collection\nproperties = edit_preprocessed_collection(\n    collection_name = ...,\n    named_vectors = ...,\n    summary = ...,\n    mappings = ...,\n    fields = ...\n)\n</code></pre> which will change the LLM generated values with manually input values. Any fields not provided will not be updated.</p> <p>You can use <code>delete_preprocessed_collection</code> which will delete the cached preprocessed metadata.</p> <p><pre><code>delete_preprocessed_collection(collection_name = ...) \n</code></pre> which permanently deletes the preprocessed collection (not the original collection). You will need to rerun preprocess for the original collection to be used for the Weaviate integration in Elysia again.</p>"},{"location":"API/","title":"Overview","text":"<p>Within the Elysia package, the API endpoints are included but are specific to the Elysia Frontend. </p> <p>However, there are a range of functionalities included that can be useful for using Elysia in an app environment.</p>"},{"location":"API/payload_formats/","title":"Payload Formats","text":"<p>Whenever a <code>Result</code> object is yielded from an Elysia tool or decision agent, two things happen:</p> <ol> <li>Any objects and metadata in the <code>Result</code> are automatically added to Elysia's Environment for future use in the decision tree.</li> <li>The <code>.to_frontend()</code> method of the <code>Result</code> parses the objects and metadata to a frontend-acceptable format and are yielded outside of the tree (to send payloads to a connected frontend).</li> </ol> <p>Similarly, an <code>Update</code> class also yields a payload outside of the tree, but does not add any objects to the environment.</p> <p>All payloads that are sent from the decision tree to the frontend have the same structure: <pre><code>{\n    \"type\": str, \n    \"id\": str,\n    \"user_id\": str,\n    \"conversation_id\": str, \n    \"query_id\": str,\n    \"payload\": dict\n}\n</code></pre></p> <p>Where the <code>\"payload\"</code> dictionary always contains:</p> <p><pre><code>{\n    \"type\": str, \n    \"metadata\": dict\n    \"objects\": list[dict],\n    ... # additional fields\n}\n</code></pre> where the additional fields are based on the <code>\"type\"</code> of the output. The dictionaries in the <code>list[dict]</code> of the <code>objects</code> is normally unique to each <code>\"type\"</code> that is returned, but will always include a <code>_REF_ID</code> field containing a unique identifier for its place in the Elysia environment. </p> <p>For example, any objects returned by the Elysia query tool will be mapped to specific fields that the frontend is 'aware' of. Items in the Weaviate collection that are returned are not known how to be displayed by the frontend as the fields are unique to the user's collection. So instead they are mapped to frontend-specific fields that are decided in advance by the preprocessing step before they are returned outside of the tree.</p> <ul> <li></li> </ul>"},{"location":"API/user_and_tree_managers/","title":"User Manager and Tree Manager","text":"<p>There exist two manager classes in Elysia designed to help structure and handle multiple users, each interacting with multiple decision trees. These are: - The TreeManager, which tracks and stores multiple decision trees. - The UserManager, which handles multiple <code>TreeManager</code>s per user, as well as a <code>[ClientManager](../Reference/Client.md)</code> object for each user.</p>"},{"location":"API/user_and_tree_managers/#treemanager-overview","title":"TreeManager Overview","text":"<p>The <code>TreeManager</code> is initialised with a <code>user_id</code>, which is the identifier for the user this manager is responsible for.</p> <p>The <code>TreeManager</code> includes configs options which is shared amongst all default trees in the <code>TreeManager</code>, which is created at initialisation of the object. Since each <code>TreeManager</code> instance is unique to each user, this is intended to be the default user configuration for a specific user. These options are </p> <ul> <li><code>style</code> - a string defining the 'style' passed to each tree. This can be used to customize the behaviour of the writing of the LLMs within the decision tree.</li> <li><code>agent_description</code> - A description of the agent that will be used in the tree. This helps define the agent's capabilities and behaviour.</li> <li><code>end_goal</code> - The ultimate objective or goal that the tree should work towards achieving, how it decides when the decision tree ends.</li> <li><code>branch_initialisation</code> - What tools are initialised as a default configuration of the tree.</li> <li><code>settings</code> - Optional configuration settings, see the Settings reference page.</li> </ul> <p>The class can also be initialised with <code>tree_timeout</code>, which controls the length of time before a particular conversation is cleaned up. See below for more details.</p> <p>Methods within <code>TreeManager</code> include:</p> <ul> <li><code>add_tree</code> - create a tree object with a <code>conversation_id</code> parameter, and optionally pass unique configuration options for that specific tree (which will not be overrided if the default <code>settings</code> object in the <code>TreeManager</code> is changed).</li> <li><code>configure</code> - a wrapper for the <code>configure</code> method for the <code>TreeManager</code>'s user <code>settings</code> object.</li> <li><code>process_tree</code> - an async function which runs the tree initialised with a <code>conversation_id</code> for a given <code>query</code> (user prompt).</li> <li><code>check_all_trees_timeout</code> - checks if any trees have been active for longer than <code>tree_timeout</code>, and removes them from the <code>TreeManager</code> if so.</li> </ul> <p>For a complete view of all the methods, see the reference page.</p>"},{"location":"API/user_and_tree_managers/#usermanager-overview","title":"UserManager Overview","text":"<p>The <code>UserManager</code> manages both a <code>TreeManager</code> and a <code>ClientManager</code> per user. Essentially, it contains a <code>users</code> dictionary which is keyed by different <code>user_id</code>s and has these managers for each user.</p> <p>It has initialisations: - <code>user_timeout</code> - controls how many minutes a user needs to be inactive before getting timed out (if you run the <code>check_all_users_timeout</code> method) - <code>tree_timeout</code> - initialisation parameter passed down to all <code>TreeManager</code> instances for each user. - <code>client_timeout</code> - initialisation parameter passed down to all <code>ClientManager</code> instances for each user.</p> <p>It has methods such as:</p> <ul> <li><code>add_user_local</code> - creates a user as well as their <code>TreeManager</code> and <code>ClientManager</code>. Can pass configuration options to create the user with these default configurations.</li> <li><code>get_user_local</code> - retrieve a user from the users dictionary, will raise an error if there is no user with that ID.</li> <li><code>initialise_tree</code> - create a tree within the tree manager for a particular user.</li> <li><code>process_tree</code> - runs the tree for a given <code>user_id</code> and <code>conversation_id</code> (tree within that user). Automatically sends error payloads if the user or tree has been timed out.</li> <li><code>check_all_users_timeout</code> - loop over all users stored in the user manager and removes any that have timed out.</li> <li><code>check_all_trees_timeout</code> - loop over all trees for all users in the user manager and removes any that have timed out.</li> <li><code>check_restart_clients</code> - loop over all clients for all users and call the <code>restart_client</code> functions, which only restarts clients if they have exceeded the <code>client_timeout</code> parameter since they were last used.</li> </ul> <p>For a complete view of all the methods, see the reference page.</p>"},{"location":"API/user_and_tree_managers/#timeouts","title":"Timeouts","text":"<p>If you're using the UserManager, you can set a scheduled <code>user_manager.check_all_trees_timeout()</code>, <code>user_manager.check_all_users_timeout()</code> and/or <code>user_manager.check_restart_clients()</code>; which will remove empty trees/users if they've been inactive for a period of time, or restart the Weaviate clients if they are also inactive. These time periods can be configured on initialisation of the UserManager, i.e.</p> <pre><code>UserManager(tree_timeout: datetime.timedelta | int, user_timeout: datetime.timedelta | int, client_timeout: datetime.timedelta | int))\n</code></pre> <p>This defaults to <code>TREE_TIMEOUT</code>, <code>USER_TIMEOUT</code> and <code>CLIENT_TIMEOUT</code> respectively in the environment variables if not set (in minutes), which itself defaults to 10 minutes. If they are set to 0, then no users/trees/clients will be restart. Not restarting the clients is not recommended.</p> <p>If you're using a TreeManager only (and not a <code>UserManager</code>), you can do the same with <code>tree_manager.check_all_trees_timeout()</code>, with same defaults.</p> <p>If you're only using the [ClientManager], you can call <code>restart_client</code> and <code>restart_async_client</code>, which automatically checks and restarts the clients individually if they have passed the <code>client_timeout</code> threshold.</p>"},{"location":"API/user_and_tree_managers/#example-scheduler-with-fastapi-lifespan","title":"Example Scheduler with FastAPI lifespan","text":"<p>For example, if using FastAPI, you can set an automatic scheduler such as:</p> <p><pre><code>from fastapi import FastAPI\nfrom apscheduler.schedulers.asyncio import AsyncIOScheduler\n\nasync def check_user_timeouts():\n    user_manager = get_user_manager()\n    await user_manager.check_all_users_timeout()\n\nasync def check_tree_timeouts():\n    user_manager = get_user_manager()\n    await user_manager.check_all_trees_timeout()\n\nasync def check_restart_clients():\n    user_manager = get_user_manager()\n    await user_manager.check_restart_clients()\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    user_manager = get_user_manager()\n\n    scheduler = AsyncIOScheduler()\n    scheduler.add_job(check_tree_timeouts, \"interval\", seconds=23)\n    scheduler.add_job(check_user_timeouts, \"interval\", seconds=29)\n    scheduler.add_job(check_restart_clients, \"interval\", seconds=31)\n\n    scheduler.start()\n    yield\n    scheduler.shutdown()\n\n    await user_manager.close_all_clients()\n</code></pre> Where the <code>get_user_manager()</code> function returns a globally defined <code>UserManager</code> (and doesn't create a new one when it's called).</p> <p>This automatically runs the functions in the user manager every 23, 29 and 31 seconds, respectively.</p>"},{"location":"Advanced/","title":"Customising Elysia Overview","text":"<p>If you haven't already, you should read the basic usage guide and the advanced usage guide (which details exactly how to specify custom agent descriptions and styles).</p> <p>There are many ways you can make Elysia into a completely custom app with its own goals and tools. This section will detail exactly how you can create tools, how to interact with the Elysia decision tree's objects and to create your own.</p> <ul> <li>Technical Overview</li> <li>How to create your own tools</li> <li>Tool Discovery and Initialization</li> <li>How to interact with the Elysia Environment</li> <li>How to create your own objects</li> <li>Elysia MCTS Implementation</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/","title":"Elysia MCTS: Optimization Caveats for Agentic AI Engineers","text":"<p>Purpose: Actionable guidance for optimizing execution trees at the DSPy optimizer level (few-shot compilation, LM switching) and Elysia level (tree structure, context injection, prompt engineering). Concise, grounded snippets only.</p>"},{"location":"Advanced/ELYSIA_MCTS/#two-optimization-layers","title":"Two optimization layers","text":"<ol> <li>DSPy layer: Example retrieval/compilation (<code>LabeledFewShot</code>, <code>k=10</code>), LM selection (<code>base_lm</code> vs <code>complex_lm</code>), fallback behavior</li> <li>Elysia layer: Tree structure (<code>_get_successive_actions</code>), prompt templates (<code>DecisionPrompt</code>), context injection (<code>ElysiaChainOfThought</code>), tool gating (<code>is_tool_available</code>), termination logic</li> </ol>"},{"location":"Advanced/ELYSIA_MCTS/#elysia-layer-tree-structure-prompt-engineering-for-quantitative-customization","title":"Elysia Layer: Tree Structure &amp; Prompt Engineering for Quantitative Customization","text":""},{"location":"Advanced/ELYSIA_MCTS/#1-exploration-horizon-via-_get_successive_actions-structure-only","title":"1. Exploration horizon via <code>_get_successive_actions</code> (structure-only)","text":"<p>Location: <code>elysia/tree/tree.py</code></p> <pre><code>def _get_successive_actions(self, successive_actions: dict, current_options: dict) -&gt; dict:\n    for branch in current_options:\n        successive_actions[branch] = {}\n        if current_options[branch][\"options\"] != {}:\n            successive_actions[branch] = self._get_successive_actions(\n                successive_actions[branch], current_options[branch][\"options\"]\n            )\n    return successive_actions\n</code></pre> <p>What it does: Recursively maps the tree shape (branch \u2192 sub-branches) and injects it into <code>DecisionPrompt.successive_actions</code> as context.</p> <p>Caveats for quantitative optimization: - No intrinsic scoring: This is pure structure\u2014no UCB, visit counts, or value estimates. The LLM sees only the topology. - Token cost scales with depth: Deep/wide trees inflate prompt size without adding decision signal. - Customization lever: Modify tree construction (<code>.add_branch()</code>, <code>.add_tool()</code>) to encode domain-specific structure. Example: for aggregation-heavy domains, create a shallow \"aggregate\" branch with many leaf tools; for exploration-heavy domains, create deeper chains with conditional gating.</p> <p>Domain-specific structural prompting: <pre><code># Example: Quantitative finance domain\ntree.add_branch(root=True, branch_id=\"base\", instruction=\"Choose: data retrieval, quantitative analysis, or reporting\")\ntree.add_branch(branch_id=\"quant_analysis\", from_branch_id=\"base\", \n    instruction=\"Select statistical method: correlation, regression, time-series, or risk metrics\")\ntree.add_tool(branch_id=\"quant_analysis\", tool=CorrelationTool)\ntree.add_tool(branch_id=\"quant_analysis\", tool=RegressionTool)\n# Result: successive_actions shows {\"quant_analysis\": {\"correlation\": {}, \"regression\": {}}} \n# \u2192 LLM sees quantitative structure explicitly\n</code></pre></p>"},{"location":"Advanced/ELYSIA_MCTS/#2-prompt-template-engineering-decisionprompt-for-explainable-quantitative-decisions","title":"2. Prompt template engineering: <code>DecisionPrompt</code> for explainable, quantitative decisions","text":"<p>Location: <code>elysia/tree/prompt_templates.py</code> (L5-L144)</p> <p>Key input fields for customization:</p> <pre><code>class DecisionPrompt(dspy.Signature):\n    instruction: str = dspy.InputField(\n        description=\"Specific guidance for this decision point that must be followed\"\n    )\n\n    tree_count: str = dspy.InputField(\n        description=\"Current attempt number as 'X/Y' where X=current, Y=max. Consider ending as X approaches Y.\"\n    )\n\n    available_actions: list[dict] = dspy.InputField(\n        description=\"List of possible actions: {name: {function_name, description, inputs}}\"\n    )\n\n    unavailable_actions: list[dict] = dspy.InputField(\n        description=\"Actions unavailable now: {name: {function_name, available_at}}\"\n    )\n\n    successive_actions: str = dspy.InputField(\n        description=\"Actions that stem from current actions (nested dict structure)\"\n    )\n\n    previous_errors: list[dict] = dspy.InputField(\n        description=\"Errors from previous actions, organized by function_name\"\n    )\n</code></pre> <p>Quantitative customization strategies:</p> <ol> <li> <p>Inject scoring/metrics into <code>instruction</code>:    <pre><code>decision_node = DecisionNode(\n    instruction=\"\"\"Choose action with highest expected information gain.\n    Priority: aggregate (cost=1, gain=high) &gt; query (cost=3, gain=medium) &gt; text (cost=0, gain=low).\n    Current budget: 5 units.\"\"\"\n)\n</code></pre></p> </li> <li> <p>Encode tool metadata in <code>available_actions</code> descriptions:    <pre><code>tool.description = \"\"\"Correlation analysis tool. \nComplexity: O(n\u00b2). Typical runtime: 2s for n=1000. \nOutput: correlation matrix + p-values. \nBest for: identifying linear relationships in numerical data.\"\"\"\n</code></pre></p> </li> <li> <p>Use <code>tree_count</code> for adaptive strategies:    <pre><code># In DecisionPrompt docstring or instruction:\n\"\"\"If tree_count shows X/Y where X &gt; Y*0.8, prioritize low-cost actions or termination.\"\"\"\n</code></pre></p> </li> <li> <p>Leverage <code>unavailable_actions.available_at</code> for conditional logic:    <pre><code># In tool's is_tool_available() docstring:\n\"\"\"Available after: 1) data retrieved, 2) schema validated, 3) min 100 rows present.\"\"\"\n# \u2192 LLM sees explicit prerequisites for quantitative workflows\n</code></pre></p> </li> </ol> <p>Output fields for explainability:</p> <pre><code>function_name: str = dspy.OutputField(\n    description=\"Select exactly one function name from available_actions...\"\n)\n\nfunction_inputs: dict[str, Any] = dspy.OutputField(\n    description=\"Inputs for selected function. Must match available_actions[function_name]['inputs'].\"\n)\n\nend_actions: bool = dspy.OutputField(\n    description=\"Has end_goal been achieved? Set True to terminate after this action.\"\n)\n</code></pre> <p>Caveat: <code>DecisionPrompt</code> has no built-in <code>reasoning</code> or <code>confidence</code> output by default. To add explainability:</p> <pre><code># Modify ElysiaChainOfThought initialization:\ndecision_module = ElysiaChainOfThought(\n    DecisionPrompt,\n    tree_data=tree_data,\n    reasoning=True,  # \u2190 Adds reasoning: str output field\n    ...\n)\n# Now output.reasoning contains step-by-step justification\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#3-context-injection-control-elysiachainofthought-for-token-efficiency","title":"3. Context injection control: <code>ElysiaChainOfThought</code> for token efficiency","text":"<p>Location: <code>elysia/util/elysia_chain_of_thought.py</code></p> <p>Optional context inputs (turn on deliberately):</p> <pre><code>ElysiaChainOfThought(\n    signature=DecisionPrompt,\n    tree_data=tree_data,\n    environment=True,           # \u2190 Retrieved objects from prior actions\n    collection_schemas=True,    # \u2190 DB schemas (EXPENSIVE)\n    tasks_completed=True,       # \u2190 Chain-of-thought history\n    reasoning=True,             # \u2190 Step-by-step output\n    message_update=True,        # \u2190 User-facing status\n)\n</code></pre> <p>Token control strategies:</p> <pre><code># Narrow schemas to specific collections:\nElysiaChainOfThought(..., collection_schemas=True, collection_names=[\"trades\", \"prices\"])\n\n# Implementation (elysia_chain_of_thought.py L328-338):\nif self.collection_schemas:\n    if self.collection_names != []:\n        kwargs[\"collection_schemas\"] = self.tree_data.output_collection_metadata(\n            collection_names=self.collection_names, with_mappings=False\n        )\n    else:\n        kwargs[\"collection_schemas\"] = self.tree_data.output_collection_metadata(with_mappings=False)\n</code></pre> <p>Quantitative domain example: <pre><code># For financial analysis: only inject schemas when choosing data tools\nif current_branch == \"data_retrieval\":\n    decision_module = ElysiaChainOfThought(..., collection_schemas=True, collection_names=[\"market_data\"])\nelse:\n    decision_module = ElysiaChainOfThought(..., collection_schemas=False)  # Save tokens\n</code></pre></p>"},{"location":"Advanced/ELYSIA_MCTS/#4-tool-availability-gating-for-conditional-workflows","title":"4. Tool availability gating for conditional workflows","text":"<p>Location: <code>elysia/tree/tree.py</code>, <code>Tree._get_available_tools()</code></p> <pre><code>async def _get_available_tools(self, current_decision_node, client_manager):\n    available_tools = []\n    unavailable_tools = []\n    for tool in current_decision_node.options.keys():\n        if \"is_tool_available\" in dir(self.tools[tool]) and await self.tools[tool].is_tool_available(\n            tree_data=self.tree_data, base_lm=self.base_lm, complex_lm=self.complex_lm, client_manager=client_manager\n        ):\n            available_tools.append(tool)\n        else:\n            is_tool_available_doc = self.tools[tool].is_tool_available.__doc__.strip() if ... else \"\"\n            unavailable_tools.append((tool, is_tool_available_doc))\n    return available_tools, unavailable_tools\n</code></pre> <p>Quantitative gating example:</p> <pre><code>class RegressionTool(Tool):\n    async def is_tool_available(self, tree_data, **kwargs):\n        \"\"\"Available when: min 30 data points, 2+ numerical columns, no missing values &gt; 10%.\"\"\"\n        if \"data\" not in tree_data.environment.environment:\n            return False\n        data = tree_data.environment.environment[\"data\"]\n        return len(data.objects) &gt;= 30 and data.metadata.get(\"numerical_cols\", 0) &gt;= 2\n</code></pre> <p>Caveat: The docstring is surfaced to the LLM as <code>unavailable_actions[tool][\"available_at\"]</code>. Make it actionable: - \u274c Bad: <code>\"Not available yet\"</code> - \u2705 Good: <code>\"Available after retrieving \u226530 rows with \u22652 numerical columns\"</code></p>"},{"location":"Advanced/ELYSIA_MCTS/#dspy-layer-optimizer-lm-switching","title":"DSPy Layer: Optimizer &amp; LM Switching","text":""},{"location":"Advanced/ELYSIA_MCTS/#1-few-shot-compilation-with-labeledfewshot","title":"1. Few-shot compilation with <code>LabeledFewShot</code>","text":"<p>Location: <code>elysia/util/elysia_chain_of_thought.py</code>, <code>aforward_with_feedback_examples()</code></p> <pre><code>examples, uuids = await retrieve_feedback(client_manager, self.tree_data.user_prompt, feedback_model, n=10)\nif len(examples) &gt; 0:\n    optimizer = dspy.LabeledFewShot(k=10)  # \u2190 Fixed k=10\n    optimized_module = optimizer.compile(self, trainset=examples)\nelse:\n    return await self.aforward(lm=complex_lm, **kwargs)  # \u2190 Fallback: no examples \u2192 complex LM\n\n# LM selection by example count:\nif len(examples) &lt; num_base_lm_examples:  # default 3\n    return await optimized_module.aforward(lm=complex_lm, **kwargs)\nelse:\n    return await optimized_module.aforward(lm=base_lm, **kwargs)\n</code></pre> <p>Caveats: - <code>k=10</code> is hardcoded; modify source to tune. - <code>num_base_lm_examples=3</code> threshold: &lt;3 examples \u2192 use complex LM (more capable), \u22653 \u2192 use base LM (faster/cheaper). - No examples \u2192 skip compilation, use complex LM.</p> <p>Quantitative tuning: - For high-stakes decisions (e.g., financial trades): set <code>num_base_lm_examples=10</code> to always use complex LM. - For low-stakes (e.g., data filtering): set <code>num_base_lm_examples=1</code> to prefer base LM.</p>"},{"location":"Advanced/ELYSIA_MCTS/#2-feedback-retrieval-behavior","title":"2. Feedback retrieval behavior","text":"<p>Location: <code>elysia/util/retrieve_feedback.py</code></p> <pre><code>if not await client.collections.exists(\"ELYSIA_FEEDBACK__\"):\n    return [], []  # \u2190 No collection \u2192 no examples\n\nsuperpositive = await feedback_collection.query.near_text(\n    query=user_prompt, filters=Filter(..., feedback==2.0), certainty=0.7, limit=n\n)\nif len(superpositive.objects) &lt; n:\n    positive = await feedback_collection.query.near_text(\n        query=user_prompt, filters=Filter(..., feedback==1.0), certainty=0.7, limit=n\n    )\n    feedback_objects = superpositive.objects + positive.objects[:(n - len(superpositive.objects))]\n\nrandom.shuffle(relevant_updates)  # \u2190 Introduces variability\nrelevant_updates = relevant_updates[:n]\n</code></pre> <p>Caveats: - Requires <code>ELYSIA_FEEDBACK__</code> collection in Weaviate. - <code>certainty=0.7</code> threshold: lower \u2192 more examples (noisier), higher \u2192 fewer examples (stricter). - Random shuffle \u2192 run-to-run variability in example selection.</p> <p>Quantitative optimization: - For deterministic behavior: remove <code>random.shuffle()</code> or seed it. - For domain-specific retrieval: add metadata filters (e.g., <code>Filter.by_property(\"domain\").equal(\"finance\")</code>).</p>"},{"location":"Advanced/ELYSIA_MCTS/#3-assertion-retry-logic","title":"3. Assertion &amp; retry logic","text":"<p>Location: <code>elysia/tree/util.py</code>, <code>AssertedModule</code></p> <pre><code>class AssertedModule(dspy.Module):\n    def __init__(self, module, assertion: Callable, max_tries: int = 3):\n        self.assertion = assertion  # (kwargs, pred) \u2192 (bool, feedback_str)\n        self.max_tries = max_tries\n\n    async def aforward(self, **kwargs):\n        pred = await self.module.acall(**kwargs)\n        num_tries = 0\n        asserted, feedback = self.assertion(kwargs, pred)\n\n        while not asserted and num_tries &lt;= self.max_tries:\n            asserted_module = self.modify_signature_on_feedback(pred, feedback)\n            pred = await asserted_module.aforward(\n                previous_feedbacks=self.previous_feedbacks,\n                previous_attempts=self.previous_attempts,\n                **kwargs\n            )\n            asserted, feedback = self.assertion(kwargs, pred)\n            num_tries += 1\n        return pred\n</code></pre> <p>Quantitative customization:</p> <pre><code># Example: Strict assertion for tool selection\ndef _tool_assertion(kwargs, pred):\n    valid = pred.function_name in self.options\n    feedback = f\"Must choose from {list(self.options.keys())}\" if not valid else \"\"\n    return valid, feedback\n\ndecision_executor = AssertedModule(decision_module, assertion=_tool_assertion, max_tries=2)\n</code></pre> <p>Caveat: Keep <code>max_tries</code> low (1-3) to avoid token/cost explosion. Each retry adds previous attempts to context.</p>"},{"location":"Advanced/ELYSIA_MCTS/#quantitative-optimization-checklist","title":"Quantitative Optimization Checklist","text":""},{"location":"Advanced/ELYSIA_MCTS/#tree-structure","title":"Tree structure","text":"<ul> <li>\u2705 Encode domain logic in branch hierarchy (shallow for aggregation, deep for sequential workflows)</li> <li>\u2705 Keep <code>_get_successive_actions</code> output shallow to minimize tokens</li> <li>\u2705 Use <code>.add_branch(instruction=...)</code> to inject quantitative guidance (costs, priorities, constraints)</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#prompt-engineering","title":"Prompt engineering","text":"<ul> <li>\u2705 Inject scoring/metrics into <code>DecisionPrompt.instruction</code></li> <li>\u2705 Add complexity/runtime metadata to tool descriptions</li> <li>\u2705 Use <code>tree_count</code> for adaptive strategies (e.g., \"if X &gt; 0.8*Y, prefer termination\")</li> <li>\u2705 Write actionable <code>is_tool_available()</code> docstrings (surfaced as <code>available_at</code>)</li> <li>\u2705 Enable <code>reasoning=True</code> for explainability</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#context-injection","title":"Context injection","text":"<ul> <li>\u2705 Enable <code>collection_schemas</code> only when needed; constrain via <code>collection_names</code></li> <li>\u2705 Use <code>tasks_completed=True</code> to avoid repeats (adds tokens but prevents loops)</li> <li>\u2705 Disable <code>environment=True</code> for stateless decisions</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#dspy-optimization","title":"DSPy optimization","text":"<ul> <li>\u2705 Ensure <code>ELYSIA_FEEDBACK__</code> exists before enabling <code>USE_FEEDBACK</code></li> <li>\u2705 Tune <code>num_base_lm_examples</code> threshold (default 3) for cost/quality tradeoff</li> <li>\u2705 Modify <code>k=10</code> in source if domain needs more/fewer examples</li> <li>\u2705 Set <code>max_tries</code> conservatively (1-3) in <code>AssertedModule</code></li> <li>\u2705 Remove <code>random.shuffle()</code> in <code>retrieve_feedback.py</code> for determinism</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#observability","title":"Observability","text":"<ul> <li>\u2705 Use <code>Tree.log_token_usage()</code> to measure impact of schemas/reasoning</li> <li>\u2705 Monitor <code>tree_count</code> to detect loops</li> <li>\u2705 Track <code>previous_errors</code> to identify systematic failures</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#architecture-overview","title":"Architecture Overview","text":""},{"location":"Advanced/ELYSIA_MCTS/#mcts-architecture-diagram","title":"MCTS Architecture Diagram","text":"<p>The following diagram illustrates Elysia's MCTS implementation with integrated DSPy components and feedback mechanisms:</p> <p></p> <p>Note: View the mermaid diagram source or PNG version</p>"},{"location":"Advanced/ELYSIA_MCTS/#diagram-components","title":"Diagram Components","text":""},{"location":"Advanced/ELYSIA_MCTS/#dspy-framework-components-blue","title":"DSPy Framework Components (Blue)","text":"<ul> <li><code>dspy.Module</code>: Base class for all reasoning modules (ElysiaChainOfThought)</li> <li><code>dspy.Signature</code>: Defines input/output structure (DecisionPrompt)</li> <li><code>dspy.LabeledFewShot</code>: Optimizer that compiles modules with labeled examples</li> <li><code>dspy.Prediction</code>: Structured LLM output predictions</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#feedback-quality-mechanisms-orange","title":"Feedback &amp; Quality Mechanisms (Orange)","text":"<ul> <li>AssertedModule: Validates decisions with custom assertion functions, implements retry logic</li> <li>CopiedModule: Integrates historical feedback from failed attempts into prompts</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#filtering-retrieval-gray","title":"Filtering &amp; Retrieval (Gray)","text":"<ul> <li>retrieve_feedback: Queries Weaviate database for similar past examples using vector similarity (threshold: 0.7)</li> <li>Filters examples by relevance and retrieves top-k (n=10) for few-shot learning</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#summary-aggregation-green","title":"Summary &amp; Aggregation (Green)","text":"<ul> <li>Aggregate Tool: Provides summary statistics (count, sum, average) on data collections</li> <li>Applies filters and grouping for data summarization</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#training-storage-pink","title":"Training &amp; Storage (Pink)","text":"<ul> <li>TrainingUpdate: Stores decision outcomes and tool results</li> <li>Weaviate DB: Persistent storage for feedback examples and training data</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#core-mcts-components","title":"Core MCTS Components","text":""},{"location":"Advanced/ELYSIA_MCTS/#1-selectionchoice-mechanism","title":"1. Selection/Choice Mechanism","text":"<p>Primary Module: <code>elysia.tree.util.DecisionNode</code> Full Path: <code>/elysia/tree/util.py</code> (lines 218-502)</p> <p>The DecisionNode class implements the selection phase of MCTS, choosing the most promising action from available options.</p> <p>Key Components: - Decision Making: Uses DSPy-based <code>ElysiaChainOfThought</code> with <code>DecisionPrompt</code> signature - Prompt Template: <code>elysia.tree.prompt_templates.DecisionPrompt</code> (<code>/elysia/tree/prompt_templates.py</code> lines 5-130) - Choice Logic: <code>__call__</code> method evaluates available tools and makes decisions</p> <pre><code># Core decision-making process\ndecision_module = ElysiaChainOfThought(\n    DecisionPrompt,\n    tree_data=tree_data,\n    environment=True,\n    collection_schemas=self.use_elysia_collections,\n    tasks_completed=True,\n    message_update=True,\n    reasoning=tree_data.settings.BASE_USE_REASONING,\n)\n\ndecision_executor = AssertedModule(\n    decision_module,\n    assertion=self._tool_assertion,\n    max_tries=2,\n)\n</code></pre> <p>DSPy Module: <code>ElysiaChainOfThought</code> extends <code>dspy.Module</code> Full Path: <code>/elysia/util/elysia_chain_of_thought.py</code> (lines 24-421)</p>"},{"location":"Advanced/ELYSIA_MCTS/#2-exploration-factor","title":"2. Exploration Factor","text":"<p>Primary Module: <code>elysia.tree.tree.Tree._get_successive_actions()</code> Full Path: <code>/elysia/tree/tree.py</code> (method within Tree class)</p> <p>The exploration mechanism evaluates future possible actions to inform current decisions.</p> <p>Key Components: - Successive Actions: Maps current actions to their potential follow-up actions - Available Tools Evaluation: <code>_get_available_tools()</code> method determines which actions are currently possible - Tool Availability Rules: Each tool can implement <code>is_tool_available()</code> and <code>run_if_true()</code> methods</p> <pre><code># Exploration through successive actions\nsuccessive_actions = self._get_successive_actions(\n    successive_actions={},\n    current_options=init_options,\n)\n\n# Decision considers future possibilities\navailable_actions: list[dict] = dspy.InputField(\n    description=\"List of possible actions to choose from for this task only\"\n)\nsuccessive_actions: str = dspy.InputField(\n    description=\"Actions that stem from actions you can choose from\"\n)\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#3-evaluation-mechanism","title":"3. Evaluation Mechanism","text":"<p>Primary Module: <code>elysia.tree.util.AssertedModule</code> Full Path: <code>/elysia/tree/util.py</code> (lines 153-215)</p> <p>The evaluation phase ensures decisions meet quality criteria through assertion-based feedback loops.</p> <p>Key Components: - Assertion Function: Custom validation logic for each decision - Retry Mechanism: Automatic retry with feedback when assertions fail - Feedback Integration: Previous failures inform subsequent attempts</p> <pre><code>class AssertedModule(dspy.Module):\n    \"\"\"\n    A module that calls another module until it passes an assertion function.\n    This function returns a tuple of (asserted, feedback).\n    If the assertion is false, the module is called again with the previous feedbacks and attempts.\n    \"\"\"\n\n    def __init__(\n        self,\n        module: ElysiaChainOfThought,\n        assertion: Callable[[dict, dspy.Prediction], tuple[bool, str]],\n        max_tries: int = 3,\n    ):\n        self.assertion = assertion\n        self.module = module\n        self.max_tries = max_tries\n        self.previous_feedbacks = []\n        self.previous_attempts = []\n</code></pre> <p>DSPy Integration: Uses <code>dspy.Module</code> base class with custom assertion logic</p>"},{"location":"Advanced/ELYSIA_MCTS/#4-back-propagationfeedback-mechanism","title":"4. Back Propagation/Feedback Mechanism","text":"<p>Primary Module: <code>elysia.tree.util.CopiedModule</code> Full Path: <code>/elysia/tree/util.py</code> (lines 77-152)</p> <p>The back propagation mechanism updates the system based on previous decisions and their outcomes.</p> <p>Key Components:</p>"},{"location":"Advanced/ELYSIA_MCTS/#a-training-updates","title":"A. Training Updates","text":"<p>Module: <code>elysia.util.objects.TrainingUpdate</code> Purpose: Stores decision outcomes for learning</p> <pre><code>results = [\n    TrainingUpdate(\n        module_name=\"decision\",\n        inputs=tree_data.to_json(),\n        outputs={k: v for k, v in output.__dict__[\"_store\"].items()},\n    ),\n    Status(str(self.options[output.function_name][\"status\"])),\n]\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#b-feedback-retrieval","title":"B. Feedback Retrieval","text":"<p>Module: <code>elysia.util.retrieve_feedback.retrieve_feedback</code> Full Path: <code>/elysia/util/retrieve_feedback.py</code> (lines 8-92)</p> <p>Retrieves similar examples from the feedback database for few-shot learning:</p> <pre><code>async def retrieve_feedback(\n    client_manager: ClientManager, \n    user_prompt: str, \n    model: str, \n    n: int = 6\n) -&gt; tuple[list[dspy.Example], list[str]]:\n    \"\"\"\n    Retrieve similar examples from the database.\n    \"\"\"\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#c-contextual-feedback-integration","title":"C. Contextual Feedback Integration","text":"<p>Module: <code>elysia.tree.util.CopiedModule</code></p> <p>Integrates previous failed attempts into new decision attempts:</p> <pre><code>class CopiedModule(dspy.Module):\n    \"\"\"\n    A module that copies another module and adds a previous_feedbacks field to the signature.\n    This is used to store the previous errored decision attempts for the decision node.\n    \"\"\"\n\n    def __init__(self, module: ElysiaChainOfThought, **kwargs):\n        feedback_desc = (\n            \"Pairs of INCORRECT previous attempts at this action, and the feedback received for each attempt. \"\n            \"Judge what was incorrect in the previous attempts. \"\n            \"Follow the feedback to improve your next attempt.\"\n        )\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#d-few-shot-learning-with-dspy","title":"D. Few-Shot Learning with DSPy","text":"<p>Module: <code>elysia.util.elysia_chain_of_thought.ElysiaChainOfThought.aforward_with_feedback_examples</code> Full Path: <code>/elysia/util/elysia_chain_of_thought.py</code> (lines 345-421)</p> <pre><code>async def aforward_with_feedback_examples(\n    self,\n    feedback_model: str,\n    client_manager: ClientManager,\n    base_lm: dspy.LM,\n    complex_lm: dspy.LM,\n    num_base_lm_examples: int = 3,\n    return_example_uuids: bool = False,\n    **kwargs,\n) -&gt; tuple[dspy.Prediction, list[str]] | dspy.Prediction:\n    \"\"\"\n    Use the forward pass of the module with feedback examples.\n    This will first retrieve examples from the feedback collection, \n    and use those as few-shot examples to run the module.\n    \"\"\"\n\n    examples, uuids = await retrieve_feedback(\n        client_manager, self.tree_data.user_prompt, feedback_model, n=10\n    )\n\n    if len(examples) &gt; 0:\n        optimizer = dspy.LabeledFewShot(k=10)\n        optimized_module = optimizer.compile(self, trainset=examples)\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#dspy-optimization-framework-in-elysia","title":"DSPy Optimization Framework in Elysia","text":"<p>Elysia leverages DSPy's sophisticated optimization framework to continuously improve its MCTS decision-making capabilities. The optimization process operates at multiple levels, from individual decision nodes to the entire reasoning pipeline.</p>"},{"location":"Advanced/ELYSIA_MCTS/#core-optimization-strategy","title":"Core Optimization Strategy","text":"<p>The optimization in Elysia follows a hierarchical few-shot learning approach where:</p> <ol> <li>Historical Examples: Retrieved from Weaviate feedback database based on semantic similarity</li> <li>Dynamic Module Compilation: DSPy optimizers compile modules with retrieved examples</li> <li>Contextual Adaptation: Modules adapt their behavior based on similar past scenarios</li> <li>Continuous Learning: Each decision contributes to the feedback database for future optimization</li> </ol>"},{"location":"Advanced/ELYSIA_MCTS/#primary-optimizer-labeledfewshot","title":"Primary Optimizer: LabeledFewShot","text":"<p>Module: <code>dspy.LabeledFewShot</code> Purpose: Implements few-shot learning by selecting the most relevant examples from a training set Key Features: - Fixed Sample Size: Uses exactly <code>k</code> examples (default k=10 in Elysia) - Random Sampling: When <code>sample=True</code>, randomly selects k examples from available training set - Sequential Selection: When <code>sample=False</code>, takes first k examples from training set - Module Compilation: Creates optimized version of the target module with selected examples</p> <pre><code># LabeledFewShot Implementation in Elysia\noptimizer = dspy.LabeledFewShot(k=10)\noptimized_module = optimizer.compile(\n    student=self,           # The ElysiaChainOfThought module to optimize\n    trainset=examples,      # Retrieved feedback examples\n    sample=True            # Use random sampling for example selection\n)\n</code></pre> <p>Optimization Process: 1. Example Retrieval: <code>retrieve_feedback()</code> fetches similar examples from Weaviate 2. Example Selection: LabeledFewShot selects k=10 most relevant examples 3. Module Compilation: Creates new module instance with selected examples as demonstrations 4. Execution: Optimized module uses examples as few-shot demonstrations for reasoning</p>"},{"location":"Advanced/ELYSIA_MCTS/#advanced-optimization-techniques","title":"Advanced Optimization Techniques","text":""},{"location":"Advanced/ELYSIA_MCTS/#1-multi-model-optimization","title":"1. Multi-Model Optimization","text":"<p>Elysia employs different optimization strategies for different model types:</p> <pre><code># Base LM optimization (faster, simpler reasoning)\nbase_optimizer = dspy.LabeledFewShot(k=3)\nbase_optimized = base_optimizer.compile(module, trainset=base_examples)\n\n# Complex LM optimization (deeper reasoning)\ncomplex_optimizer = dspy.LabeledFewShot(k=10)\ncomplex_optimized = complex_optimizer.compile(module, trainset=complex_examples)\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#2-contextual-example-weighting","title":"2. Contextual Example Weighting","text":"<p>The system retrieves examples based on: - Semantic Similarity: Weaviate vector similarity search (threshold: 0.7) - Task Type Matching: Examples from similar decision contexts - Success Patterns: Preferentially selects examples that led to successful outcomes - Failure Learning: Includes failed attempts to avoid repeating mistakes</p>"},{"location":"Advanced/ELYSIA_MCTS/#3-dynamic-optimization-parameters","title":"3. Dynamic Optimization Parameters","text":"<pre><code># Adaptive k-value based on available examples\nk_value = min(10, len(examples)) if len(examples) &gt; 0 else 0\noptimizer = dspy.LabeledFewShot(k=k_value)\n\n# Context-aware example selection\nif task_complexity == \"high\":\n    optimizer = dspy.LabeledFewShot(k=15)  # More examples for complex tasks\nelse:\n    optimizer = dspy.LabeledFewShot(k=5)   # Fewer examples for simple tasks\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#optimization-limitations-and-drawbacks","title":"Optimization Limitations and Drawbacks","text":""},{"location":"Advanced/ELYSIA_MCTS/#1-fixed-sample-size-constraint","title":"1. Fixed Sample Size Constraint","text":"<p>Primary Limitation: LabeledFewShot relies on a fixed sample size (k=10), which creates several learning and generalization challenges:</p> <ul> <li>Insufficient Context for Complex Tasks: Complex reasoning tasks may require more than 10 examples to capture the full decision space</li> <li>Overfitting to Limited Examples: With only 10 examples, the model may overfit to specific patterns rather than learning generalizable strategies</li> <li>Inconsistent Learning: Different decision contexts may have vastly different optimal example counts, but the fixed k-value treats all scenarios uniformly</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#2-random-sampling-bias","title":"2. Random Sampling Bias","text":"<p>When <code>sample=True</code> (default in Elysia), the optimizer randomly selects examples, which can lead to: - Suboptimal Example Selection: Random sampling may miss the most relevant examples for the current context - Inconsistent Performance: Different runs may select different examples, leading to variable decision quality - Loss of Temporal Patterns: Sequential examples that show decision progression are lost in random sampling</p>"},{"location":"Advanced/ELYSIA_MCTS/#3-static-optimization-approach","title":"3. Static Optimization Approach","text":"<p>The current implementation uses static optimization that doesn't adapt based on: - Task Complexity: All tasks use the same k=10 examples regardless of complexity - Historical Performance: No feedback loop to adjust k-value based on past optimization success - Context Diversity: Doesn't consider the diversity of available examples when selecting the optimal subset</p>"},{"location":"Advanced/ELYSIA_MCTS/#4-limited-generalization-capabilities","title":"4. Limited Generalization Capabilities","text":"<p>The fixed sample size approach has several generalization limitations:</p> <pre><code># Current approach - fixed k=10\noptimizer = dspy.LabeledFewShot(k=10)  # Always uses exactly 10 examples\n\n# Problems:\n# 1. May not be enough for complex multi-step reasoning\n# 2. May be too many for simple binary decisions\n# 3. No adaptation based on example quality or relevance\n# 4. No consideration of example diversity or coverage\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#5-memory-and-computational-constraints","title":"5. Memory and Computational Constraints","text":"<ul> <li>Storage Overhead: Storing and retrieving large numbers of examples impacts performance</li> <li>Computational Cost: Processing more examples increases inference time</li> <li>Cache Invalidation: Fixed sample size doesn't adapt to changing example relevance</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#proposed-improvements","title":"Proposed Improvements","text":""},{"location":"Advanced/ELYSIA_MCTS/#1-adaptive-sample-size","title":"1. Adaptive Sample Size","text":"<pre><code># Proposed adaptive approach\ndef adaptive_k_selection(examples, task_complexity, context_diversity):\n    base_k = 5\n    complexity_multiplier = {\"simple\": 1, \"medium\": 2, \"complex\": 3}\n    diversity_bonus = min(5, context_diversity * 2)\n\n    return min(20, base_k * complexity_multiplier[task_complexity] + diversity_bonus)\n\nk_value = adaptive_k_selection(examples, task_complexity, context_diversity)\noptimizer = dspy.LabeledFewShot(k=k_value)\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#2-quality-based-example-selection","title":"2. Quality-Based Example Selection","text":"<pre><code># Proposed quality-aware selection\ndef select_quality_examples(examples, k, quality_threshold=0.8):\n    # Filter by quality score\n    high_quality = [ex for ex in examples if ex.quality_score &gt;= quality_threshold]\n\n    # If not enough high-quality examples, include medium quality\n    if len(high_quality) &lt; k:\n        medium_quality = [ex for ex in examples if 0.6 &lt;= ex.quality_score &lt; quality_threshold]\n        selected = high_quality + medium_quality[:k-len(high_quality)]\n    else:\n        selected = high_quality[:k]\n\n    return selected\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#3-dynamic-optimization-strategy","title":"3. Dynamic Optimization Strategy","text":"<pre><code># Proposed dynamic optimization\nclass AdaptiveLabeledFewShot:\n    def __init__(self, min_k=3, max_k=20):\n        self.min_k = min_k\n        self.max_k = max_k\n        self.performance_history = []\n\n    def compile(self, student, trainset, context_metadata=None):\n        # Determine optimal k based on context and history\n        optimal_k = self._determine_optimal_k(trainset, context_metadata)\n\n        # Use quality-based selection\n        selected_examples = self._select_quality_examples(trainset, optimal_k)\n\n        # Compile with selected examples\n        return dspy.LabeledFewShot(k=optimal_k).compile(student, trainset=selected_examples)\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#integration-with-mcts-process","title":"Integration with MCTS Process","text":"<p>The optimization process is deeply integrated into Elysia's MCTS workflow:</p> <ol> <li>Pre-Decision Optimization: Before each decision, the system retrieves and optimizes the decision module</li> <li>Context-Aware Learning: Optimization considers the current tree state and available actions</li> <li>Feedback Integration: Each decision outcome contributes to the training set for future optimization</li> <li>Multi-Level Optimization: Different parts of the MCTS process (selection, evaluation, backpropagation) use different optimization strategies</li> </ol> <p>This sophisticated optimization framework enables Elysia to continuously improve its decision-making capabilities while maintaining the systematic exploration and exploitation balance characteristic of MCTS algorithms.</p>"},{"location":"Advanced/ELYSIA_MCTS/#alternative-dspy-optimizers-for-enhanced-learning","title":"Alternative DSPy Optimizers for Enhanced Learning","text":"<p>While Elysia currently uses <code>LabeledFewShot</code>, several other DSPy optimizers could provide enhanced learning capabilities:</p>"},{"location":"Advanced/ELYSIA_MCTS/#1-bootstrapfewshot","title":"1. BootstrapFewShot","text":"<p>Module: <code>dspy.BootstrapFewShot</code> Purpose: Generates synthetic examples by running the module on unlabeled inputs and using high-confidence outputs as training examples Potential in Elysia: Could generate synthetic decision examples from historical tree states, expanding the training set beyond human-annotated feedback</p>"},{"location":"Advanced/ELYSIA_MCTS/#2-miprov2","title":"2. MIPROv2","text":"<p>Module: <code>dspy.MIPROv2</code> Purpose: Multi-prompt optimization that generates and optimizes multiple prompt variations Potential in Elysia: Could optimize different prompt templates for different decision contexts (e.g., tool selection vs. parameter optimization)</p>"},{"location":"Advanced/ELYSIA_MCTS/#3-copro","title":"3. COPRO","text":"<p>Module: <code>dspy.COPRO</code> Purpose: Coordinate ascent optimization for prompt engineering Potential in Elysia: Could optimize the decision prompts used in <code>DecisionPrompt</code> signature for better reasoning quality</p>"},{"location":"Advanced/ELYSIA_MCTS/#4-bootstrapfinetune","title":"4. BootstrapFinetune","text":"<p>Module: <code>dspy.BootstrapFinetune</code> Purpose: Combines few-shot learning with model fine-tuning Potential in Elysia: Could fine-tune specialized models for different types of decisions (e.g., tool selection vs. parameter tuning)</p>"},{"location":"Advanced/ELYSIA_MCTS/#5-gepa-generative-prompt-evolution","title":"5. GEPA (Generative Prompt Evolution)","text":"<p>Module: <code>dspy.GEPA</code> Purpose: Evolutionary optimization of prompts using genetic algorithms Potential in Elysia: Could evolve decision-making prompts over time, adapting to changing user patterns and task requirements</p>"},{"location":"Advanced/ELYSIA_MCTS/#comparative-analysis-of-optimization-strategies","title":"Comparative Analysis of Optimization Strategies","text":"Optimizer Learning Type Sample Efficiency Generalization Computational Cost Best Use Case in Elysia LabeledFewShot Few-shot High Limited Low Current implementation, simple decisions BootstrapFewShot Self-supervised Medium Good Medium Generating synthetic examples MIPROv2 Multi-prompt High Excellent High Complex decision contexts COPRO Coordinate ascent Medium Good Medium Prompt optimization BootstrapFinetune Fine-tuning Low Excellent Very High Specialized decision models GEPA Evolutionary Low Excellent High Long-term adaptation"},{"location":"Advanced/ELYSIA_MCTS/#mcts-process-flow","title":"MCTS Process Flow","text":""},{"location":"Advanced/ELYSIA_MCTS/#1-tree-initialization","title":"1. Tree Initialization","text":"<p>Location: <code>elysia.tree.tree.Tree.__init__</code> - Initializes decision nodes and tree structure - Sets up environment and tree data - Configures DSPy language models</p>"},{"location":"Advanced/ELYSIA_MCTS/#2-main-execution-loop","title":"2. Main Execution Loop","text":"<p>Location: <code>elysia.tree.tree.Tree.async_run</code> (<code>/elysia/tree/tree.py</code> lines 1431-1550)</p> <pre><code>while True:\n    # Selection: Get available tools\n    available_tools, unavailable_tools = await self._get_available_tools(\n        current_decision_node, client_manager\n    )\n\n    # Exploration: Get successive actions\n    successive_actions = self._get_successive_actions(\n        successive_actions={},\n        current_options=init_options,\n    )\n\n    # Decision: Make choice using MCTS-like reasoning\n    decision, results = await current_decision_node(\n        tree_data=self.tree_data,\n        base_lm=self.base_lm,\n        complex_lm=self.complex_lm,\n        available_tools=available_tools,\n        unavailable_tools=unavailable_tools,\n        successive_actions=successive_actions,\n        client_manager=client_manager,\n    )\n\n    # Evaluation &amp; Back Propagation: Store results for learning\n    self.training_updates.extend(results)\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#3-feedback-integration","title":"3. Feedback Integration","text":"<p>Location: Throughout the decision process</p> <ul> <li>Immediate Feedback: AssertedModule provides immediate validation</li> <li>Historical Feedback: CopiedModule incorporates past failures</li> <li>Database Feedback: retrieve_feedback provides similar examples</li> <li>Contextual Learning: DSPy few-shot learning optimizes modules</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#control-parameters","title":"Control Parameters","text":""},{"location":"Advanced/ELYSIA_MCTS/#1-choice-control","title":"1. Choice Control","text":"<ul> <li>max_tries: Maximum retry attempts in AssertedModule (default: 3)</li> <li>assertion function: Custom validation logic for decisions</li> <li>USE_REASONING: Controls whether to include step-by-step reasoning</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#2-exploration-control","title":"2. Exploration Control","text":"<ul> <li>recursion_limit: Maximum tree depth (TreeData.recursion_limit)</li> <li>tool availability rules: Custom logic for when tools can be used</li> <li>successive_actions depth: How far ahead to look in action tree</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#3-evaluation-control","title":"3. Evaluation Control","text":"<ul> <li>feedback retrieval count: Number of examples to retrieve (n parameter)</li> <li>certainty threshold: Weaviate similarity threshold (default: 0.7)</li> <li>few-shot examples: Number of examples for DSPy optimization (k=10)</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#4-back-propagation-control","title":"4. Back Propagation Control","text":"<ul> <li>USE_FEEDBACK: Global setting to enable/disable feedback system</li> <li>feedback storage: Weaviate database configuration</li> <li>training update collection: What information to store for learning</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#dspy-framework-integration","title":"DSPy Framework Integration","text":"<p>Elysia extensively uses the DSPy framework for structured LLM interactions:</p>"},{"location":"Advanced/ELYSIA_MCTS/#core-dspy-components-used","title":"Core DSPy Components Used:","text":"<ol> <li>dspy.Module: Base class for all reasoning modules</li> <li>dspy.Signature: Defines input/output structure for LLM calls</li> <li>dspy.LabeledFewShot: Optimizer for few-shot learning</li> <li>dspy.InputField/OutputField: Structured prompt fields</li> <li>dspy.Prediction: Structured LLM outputs</li> </ol>"},{"location":"Advanced/ELYSIA_MCTS/#key-dspy-modules-in-elysia","title":"Key DSPy Modules in Elysia:","text":"<ul> <li>ElysiaChainOfThought: Main reasoning module</li> <li>DecisionPrompt: Decision-making signature</li> <li>AssertedModule: Quality assurance wrapper</li> <li>CopiedModule: Feedback integration wrapper</li> </ul>"},{"location":"Advanced/ELYSIA_MCTS/#usage-examples","title":"Usage Examples","text":""},{"location":"Advanced/ELYSIA_MCTS/#basic-decision-making","title":"Basic Decision Making","text":"<pre><code># Create decision node\ndecision_node = DecisionNode(\n    id=\"root\",\n    options=available_tools,\n    instruction=\"Choose the best tool for the task\"\n)\n\n# Make decision with MCTS-like reasoning\ndecision, results = await decision_node(\n    tree_data=tree_data,\n    base_lm=base_lm,\n    complex_lm=complex_lm,\n    available_tools=tools,\n    unavailable_tools=[],\n    successive_actions=future_actions,\n    client_manager=client_manager,\n)\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#enabling-feedback-learning","title":"Enabling Feedback Learning","text":"<pre><code># Enable feedback in settings\ntree_data.settings.USE_FEEDBACK = True\n\n# Decision will automatically use historical examples\noutput, uuids = await decision_executor.aforward_with_feedback_examples(\n    feedback_model=\"decision\",\n    client_manager=client_manager,\n    base_lm=base_lm,\n    complex_lm=complex_lm,\n    **decision_inputs\n)\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#custom-assertion-functions","title":"Custom Assertion Functions","text":"<pre><code>def custom_assertion(kwargs, pred):\n    # Custom validation logic\n    is_valid = pred.function_name in allowed_functions\n    feedback = f\"Must choose from: {allowed_functions}\" if not is_valid else \"\"\n    return is_valid, feedback\n\n# Use with AssertedModule\ndecision_executor = AssertedModule(\n    decision_module,\n    assertion=custom_assertion,\n    max_tries=5,\n)\n</code></pre>"},{"location":"Advanced/ELYSIA_MCTS/#conclusion","title":"Conclusion","text":"<p>Elysia's MCTS implementation provides a sophisticated framework for decision-making in complex reasoning tasks. The system combines the systematic exploration of MCTS with the contextual understanding of large language models, enhanced by continuous learning through feedback mechanisms.</p> <p>The modular design allows for fine-grained control over each aspect of the reasoning process, from initial choice selection to final evaluation and learning. The extensive use of the DSPy framework ensures structured, optimizable interactions with language models throughout the process.</p>"},{"location":"Advanced/advanced_tool_construction/","title":"Advanced Tool Construction Overview","text":"<p>This page details how to create more custom and flexible tools in Elysia, by inheriting the <code>Tool</code> class and adding it to the decision tree via the <code>.add_tool</code> of the <code>Tree</code> object.</p> <p>To see an easier method of creating tools, see the Creating a Tool guide.</p> <p>This page will detail all relevant information for tool construction, to get started with an example, see - A basic text response example - A more complex example dealing cards to Elysia</p>"},{"location":"Advanced/advanced_tool_construction/#initialisation","title":"Initialisation","text":"<p>A tool must be initialised with <pre><code>    def __init__(self, logger: Logger | None = None, **kwargs):\n        super().__init__(\n            name=...,\n            description=...,\n            status=..., # optional\n            inputs=..., # optional\n            end=..., # optional\n            **kwargs # required\n        )\n</code></pre></p> <ul> <li><code>name</code>: A short, one or two word name of the tool.</li> <li><code>description</code>: A detailed description of what the tool will do and what it will accomplish. This is how the LLM decides whether to call the tool or not, so it is important that this is comprehensive and detailed.</li> <li><code>status</code> (optional): A short 'update' message that is displayed whilst the tool is running.</li> <li><code>inputs</code> (optional): A dictionary of inputs to your tool, which the LLM will decide on, which conform to the following structure:     <pre><code>{\n    input_name: {\n        \"description\": str,\n        \"type\": Any,\n        \"default\": Any,\n        \"required\": bool\n    },\n    ...\n}\n</code></pre>     You can have as many inputs as you want, but similar to the description field, the descriptions here need to be informative so that the LLM knows exactly what to choose.</li> <li><code>end</code> (optional): A bool denoting whether the tool is capable of ending the entire decision tree. For example, a <code>text_response</code> tool can end the process, but a <code>query</code> tool cannot. This is because a query tool returns some information which is then parsed by the decision tree afterwards, to see if the retrieved information was worthwhile. Note that setting <code>end=True</code> does not guarantee that after this tool is finished running, the decision process ends, it only allows the model to choose that performing this action can end the tree.</li> <li><code>**kwargs</code> (required)</li> </ul> <p>The <code>logger</code> can be automatically assigned to the initialisation of the tool and is passed by default into the Elysia decision tree. Save this as <code>self.logger = logger</code> to use it in the tool call later.</p>"},{"location":"Advanced/advanced_tool_construction/#tool-call","title":"Tool Call","text":"<p>The tool should have an async <code>__call__</code> method, <pre><code>    async def __call__(\n        tree_data: TreeData,\n        inputs: dict,\n        base_lm: dspy.LM,\n        complex_lm: dspy.LM,\n        client_manager: ClientManager,\n        **kwargs\n    ):\n        # tool call here\n</code></pre> which has the following inputs:</p> <ul> <li><code>tree_data</code>, an object of type <code>TreeData</code> which contains some information about the state of the decision making process at this point. This is likely the most relevant data to use in your tool calls, if the tool will affect the decision tree in some way. Here, you can access the environment (via <code>tree_data.environment</code>), the tasks completed dictionary (<code>tree_data.tasks_completed</code>), the collection metadata (<code>tree_data.collection_data</code>) and more - see here for all data you can access.</li> <li><code>inputs</code>: the inputs previously defined, formatted as      <pre><code>{\n    input_name_1: value,\n    input_name_2: value,\n    ...\n}\n</code></pre>     which were given by the LLM (or reverted to their default values, if the LLM chose nothing for a particular input).</li> <li><code>base_lm</code>: a <code>dspy.LM</code> object that can be used to interface with LLMs within the tool. This model is the same picked in the <code>elysia.configure(base_model=\"...\", base_provider=\"...\")</code> call. You can use this directly, e.g.     <pre><code>base_lm(\"hello, world!\")\n</code></pre>     or via a DSPy signature or module.</li> <li><code>complex_lm</code>: same as above for the complex LM specified.</li> <li><code>client_manager</code>: the interface to the Weaviate cluster you are connected to.</li> </ul> <p>The <code>__call__</code> method is automatically run when the LLM decision agent chooses to use that tool. You can use any of these inputs within your tool method and the code will be executed.</p> <p>Within the <code>__call__</code> method of the tool, you will want to interact with the decision tree in some way. There are multiple ways of doing this, either via returning various objects that Elysia defines within <code>elysia.objects</code>, or by interacting with the environment.</p>"},{"location":"Advanced/advanced_tool_construction/#returning-objects","title":"Returning Objects","text":"<p>If you return an Elysia specific object, they will be returned to the decision tree and automatically parsed in different ways which automatically add the relevant objects to the environment, and send any payloads to the frontend.</p> <p>Within your tool's call method, you may want to <code>yield</code> different objects to bring them back to the tree.</p> <ul> <li>Any class that inherits from the <code>Update</code> class will send updates to the frontend, such as a status message.</li> <li>Any class that inherits from the <code>Result</code> class have their corresponding objects added to the tree's environment, which the decision agent will 'look at', so that it can continue making decisions and respond accordingly to the user. Then, if applicable, relevant payloads will be sent to the frontend.</li> </ul>"},{"location":"Advanced/advanced_tool_construction/#status","title":"Status","text":"<p>A <code>Status</code> message is initialised with a single string argument, this displays on the frontend or the progress bar a unique message.</p>"},{"location":"Advanced/advanced_tool_construction/#result","title":"Result","text":"<p>Running inside of the call something like: <pre><code>    yield Result(\n        objects = [\n            {\n                \"title\": \"Example Result\",\n                \"content\": \"This is just an example of a result\"\n            }\n        ]\n    )\n</code></pre> will mean that this particular object gets added to the Tree's 'Environment', and the LLM can look at this to make further decisions. This will also automatically parse this object as a payload to a frontend, if one is connected.</p> <p>The arguments for the <code>Result</code> are:  - <code>objects</code>: a list of dictionaries that contain your specific objects. Currently, the keys of the dictionary do not matter, but if you want to display these items on the frontend, they need to conform to specific keys (see later)  - <code>metadata</code>: a dictionary of metadata items. You can use this to separate global information from object-specific information.  - <code>payload_type</code>: a string describing the type of objects you are giving.  - <code>mapping</code>: a dictionary mapping frontend-aware fields to the fields in <code>objects</code> (see here).</p> <p>See the custom objects page for more detail.</p>"},{"location":"Advanced/advanced_tool_construction/#interacting-with-the-environment","title":"Interacting with the Environment","text":"<p>See here a full description of the methods that you can use to interact with the environment.</p> <p>In short, the environment can be modified either by yielding <code>Result</code> objects, or by calling the environment methods explicitly.  You can do so via calling the <code>.add()</code>, <code>.add_objects()</code>, <code>.replace()</code> or <code>.remove()</code> from the <code>tree_data</code>.</p> <p>Note: If you add items to the environment and also yield a <code>Result</code> object with the same items, there will likely be duplicate items in the environment.</p>"},{"location":"Advanced/advanced_tool_construction/#displaying-objects-frontend-only","title":"Displaying Objects (Frontend Only)","text":"<p>You can yield a <code>Result</code> to the frontend, and by specifying the <code>payload_type</code>, the frontend will be aware of the type of object sent. The payload type currently must be one of the objects in the reference page, and you must also either conform to the field structure for each type or provide a <code>mapping</code> that maps from the expected fields to the fields in the objects.</p> <p>To display your objects without any mappings or display types, you can specify the payload type as <code>table</code>.</p>"},{"location":"Advanced/advanced_tool_construction/#easy-llm-calls-with-elysia-chain-of-thought","title":"Easy LLM calls with Elysia Chain of Thought","text":"<p>An easy way to access attributes from the tree (if you are calling an LLM within the tool) is to use the custom <code>ElysiaChainOfThought</code> DSPy module with specific arguments. This automatically adds information from the <code>tree_data</code> to an LLM prompt as inputs in a DSPy signature, as well as some specific outputs deemed useful within the decision tree environment (and a chain of thought reasoning field output field).</p> <p>To call this, you can do, for example <pre><code>from elysia.util.elysia_chain_of_thought import ElysiaChainOfThought\nmy_module = ElysiaChainOfThought(\n    MyCustomSignature, # a dspy signature needing to be defined\n    tree_data=tree_data, # tree_data input from the tool\n    message_update: bool = True,\n    environment: bool = False,\n    collection_schemas: bool = False,\n    tasks_completed: bool = False,\n    collection_names: list[str] = [],\n)\n</code></pre> By setting the boolean flags for the different variables, you can control the inputs and outputs assigned, whereas some inputs are always included (such as user prompt).</p> <p>To use the augmented module via <code>ElysiaChainOfThought</code>, call the <code>.aforward()</code> method of the new module, passing all your new inputs as keyword arguments. You do not need to include keyword arguments for the other inputs, like the <code>environment</code> or <code>user_prompt</code>, they are automatically added, e.g. <pre><code>my_module.aforward(input1=..., input2=..., lm=...)\n</code></pre> The <code>lm</code> parameter can be inherited from the tool inputs, i.e. <code>base_lm</code> or <code>complex_lm</code>. Or you can define your own LMs via <code>dspy.LM</code>.</p> <p>See the description for more details</p>"},{"location":"Advanced/advanced_tool_construction/#adding-tools-to-the-tree","title":"Adding Tools to the Tree","text":"<p>To add a Tool to be evaluated in the tree, just run <code>.add_tool</code>. For example</p> <p><pre><code>.add_tool(TextResponse)\n</code></pre> This will add the <code>TextResponse</code> tool to the root branch, by default (the base of the decision tree).</p> <p>Elysia sometimes has branches in the decision tree, which can be created via <code>add_branch</code>. If you want to add a tool to a particular branch, specify the <code>branch_id</code>, e..g if we have a branch called \"responses\", then</p> <pre><code>.add_tool(TextResponse, branch_id=\"responses\")\n</code></pre> <p>You can add tools on top of existing tools. Assume that the decision tree has the <code>multi_branch</code> structure, so that at the root node there are two options: <code>search</code> and <code>text_response</code>. The <code>text_response</code> option is a single tool, whereas the <code>search</code> option is in fact a branch with two options: <code>query</code> and <code>aggregate</code>.</p> <p>If you wanted to add a tool called <code>CheckOutput</code> to be run after the query tool, then you can do: <pre><code>.add_tool(CheckOutput, branch_id=\"search\", from_tool_ids = [\"query\"])\n</code></pre> which will add the <code>CheckOutput</code> tool to the line <code>search -&gt; query</code>, resulting in <code>search -&gt; query -&gt; check_output</code>.</p> <p>Note that the <code>search</code> branch still has two options, but if the decision LLM chooses to do the <code>query</code> tool, then the <code>check_output</code> tool is available for choice after querying. Also note that if a tool has no inputs and is alone in a decision node (it is the only option for the LLM to pick), the LLM decision will be skipped and the node will be automatically added. You can add more nodes to after the <code>query</code> tool and then the decision LLM will now resume operations at that node.</p>"},{"location":"Advanced/advanced_tool_construction/#self-healing-errors","title":"Self Healing Errors","text":"<p>You can yield an <code>Error</code> object to 'return' an error from the tool to the decision tree. These errors are saved within the tree data and automatically added to the decision nodes as well as any LLM calls made with <code>ElysiaChainOfThought</code> called within that tool. The LLM is 'informed' about these errors via an input to the prompts. The LLM can choose to continue calling the tool again, in spite of the error (if it seems fixable), or it can use the information to end the conversation and inform the user of an error, or to try a different tool that will not error.</p> <p>The <code>Error</code> object is initialised with a single string argument, which should be informative and descriptive.</p> <p>Note that this does not raise an error within Python, it is used to 'inform' the LLM that a potentially preventable error has occurred somewhere within the tool.</p> <p>For example, the Query tool built into Elysia will yield <code>Error</code> objects if the LLM creates a query which fails to run in Weaviate, such as not having the correct filter type for a particular property. The decision agent will read the error, and perhaps try to call the query tool again. Upon seeing the previous error in the error history, the query LLM agent should see that it should instead use a different filter property type, and correct itself.</p>"},{"location":"Advanced/advanced_tool_construction/#advanced-tool-methods","title":"Advanced Tool Methods","text":""},{"location":"Advanced/advanced_tool_construction/#run_if_true","title":"<code>run_if_true</code>","text":"<p>You can optionally choose to add another method to your Tool - <code>run_if_true</code>. This is a method that will be checked at the start of every decision tree, for every tool that has this method. If you don't wish to use this method, then simply do not define one.</p> <p>The <code>run_if_true</code> method returns two arguments (<code>tuple[bool, dict]</code>):</p> <ul> <li>a boolean value indicating whether the tool should be called straight away,</li> <li>a dictionary of <code>inputs</code> for if this tool gets called.</li> </ul> <p>If <code>run_if_true</code> returns <code>True</code>, then the <code>__call__</code> method of your tool will be called and carried out regardless of if the LLM wishes to use this tool or not. It is a hardcoded rule to run the tool. Some potential examples of using this include:</p> <ul> <li>The <code>run_if_true</code> method can count the number of tokens in the environment, and if the environment is getting too large, it runs the tool. Then the <code>__call__</code> method will be shrinking the environment in some way (e.g. using an LLM or just taking one particular item from it).</li> <li>If the user is asking about a particular subject, e.g. if the <code>user_prompt</code> (inside of <code>tree_data</code>) contains a specific word, then you could augment the <code>tree_data</code> to include some more specific information.</li> </ul> <pre><code>async def run_if_true(\n    self,\n    tree_data,\n    base_lm,\n    complex_lm,\n    client_manager,\n) -&gt; tuple[bool, dict]:\n    ...\n</code></pre> <p>Like the <code>__call__</code> and <code>is_tool_available</code> methods, this method has access to the tree data object, as well as some language models used by the tree and the ClientManager, to use a Weaviate client.</p> <p>See the reference for more details.</p>"},{"location":"Advanced/advanced_tool_construction/#is_tool_available","title":"<code>is_tool_available</code>","text":"<p>This method should return <code>True</code> if the tool is available to be used by the LLM. It should return <code>False</code> if the LLM should not have access to it. This can depend on the environment. For example, you can use <code>tree_data.environment.is_empty()</code> and the tool is only accessible if the environment is empty. Likewise you can use <code>not tree_data.environment.is_empty()</code> for it only to be available if the environment has something in it.</p> <pre><code>async def is_tool_available(\n    self,\n    tree_data,\n    base_lm,\n    complex_lm,\n    client_manager,\n) -&gt; bool:\n    \"\"\"A brief reason when this tool will become available goes here.\"\"\"\n    ...\n</code></pre> <p>Like the <code>__call__</code> and <code>run_if_true</code> methods, this method has access to the tree data object, as well as some language models used by the tree and the ClientManager, to use a Weaviate client.</p> <p>You should give a brief reason in the docstring of <code>is_tool_available</code> as to when it will become available, so that the LLM can perform actions towards completing this goal if it judges the tool to be useful to the current prompt.</p> <p>See the reference for more details. </p>"},{"location":"Advanced/advanced_tool_construction/#example-text-response-basic","title":"Example: Text Response (basic)","text":"<p>Consider the generic text response tool that Elysia will use if the conversation ends without a sufficient answer.</p> <pre><code>import dspy\nfrom elysia.objects import Response, Tool\nfrom elysia.tree.objects import TreeData\nfrom elysia.util.client import ClientManager\nfrom elysia.tools.text.prompt_templates import TextResponsePrompt\nfrom elysia.util.elysia_chain_of_thought import ElysiaChainOfThought\n\nclass TextResponse(Tool):\n    def __init__(self, **kwargs):\n        super().__init__(\n            name=\"final_text_response\",\n            description=\"\",\n            status=\"Writing response...\",\n            inputs={},\n            end=True,\n        )\n\n    async def __call__(\n        self,\n        tree_data: TreeData,\n        inputs: dict,\n        base_lm: dspy.LM,\n        complex_lm: dspy.LM,\n        client_manager: ClientManager | None = None,\n        **kwargs\n    ):\n        text_response = ElysiaChainOfThought(\n            TextResponsePrompt,\n            tree_data=tree_data,\n            environment=True,\n            tasks_completed=True,\n            message_update=False,\n        )\n\n        output = await text_response.aforward(\n            lm=base_lm,\n        )\n\n        yield Response(text=output.response)\n</code></pre> <p>The tool is simple, it is initialised and the descriptions are added to the Tool. Then the <code>__call__</code> method simply runs the text_response agent. Whilst the <code>TextResponsePrompt</code> is not shown here, it is a simple input -&gt; output call, where different parts of the <code>tree_data</code> are used as inputs to the LLM to give it context before answering. The relevant information from the <code>tree_data</code> are automatically inserted into the prompt via the <code>ElysiaChainOfThought</code> custom DSPy module.</p> <p>Note: If using DSPy within your tool, make sure to always call <code>aforward</code> method on the module so that it can be used async.</p>"},{"location":"Advanced/advanced_tool_construction/#example-dealing-cards-randomly-from-a-deck-intermediate","title":"Example: Dealing Cards Randomly from a Deck (Intermediate)","text":"<p>Let's create a tool that deals cards, adds them to the environment and displays them on the frontend.</p> <p>Just for fun, these cards, when they are dealt, change the Elysia conversation somewhat, by some modifiers we will define ourselves.</p> <pre><code>import random\nfrom elysia import Tool\nfrom elysia.tools import Ecommerce\n\nclass DealCards(Tool):\n    def __init__(self, **kwargs):\n        super().__init__(\n            name=\"deal_cards\",\n            description=\"\"\"\n            This tool should always be run at the start of any interaction with the user.\n            It defines any modifiers that get added to the conversation, from a random pool.\n            It does so by 'dealing cards' to the user as if they were a player of a card game.\n            These then will be displayed.\n            Call this tool when specifically asked for, or at the start of any conversation.\n            \"\"\",\n            status=\"Dealing cards...\",\n            inputs={\n                \"num_cards\": {\n                    \"description\": \"The number of cards to deal\",\n                    \"type\": int,\n                    \"default\": 3,\n                    \"required\": False,\n                }\n            },\n            end=False,\n        )\n        self.logger = kwargs.get(\"logger\", None)\n\n    def select_random_cards(self, num_cards=3):\n        possible_cards = [\n            {\n                \"title\": \"The Jumbled\",\n                \"effect\": \"Sometimes, the Elysia agent will say words in the wrong order.\",\n                \"rarity\": 3,\n                \"image\": \"https://i.imgur.com/KdGeZTp.png\",\n            },\n            {\n                \"title\": \"The Comedian\",\n                \"effect\": \"At the end of every sentence, the Elysia agent will tell a joke.\",\n                \"rarity\": 2,\n                \"image\": \"https://i.imgur.com/I8yVXHa.png\",\n            },\n            {\n                \"title\": \"The Sarcastic\",\n                \"effect\": \"Most interactions end with the Elysia agent making a sarcastic remark.\",\n                \"rarity\": 1,\n                \"image\": \"https://i.imgur.com/oFkwt1M.png\",\n            },\n            {\n                \"title\": \"The Bro\",\n                \"effect\": \"Elysia must now use the word 'bro' a lot more, and apply similar slang everywhere.\",\n                \"rarity\": 1,\n                \"image\": \"https://i.imgur.com/J6dLbTZ.png\",\n            },\n            {\n                \"title\": \"The Philosopher\",\n                \"effect\": \"The Elysia agent will now try to philosophise at every opportunity.\",\n                \"rarity\": 3,\n                \"image\": \"https://i.imgur.com/D6VSitF.png\",\n            },\n        ]\n        return random.choices(\n            possible_cards, weights=[1 / card[\"rarity\"] for card in possible_cards], k=3\n        )\n\n    async def __call__(self, tree_data, inputs, base_lm, complex_lm, client_manager, **kwargs):\n        self.logger.info(f\"Dealing {inputs['num_cards']} cards\")\n        cards = self.select_random_cards(inputs[\"num_cards\"])\n\n        yield Ecommerce(\n            objects=cards,\n            mapping={\n                \"description\": \"effect\",\n                \"name\": \"title\",\n                \"price\": \"rarity\",\n                \"image\": \"image\",\n            },\n            metadata={\n                \"num_cards\": inputs[\"num_cards\"],\n            },\n            name=\"cards\",\n            llm_message=\"\"\"\n            Cards have been successfully dealt for this prompt! \n            Dealt {num_cards} out of a possible 5.\n            Look at them in the environment to apply their modifiers to the conversation.\n            Pay attention to what these cards do, and how they affect the conversation.\n            You should apply the modifiers together, in a combination, not only one at a time.\n            \"\"\",\n        )\n</code></pre> <p>Let's break down the different components of this tool.</p> <ol> <li>In the <code>__init__</code>, we gave the tool name, a hefty description as well as a single input - the number of cards to deal. Make sure you provide the <code>**kwargs</code> argument also.</li> <li>There is a custom method that randomly chooses <code>num_cards</code> out of 5 possible cards, hand-written.</li> <li> <ul> <li>The <code>__call__</code> method, when the tool gets chosen, simply calls the <code>select_random_cards</code> method with the input that has come from the decision agent. </li> <li>Then it yields an <code>Ecommerce</code> object (placeholder) which will display the card. </li> <li>Since the <code>Ecommerce</code> object has pre-defined fields, to choose which of the card's fields go where, the <code>mapping</code> places the card <code>effect</code> in the Ecommerce <code>description</code>, the card <code>title</code> in place of the <code>name</code> field, the <code>rarity</code> becomes the <code>price</code> and the image field name is the same, but it is mapped anyway.</li> <li>The <code>llm_message</code> argument of the Ecommerce <code>Result</code>, describes what happens to the LLM whenever this tool is completed. This <code>llm_message</code> is persistent through further calls in Elysia, it will remain there for all future events in this conversation. In this case, it re-iterates the point that the cards add custom modifiers, and shows how many cards were dealt to the user at this point.</li> </ul> </li> </ol> <p>We could add more features to this card, for example, modifying the <code>tree_data.environment</code> object to find any existing cards in the environment (with the name \"cards\") and overwriting them with the new deal.</p>"},{"location":"Advanced/custom_objects/","title":"Returning Objects","text":""},{"location":"Advanced/custom_objects/#result","title":"Result","text":"<p>For more detail, see the <code>Result</code> description in the reference.</p> <p>A <code>Result</code> is a class with a <code>to_json()</code> and <code>to_frontend()</code> method which returns the objects defined within the class, and their metadata, to either the decision tree environment and an attached frontend, respectively. By default, any <code>Result</code> objects yielded within a tool will automatically have these objects assigned and sent.</p> <p>I.e. <pre><code># top of file\nfrom elysia.objects import Result\n\n# existing tool code\nyield Result(\n    objects = [...],\n    metadata = {...},\n    payload_type = \"&lt;your_type_here&gt;\",\n    name = \"&lt;name_to_go_in_environment&gt;\",\n    mapping = {...}\n)\n# continued tool code\n</code></pre></p>"},{"location":"Advanced/custom_objects/#parameters","title":"Parameters","text":"<p>Objects</p> <p>The <code>Result</code> class stores <code>objects</code>, which are a list of dictionaries containing anything that should be added to the environment or sent to the frontend. These can be stored however you like, but an Elysia-aware frontend expects the objects to have a certain format - the fields of the dictionaries need to be specific to the type of object being sent.</p> <p>For example, an Elysia frontend knows what the payload of a 'document' object should be, and there are fields such as <code>'title'</code>, <code>'content'</code> and <code>'author'</code> which it expects. But the objects returned from a retrieval might not have the fields line up exactly like this - maybe it has fields called <code>'document_header'</code>, <code>'text_content'</code> and <code>'writer'</code> instead.</p> <p>Metadata</p> <p>Results also can have metadata attached to them, which is a single dictionary that contains any information about this particular list of objects that exist across all objects, rather than on an individual level. For example, if we have retrieved some documents using a search query, the metadata can contain what that search query was and what collection was searched, whereas <code>objects</code> are the objects themselves.</p> <p>Payload Type</p> <p>The <code>payload_type</code> parameter is used to tell the frontend what to expect. Within Elysia, there are some predefined types (see here) that the built-in frontend is aware of. However, you can specify your own payload types if you are building a different frontend that is aware of different payload types.</p> <p>Mapping</p> <p>Within the <code>Result</code> there is also a <code>mapping</code> field which is a dictionary which maps from these fields to the fields contained within the objects themselves. For example, if we had an object with fields <code>\"document_header\"</code>, <code>\"text_content\"</code> and <code>\"writer\"</code>, then you may want to specify the <code>mapping</code> as <pre><code>{\n    \"title\": \"document_header\",\n    \"content\": \"text_content\",\n    \"author\": \"writer\n}\n</code></pre> So that on the frontend, when a document is displayed, the correct field values appear in the relevant fields.</p> <p>Name</p> <p>To index within the tree's environment, a <code>name</code> string identifier must be used to identify what type of objects are added to the environment. E.g., in the retrieval setting, this could be the collection name queried.</p>"},{"location":"Advanced/custom_objects/#frontend","title":"Frontend","text":"<p>When the default <code>.to_frontend(user_id, conversation_id, query_id)</code> method is called, it first calls <code>to_json(mapping=True)</code>, which returns a list of all objects that are mapped to their frontend-specific values, and then returns an augmented payload:</p> <pre><code>{\n    \"type\": \"result\",\n    \"user_id\": str,\n    \"conversation_id\": str,\n    \"query_id\": str,\n    \"id\": str,\n    \"payload\": dict = {\n        \"type\": str,\n        \"objects\": list[dict],\n        \"metadata\": dict,\n    },\n}\n</code></pre> <p>The outer level <code>type</code> field will default to <code>\"result\"</code>, by definition of the <code>Result</code> class. The <code>\"payload\"</code> field contains the unique <code>payload_type</code> (created on initialisation of the <code>Result</code>), as well as the objects/metadata within the <code>Result</code> object. Other fields, such as <code>user_id</code> and so forth, are inputs to the function which are automatically assigned when the decision tree processes the results.</p>"},{"location":"Advanced/custom_objects/#llm-message","title":"LLM Message","text":"<p>The <code>Result</code> class also has a <code>llm_message</code> parameter (or <code>.llm_parse()</code> method) which can be used to display custom information to both the decision agent LLM as well as any tools which use the <code>tasks_completed</code> field in <code>tree_data</code>.</p> <p>llm_message:</p> <p>The message can be formatted using placeholders given by:</p> <ul> <li><code>{payload_type}</code>: The payload type of the object created at initialisation</li> <li><code>{name}</code>: The name of the object</li> <li><code>{num_objects}</code>: The number of objects in the <code>Result</code></li> </ul> <p>Additionally, any key in the metadata dictionary can be used.</p> <p>E.g., on initialising the <code>Result</code>, you can pass <code>llm_message = \"Retrieved {num_objects} from {collection_name}\"</code> if you have <code>collection_name</code> in the metadata.</p>"},{"location":"Advanced/custom_objects/#custom-objects","title":"Custom Objects","text":""},{"location":"Advanced/custom_objects/#using-result-directly","title":"Using Result Directly","text":"<p>If you want specific payload types or similar, you can directly use <code>Result</code> and simply change the initialisation parameters for your own use-case. E.g. in the tool call, if we are dealing cards randomly, you can use</p> <p><pre><code>yield Result(\n    objects = [\n        {\"card_title\": \"Jack of Clubs\", \"card_value\": 11},\n        {\"card_title\": \"8 of Diamonds\", \"card_value\": 8}\n    ], \n    metadata = {\"deck_size\": 52},\n    payload_type = \"playing_cards\",\n    name=\"dealt_cards\",\n    llm_message=\"Dealt {num_objects} cards out of a possible {deck_size}.\"\n)\n</code></pre> Here, the <code>llm_message</code> will be input to the <code>tasks_completed</code> field of the <code>tree_data</code>, and when input to any tools using that field, this message will be displayed alongside what prompt was used and the tool called.</p> <p>Also, the frontend payload will use the <code>playing_cards</code> payload type, and the decision tree processing the result will automatically create the correct frontend payload.</p>"},{"location":"Advanced/custom_objects/#defining-a-new-subclass","title":"Defining a New Subclass","text":"<p>You can create your own subclass of the <code>Result</code> class to customise your objects specifically for your use-case. This can be used when you may want some custom rules when the <code>.to_json</code>, <code>.to_frontend</code>, or <code>.llm_parse</code> methods are automatically used.</p> <p>For example, if we just want to </p> <pre><code>class Card(Result):\n    def __init__(\n        self,\n        objects: list[dict],\n        metadata: dict = {},\n        name: str = \"card\",\n        mapping: dict | None = None,\n        llm_message: str | None = None,\n        unmapped_keys: list[str] = [],\n    ):\n        Result.__init__(\n            self,\n            objects=objects,\n            payload_type=\"playing_card\",\n            metadata=metadata,\n            name=name,\n            mapping=mapping,\n            llm_message=llm_message,\n            unmapped_keys=unmapped_keys,\n        )\n\n    def to_json(self, mapping: bool = False) -&gt; list[dict]:\n\n        output_objects = self.objects\n        for card in output_objects:\n            suit = \"\u2665\ufe0f\u2666\ufe0f\u2663\ufe0f\u2660\ufe0f\"[hash(str(card)) % 4] if \"card_title\" in card else \"\ud83c\udccf\"\n            card[\"suit_emoji\"] = suit\n            card[\"is_lucky\"] = card.get(\"card_value\", 0) &gt; 10\n\n        # Apply mapping if requested\n        if mapping and self.mapping:\n            output_objects = self.do_mapping(output_objects)\n\n        return output_objects\n\n    def llm_parse(self):\n        out = f\"\"\"\n        The first object in the hand was {self.objects[0]}.\n        There were {len(self.objects)} cards in the hand in total.\n        \"\"\"\n        if \"deck_size\" in self.metadata:\n            out += f\"There were a possible {self.metadata['deck_size']} cards to be dealt\"\n        return out\n</code></pre> <p>Here, the <code>to_json</code> method was overwritten to add a bit of flavour to the objects, which didn't exist in the original objects retrieved. The <code>llm_parse</code> method was overwritten so it could use information from the objects themselves, which was not available using the generic placeholders.</p>"},{"location":"Advanced/environment/","title":"Environment","text":"<p>The environment is a persistent object across all actions, tools and decisions performed within the Elysia decision tree. It can be used to store global information, such as retrieved objects or information that needs to be seen across all tools and actions.</p>"},{"location":"Advanced/environment/#overview","title":"Overview","text":"<p>The 'Environment' variable contains all objects returned from any tools called by the Elysia decision tree. In essence, it is a dictionary which is keyed by two variables:</p> <ul> <li><code>tool_name</code> (str): the name of the tool used to add this field to the environment</li> <li><code>name</code> (str): a subkey of <code>tool_name</code>, a unique <code>name</code> associated to the returns from that tool. E.g. a collection name from a retrieval.</li> </ul> <p>And when these items are accessed, it is a list of dictionaries, where each dictionary contains two subfields:</p> <ul> <li><code>objects</code> (list[dict])</li> <li><code>metadata</code> (dict)</li> </ul> <p>where each item in <code>objects</code> is a list of objects retrieved during the call of that tool. Each set of objects has its own corresponding metadata. </p> <p>For example, if Elysia calls the 'Query' tool, then the <code>tool_name</code> is <code>\"query\"</code> and the <code>name</code> is the name of the collection queried. Each list of objects has metadata associated with the query used to retrieve the data. So each list of objects has unique metadata.</p> Environment Example  Below is an example of what the environment looks like, after the tools `query` and `aggregate` have been called within this tree session. <pre><code>{\n    \"query\": {\n        \"message_result\": [\n            {\n                \"objects\": [\n                    {\"message_id\": 1, \"message_content\": \"Hi this is an example message about frogs!\"},\n                    {\"message_id\": 2, \"message_content\": \"Hi this is also an example message about reindeer!\"},\n                ], \n                \"metadata\": {\n                    \"collection_name\": \"example_email_messages_collection\",\n                    \"query_search_term\": \"animals\"\n                }\n            },\n        ]\n    },\n    \"aggregate\": {\n        \"pet_food_result\": [\n            {\n                \"objects\": [\n                    {\n                        \"average_price\": 45.99, \n                        \"product_count\": 150, \n                    }\n                ],\n                \"metadata\": {\n                    \"collection_name\": \"pet_food\",\n                    \"group_by\": {\"field\": \"animal\", \"value\": \"frog\"} \n                }\n            }\n        ]\n    }\n}\n</code></pre> This is just an example and not exactly how the structure within Elysia's inbuilt query and aggregate tools behave (they have much more information and would be harder to follow).  Note the levels of indexing the environment.  - The outer most level is the tool name that yielded the result (`\"query\"` and `\"aggregate\"`). - The next level is a `name` parameter associated with the `Result` that was yielded (`\"message_result\"` for query and `\"pet_food_result\"` for aggregate). - After the `name` key, there is a list of dictionaries. This list corresponds to a different result that was yielded within the same tool/name combination. - Each element of the list underneath `name` contains an `objects` and `metadata`, where the metadata is shared amongst all objects in this element."},{"location":"Advanced/environment/#interacting-with-the-environment","title":"Interacting with the Environment","text":"<p>For a full breakdown of all the methods, see the Environment reference page.</p>"},{"location":"Advanced/environment/#automatic-assignment","title":"Automatic Assignment","text":"<p>When yielding a <code>Result</code> object from a Tool, the result's <code>to_json()</code> method will return a list of dictionaries which automatically gets created or appended to the objects field in <code>environment[tool_name][name]</code>. The metadata are added at the same point.</p> <p>This calls the <code>.add()</code> method on the environment using the <code>Result</code> object.</p> Environment Example (cont. pt. 1)  From the `aggregate` tool, we can yield and initialise a `Result` back to the decision tree, where it is processed by the tree logic:  <pre><code>yield Result(\n    name=\"pet_food_result\",\n    objects = [\n        {\n            \"average_price\": 12.52, \n            \"product_count\": 33,\n        }\n    ],\n    metadata = {\n        \"collection_name\": \"pet_food\",\n        \"group_by\": {\"field\": \"animal\", \"value\": \"reindeer\"} \n    }\n)\n</code></pre>  And the updated environment looks like:  <pre><code>{\n    \"query\": {\n        \"message_result\": [...]\n    },\n    \"aggregate\": {\n        \"pet_food_result\": [\n            {\n                \"objects\": [\n                    {\n                        \"average_price\": 45.99, \n                        \"product_count\": 150, \n                    }\n                ],\n                \"metadata\": {\n                    \"collection_name\": \"pet_food\",\n                    \"group_by\": {\"field\": \"animal\", \"value\": \"frog\"} \n                }\n            },\n            {\n                \"objects\": [\n                    {\n                        \"average_price\": 12.52, \n                        \"product_count\": 33,\n                    }\n                ],\n                \"metadata\": {\n                    \"collection_name\": \"pet_food\",\n                    \"group_by\": {\"field\": \"animal\", \"value\": \"reindeer\"} \n                }\n            }\n        ]\n    }\n}\n</code></pre> Notice how a new entry was not added to either the first or second level of the environment dictionary, but was instead appended to the existing entries under `aggregate -&gt; pet_food_result`"},{"location":"Advanced/environment/#add-and-add_objects","title":"<code>.add()</code> and <code>.add_objects()</code>","text":"<p>See the reference page.</p> <p>When calling a tool, you can specifically add a <code>Result</code> object to the environment via  <pre><code>environment.add(tool_name, Result)\n</code></pre> The corresponding <code>to_json()</code> method in the <code>Result</code> is used to obtain the objects which get added.</p> <p>You can also have more control over which objects get added specifically by using <pre><code>environment.add_objects(tool_name, name, objects, metadata)\n</code></pre> where <code>objects</code> is a list of dictionaries, <code>metadata</code> is a dictionary and <code>tool_name</code> and <code>name</code> are string identifiers.</p> Environment Example (cont. pt. 2) If we were to do <pre><code>frog_result = Result(\n    objects = [\n        {\n            \"animal\": \"frog\",\n            \"description\": \"Green and slimy\"\n        }\n    ],\n    name=\"animal_description\"\n)\nenvironment.add(tool_name=\"descriptor\", result=frog_result)\n</code></pre> Then the environment would be updated to  <pre><code>{\n    \"query\": {\n        \"message_result\": [...]\n    },\n    \"aggregate\": {\n        \"pet_food_result\": [...]\n    }\n    \"descriptor\": {\n        \"animal_description\": [\n            {\n                \"objects\": [\n                    {\n                        \"animal\": \"frog\",\n                        \"description\": \"Green and slimy\"\n                    }\n                ],\n                \"metadata\": {}\n            }\n        ]\n    }\n}\n</code></pre> Even though we never interfaced with a tool called `descriptor`."},{"location":"Advanced/environment/#replace","title":"<code>.replace()</code>","text":"<p>See the reference page.</p> <p>Change an item in the environment with another item  <pre><code>environment.replace(tool_name, name, objects, metadata, index)\n</code></pre> either replace the entire list of items (objects + metadatas) or a single item at a particular index. <code>index</code> will only replace a particular item at that location, or if <code>None</code> (default) will replace the entire list.</p> Environment Example (cont. pt. 3) If we were to change the results from the `\"descriptor\"` to something else, <pre><code>environment.replace(\n    tool_name=\"descriptor\", \n    name=\"animal_description\",\n    objects = [{\"animal\": \"reindeer\", \"description\": \"Has a red nose\"}]\n)\n</code></pre> Then the environment would be updated to  <pre><code>{\n    \"query\": {\n        \"message_result\": [...]\n    },\n    \"aggregate\": {\n        \"pet_food_result\": [...]\n    }\n    \"descriptor\": {\n        \"animal_description\": [\n            {\n                \"objects\": [\n                    {\n                        \"animal\": \"reindeer\",\n                        \"description\": \"Has a red nose\"\n                    }\n                ],\n                \"metadata\": {}\n            }\n        ]\n    }\n}\n</code></pre>"},{"location":"Advanced/environment/#find","title":"<code>.find()</code>","text":"<p>See the reference page.</p> <p>You can use <pre><code>environment.find(tool_name, name, index)\n</code></pre> Which is an easy way to retrieve objects from the environment associated with the <code>tool_name</code> and <code>name</code> string identifiers. <code>index</code> is a parameter which finds the corresponding location of an item for these identifiers. Defaults to <code>None</code>, in which case it returns a list of all items.</p> <p>This is essentially just an alias to <code>environment.environment[tool_name][name][index]</code>.</p>"},{"location":"Advanced/environment/#remove","title":"<code>.remove()</code>","text":"<p>See the reference page.</p> <p>Items (objects + metadata) in the environment can be removed via <pre><code>environment.remove(tool_name, name, index)\n</code></pre> which uses the <code>tool_name</code> and <code>name</code> string identifiers to find the corresponding item. The <code>index</code> parameter will remove the objects and metadata associated only with that position in the list. E.g., if <code>index=-1</code>, then the most recent entry in the list will be deleted. This defaults to <code>None</code>, in which case the entire set of objects for <code>tool_name</code> and <code>name</code> are removed. You can, of course, check the length of items beforehand via <code>len(environment.find(tool_name, name))</code> and use that to define the index. Will raise an <code>IndexError</code> if that index does not exist.</p>"},{"location":"Advanced/environment/#the-hidden-environment","title":"The Hidden Environment","text":"<p>Within the environment there is also a dictionary, <code>environment.hidden_environment</code>, designed to be used as a store of data that is not shown to the LLM. You can save any type of object within this dictionary as it does not need to be converted to string to converted to LLM formatting.</p> <p>For example, this could be used to save raw retrieval objects that are not converted to their simple object properties, so you can still access the metadata output from the retrieval method that you otherwise wouldn't save inside the object metadata.</p>"},{"location":"Advanced/environment/#some-quick-usecases","title":"Some Quick Usecases","text":"<ul> <li>You may want to create a tool that only runs when the environment is non empty, so the <code>run_if_true</code> method of the tool (see here for details) returns <code>not tree_data.environment.is_empty()</code>.</li> <li>Your tool may not want to return any objects to the frontend, so instead of returning specific <code>Result</code> objects, you could modify the environment via <code>.add_objects()</code>, <code>.replace()</code> and <code>.remove()</code>. This stores 'private' variables that are not seen by the user unless they can manually inspect the environment.</li> </ul>"},{"location":"Advanced/local_models/","title":"Local Models","text":"<p>Elysia integrates its LLM connections via DSPy, which uses LiteLLM under the hood. The easiest way to get connected is via Ollama, but it is also possible to connect to OpenAI compatible endpoints. </p>"},{"location":"Advanced/local_models/#getting-connected","title":"Getting Connected","text":""},{"location":"Advanced/local_models/#connecting-via-ollama","title":"Connecting via Ollama","text":"<p>First, make sure your Ollama server is running either via the Ollama app or <code>ollama run &lt;model_name&gt;</code>. E.g., <code>ollama run gpt-oss:20b</code>, which we'll use in this example. Within Python, you can configure your model API base to your Ollama api endpoint (default to <code>http://localhost:11434</code>) via the <code>model_api_base</code> parameter of <code>configure</code>.</p> <pre><code>from elysia import configure\nconfigure(\n    base_provider=\"ollama\",\n    complex_provider=\"ollama\",\n    base_model=\"gpt-oss:20b\",\n    complex_model=\"gpt-oss:20b\",\n    model_api_base=\"http://localhost:11434\",\n)\n</code></pre> <p>On the app side, this is configurable via the 'Api Base URL' parameter in the Settings. Set both of your providers to <code>ollama</code>, and your base and complex model to whatever model you are currently hosting, and this should work out-of-the-box.</p>"},{"location":"Advanced/local_models/#connecting-via-openai-compatible-endpoints-experimental","title":"Connecting via OpenAI-Compatible Endpoints (Experimental)","text":"<p>See the LiteLLM docs for more detail on using OpenAI compatible endpoints. In short, you can set your provider to <code>openai</code> and create a fake API key (e.g. <code>OPENAI_API_KEY=fake-key</code>) to enable connection to an OpenAI endpoint that is not hosted by OpenAI. You will also need to supply a <code>model_api_base</code> to the Elysia config which will point towards where your model is hosted. E.g.</p> <pre><code>from elysia import configure\nconfigure(\n    base_provider=\"openai\",\n    complex_provider=\"openai\",\n    base_model=\"&lt;your_model_name_here&gt;\",\n    complex_model=\"&lt;your_model_name_here&gt;\",\n    openai_api_key=\"...\",\n    model_api_base=\"...\"\n)\n</code></pre>"},{"location":"Advanced/local_models/#warning","title":"Warning","text":"<p>Elysia uses a long context, quite long context, due to the nature of the collection schemas, long instruction sets, environment and more being included in every prompt. So these models can and will probably run quite slowly, if hosted on a machine with low compute power (e.g. not a high powered GPU). </p>"},{"location":"Advanced/local_models/#recommendations","title":"Recommendations","text":""},{"location":"Advanced/local_models/#model-choice","title":"Model Choice","text":"<p>The <code>gpt-oss</code> family of models have been shown to work well with Elysia, and can handle the structured outputs well. These are trained specifically for agentic tasks and reasoning chains.</p> <p>Please let us know on the GitHub discussions if you've had any success with any other models.</p>"},{"location":"Advanced/local_models/#speeding-up","title":"Speeding Up","text":"<p>Disabling complex Weaviate integration to Elysia</p> <p>Within the python package, you can configure the Elysia process to be faster by disabling connection to your Weaviate cluster, if applicable, by removing your weaviate api key and url. Or, there is an optional setting on initialising the <code>Tree</code> to disable using Weaviate collections in Elysia:</p> <pre><code>from elysia import Tree\ntree = Tree(use_elysia_collections=False)\n</code></pre> <p>Setting this to <code>False</code> (default <code>True</code>) will disable the Elysia decision agent having access to the preprocessed schemas for any connected Weaviate collections. If you are not using a complex Weaviate integration, then this is safe to disable.</p> <p>Note that this also disables the inbuilt Query and Aggregate tools that Elysia is by default initialised to. If you are doing this, you should also add your own custom tools to Elysia for it to be worth anything! See here for an intro to creating tools.</p> <p>Disabling chain of thought reasoning (Experimental)</p> <p>One of the biggest slowdowns of LMs is the number of output tokens they produce. There is an experimental configuration option for removing models from outputting chain of thought reasoning at every step, done so via:</p> <pre><code>from elysia import configure\nconfigure(\n    base_use_reasoning=False,\n    complex_use_reasoning=False\n)\n</code></pre> <p>You can choose to disable just for the base model (e.g. the decision agent) or just the complex model (which some tools will use). Custom tools can also make use of the base/complex models also via the custom DSPy Module <code>ElysiaChainOfThought</code>.</p> <p>Use this with caution - it will degrade accuracy significantly. </p> <p>Future Plans</p> <p>Future versions of Elysia will hopefully include a simplified version of the system instructions and inputs to the decision agent/tools, that can be enabled via a flag in configure, e.g. <code>configure(..., simplify=True)</code> that will shrink the context size cleverly.</p> <p>Stay tuned for more improvements coming to local models in Elysia by following/starring the GitHub repository. Or feel free to make a contribution!</p>"},{"location":"Advanced/local_models/#troubleshooting","title":"Troubleshooting","text":""},{"location":"Advanced/local_models/#when-using-local-models-elysia-times-out-in-the-app-and-i-get-an-error","title":"When using local models, Elysia times out in the app, and I get an error","text":"<p>Try configuring your Tree Timeout in the configuration page to be higher. If a single request takes longer than this value, the conversation will time out and lead to an error.</p>"},{"location":"Advanced/local_models/#im-getting-random-errors-that-dont-seem-to-make-sense","title":"I'm getting random errors that don't seem to make sense","text":"<p>This could be one of many things:</p> <ul> <li>The conversation could be timing out (see above)</li> <li>Your model connection is failing</li> <li>A smaller local model may be failing to include every output in the response, or failing the structured output of DSPy. Try a larger model if you can with the same prompt, to see if the error persists.  If all the errors continue happening, open a GitHub issue!</li> </ul>"},{"location":"Advanced/local_models/#nothing-is-helping-elysia-still-isnt-running-with-my-local-model","title":"Nothing is helping, Elysia still isn't running with my local model","text":"<p>In Python, test the connection to your local model through LiteLLM directly:</p> <pre><code>from litellm import completion\n\nresponse = completion(\n    model=\"ollama/gemma3:4b\", # or whichever model you are using\n    messages=[{ \"content\": \"hi\", \"role\": \"user\"}], \n    api_base=\"http://localhost:11434\"\n)\nprint(response)\n</code></pre> <p>If the response is failing, then there is likely a problem with your connection to your model or Ollama (or very unlikely, LiteLLM). If this works, then try the connection in Elysia:</p> <pre><code>from elysia import Tree, Settings\n\nsettings = Settings()\nsettings.configure(\n    base_model=\"gemma3:4b\",    # or whichever model you are using\n    complex_model=\"gemma3:4b\", # or whichever model you are using\n    base_provider=\"ollama\",\n    complex_provider=\"ollama\",\n    model_api_base=\"http://localhost:11434\", # or wherever your Ollama instance is \n)\ntree = Tree(settings=settings)\n</code></pre> <p>Then:</p> <pre><code>print(tree.base_lm(\"hi\")) # should be a generic response without using elysia\n</code></pre> <p>This should be a direct calling of the LM, so a quick response with not a large amount of input tokens. Then you can run the decision tree:</p> <pre><code>tree(\"hi\") \n</code></pre> <p>This now includes all context:</p> <ul> <li>System instructions for the decision tree</li> <li>Tool descriptions</li> <li>Conversation history</li> <li>Items in the internal environment (can be very large after processing requests like queries)</li> <li>Collection schemas (this is a big one)</li> </ul> <p>So the request will take a lot longer. Leave this for as long as it needs. It might take a while - that's fine because we are just testing the connection.</p> <p>If the model works (via <code>tree.base_lm(\"hi\")</code>) but this step errors, it is either the model doing something wrong, or another error.</p> <p>If it doesn't look like the model is doing something wrong, open a GitHub issue, including a full error log from the python terminal.</p> <p>If it is just taking a long time, then you may want to try a smaller model (not recommended currently) or finding access to some larger compute.</p>"},{"location":"Advanced/technical_overview/","title":"Technical Overview","text":"<p>Let's break down the Elysia decision tree, how exactly it runs and how objects persist over multiple iterations. </p> <p>We will consider a 'standard' Elysia set up as an example, where we have access to two tools: 'Query' and 'Aggregate', which both interact with custom data. There are two additional tools: 'Summarize' and 'Text Response', which both provide text outputs (with slightly different specifications).</p> <p></p>"},{"location":"Advanced/technical_overview/#decision-agent","title":"Decision Agent","text":"<p>The decision agent (which is run from the <code>base_model</code>) is responsible for choosing the tools to call/nodes for each decision step. The decision tree structure consists of 'branches', which are subcategories for tools, used to separate out different tools if there are many of them, and tools, which are actions to perform. Tools should add information to the Elysia environment. The environment is used within the decision agent as well as any tools so the process is aware of any retrieved or collected data.</p> <p>The inputs to the decision tree are:</p> <ul> <li>The tree data (see below).</li> <li>Tool descriptions.</li> <li>Instruction for the branch (e.g. how to make the choice between the currently available tools).</li> <li>Metadata on the tree, including how many loops through the tree has been made, as well as future tools that exist within a branch.</li> </ul> <p>The decision agent assigns the following outputs:</p> <ul> <li>Tool to use.</li> <li>Inputs to the tool if they exist.</li> <li>Whether to end the tree after calling this tool (although this is conditional on the tool also allowing the process to end after its call).</li> <li>A message update to the user.</li> <li>Whether the task is impossible given all other information.</li> </ul>"},{"location":"Advanced/technical_overview/#tree-data","title":"Tree Data","text":"<p>The tree data is a subset of the most important data in the Elysia decision tree. This includes the most pertinent information, such as the user prompt and the conversation history, as well as the Atlas, an alias to the style, agent description and end goal that the agent must adhere to, and the environment, a collection of all data retrieved or collected by Elysia during tool evaluations. A history of completed tasks and custom formatted text from the tools is also included. Any errors (that were manually caught and yielded) during tool evaluations are also included into the tree data, so that the decision agent or subsequent runs of the tools can be aware of previous failures.</p> <p>The tree data is included in the decision agent and can be also included in any LLM calls within tools. The tree data is essentially the way that tools 'interact' with Elysia, by including information of the state of the tree and updating this state information.</p> <p>To see a full breakdown of the tree data, see the description for the <code>TreeData</code> class. </p>"},{"location":"Advanced/tool_discovery/","title":"Tool Discovery and Modular Initialization","text":""},{"location":"Advanced/tool_discovery/#overview","title":"Overview","text":"<p>Elysia implements a modular tool discovery and initialization system that: 1. Centralizes default tool configurations 2. Automatically discovers and loads MCP tools 3. Provides utilities for generating tool discovery YAML configurations 4. Maintains separation between UI layer and core Tree logic</p>"},{"location":"Advanced/tool_discovery/#system-components","title":"System Components","text":""},{"location":"Advanced/tool_discovery/#1-tool-discovery-utility-elysiautiltool_discoverypy","title":"1. Tool Discovery Utility (<code>elysia/util/tool_discovery.py</code>)","text":"<p>Purpose: Discover all available tools and generate YAML configurations</p> <p>Key Functions:</p> <ul> <li><code>discover_tools_from_module()</code>: Finds all Tool subclasses from custom_tools and MCP modules</li> <li><code>get_tool_metadata()</code>: Extracts metadata from discovered tools</li> <li><code>generate_tool_discovery_yaml()</code>: Creates YAML configuration file</li> <li><code>get_tools_by_category()</code>: Filters tools by category</li> </ul> <p>Circular Import Handling: Uses <code>TYPE_CHECKING</code> and late imports to avoid circular dependencies</p>"},{"location":"Advanced/tool_discovery/#2-ui-module-for-default-tools-elysiatoolsuidefault_toolspy","title":"2. UI Module for Default Tools (<code>elysia/tools/ui/default_tools.py</code>)","text":"<p>Purpose: Centralize tool loading logic for different initialization modes</p> <p>Key Features:</p>"},{"location":"Advanced/tool_discovery/#default_tool_configs","title":"DEFAULT_TOOL_CONFIGS","text":"<p>Dictionary defining tool configurations for each mode:</p> <ul> <li><code>multi_branch</code>: Organizes tools into \"base\" and \"search\" branches</li> <li><code>one_branch</code>: All tools in a single \"base\" branch  </li> <li><code>empty</code>: No tools loaded by default</li> </ul>"},{"location":"Advanced/tool_discovery/#load_default_tools_for_mode","title":"load_default_tools_for_mode()","text":"<p>Main function that loads tools based on mode: - Automatically creates branches - Adds tools to appropriate branches - Supports <code>additional_tool_classes</code> parameter for dynamic tools - Auto-discovers and loads MCP tools</p> <p>Circular Import Handling: Direct imports from tool modules, <code>TYPE_CHECKING</code> for Tree</p>"},{"location":"Advanced/tool_discovery/#3-tree-class-integration-elysiatreetreepy","title":"3. Tree Class Integration (<code>elysia/tree/tree.py</code>)","text":"<p>Simplified Methods: - <code>multi_branch_init()</code>, <code>one_branch_init()</code>, and <code>empty_init()</code> use <code>load_default_tools_for_mode()</code></p> <p>Auto-Loading Method: - <code>_load_additional_discovered_tools()</code>: Runs after base initialization   - Automatically discovers and loads MCP tools   - Filters for tools with \"mcp\" in their name   - Adds them to the root branch</p>"},{"location":"Advanced/tool_discovery/#4-tool-discovery-yaml-configuration","title":"4. Tool Discovery YAML Configuration","text":"<p>File: <code>elysia/config/discovered_tools.yaml</code></p> <p>Structure: <pre><code>discovered_tools:\n  retrieval:\n    Query: \n      class_name: Query\n      name: query\n      description: Queries the knowledge base...\n      available: true\n    Aggregate: \n      class_name: Aggregate\n      name: aggregate\n      description: Aggregates information...\n      available: true\n  text:\n    CitedSummarizer: {...}\n    FakeTextResponse: {...}\n  visualization:\n    BasicLinearRegression: {...}\n    Visualise: {...}\n  postprocessing:\n    SummariseItems: {...}\n  mcp:\n    # MCP tools dynamically added when configured\n  other: {}\n</code></pre></p>"},{"location":"Advanced/tool_discovery/#api-flow","title":"API Flow","text":""},{"location":"Advanced/tool_discovery/#inittree-endpoint-flow","title":"/init/tree Endpoint Flow","text":"<ol> <li>User Request \u2192 <code>/init/tree/{user_id}/{conv_id}</code></li> <li>UserManager.initialise_tree() \u2192 Creates tree via TreeManager</li> <li>TreeManager.add_tree() \u2192 Creates new Tree instance</li> <li>Tree.init() \u2192 Calls <code>set_branch_initialisation()</code></li> <li>set_branch_initialisation() \u2192</li> <li>Calls mode-specific init (e.g., <code>one_branch_init()</code>)</li> <li><code>one_branch_init()</code> \u2192 <code>load_default_tools_for_mode(tree, \"one_branch\")</code></li> <li>Then calls <code>_load_additional_discovered_tools()</code></li> <li>_load_additional_discovered_tools() \u2192</li> <li>Discovers all tools via <code>discover_tools_from_module()</code></li> <li>Filters for MCP tools (tools with \"mcp\" in name)</li> <li>Adds them to root branch via <code>add_tool()</code></li> <li>Tree Ready \u2192 MCP tools now available alongside default tools</li> </ol>"},{"location":"Advanced/tool_discovery/#key-benefits","title":"Key Benefits","text":"<ol> <li>Modularity: Tool loading logic is centralized and easily maintainable</li> <li>Extensibility: New tools are automatically discovered and can be auto-loaded</li> <li>MCP Integration: MCP tools configured in <code>mcp.json</code> are automatically added to trees</li> <li>No Code Changes Needed: Just configure <code>mcp.json</code> and tools appear</li> <li>Backward Compatible: Existing tool addition methods still work</li> <li>Clear Separation: UI layer separate from core Tree logic</li> </ol>"},{"location":"Advanced/tool_discovery/#usage-examples","title":"Usage Examples","text":""},{"location":"Advanced/tool_discovery/#basic-usage-automatic","title":"Basic Usage (Automatic)","text":"<pre><code>from elysia import Tree\n\n# MCP tools automatically loaded if configured in mcp.json\ntree = Tree()  # or Tree(branch_initialisation=\"one_branch\")\n</code></pre>"},{"location":"Advanced/tool_discovery/#advanced-usage-custom-tools","title":"Advanced Usage (Custom Tools)","text":"<pre><code>from elysia import Tree\nfrom elysia.tools.ui import load_default_tools_for_mode\nfrom my_tools import MyCustomTool\n\ntree = Tree(branch_initialisation=\"empty\")\nload_default_tools_for_mode(\n    tree,\n    \"one_branch\",\n    additional_tool_classes=[MyCustomTool]\n)\n</code></pre>"},{"location":"Advanced/tool_discovery/#generate-tool-discovery-yaml","title":"Generate Tool Discovery YAML","text":"<pre><code>from elysia.util import generate_tool_discovery_yaml\n\ngenerate_tool_discovery_yaml('elysia/config/discovered_tools.yaml')\n</code></pre> <p>Or use the helper script:</p> <pre><code>python elysia/generate_discovered_tools.py\n</code></pre>"},{"location":"Advanced/tool_discovery/#default-tool-configurations","title":"Default Tool Configurations","text":""},{"location":"Advanced/tool_discovery/#one-branch-mode","title":"One Branch Mode","text":"<p>All tools in a single \"base\" branch: - CitedSummarizer - FakeTextResponse - Aggregate - Query (with SummariseItems dependency) - Visualise</p>"},{"location":"Advanced/tool_discovery/#multi-branch-mode","title":"Multi Branch Mode","text":"<p>Tools organized into branches:</p> <p>Base Branch: - CitedSummarizer - FakeTextResponse - Visualise</p> <p>Search Branch: (from base) - Aggregate - Query (with SummariseItems dependency)</p>"},{"location":"Advanced/tool_discovery/#empty-mode","title":"Empty Mode","text":"<p>No default tools loaded - start with a clean slate.</p>"},{"location":"Advanced/tool_discovery/#files-in-the-system","title":"Files in the System","text":"<p>Created: - <code>elysia/util/tool_discovery.py</code> - <code>elysia/tools/ui/default_tools.py</code> - <code>elysia/tools/ui/__init__.py</code> - <code>elysia/config/discovered_tools.yaml</code> - <code>elysia/generate_discovered_tools.py</code> (helper script)</p> <p>Modified: - <code>elysia/tree/tree.py</code> - <code>elysia/util/__init__.py</code></p>"},{"location":"Advanced/tool_discovery/#mcp-tool-integration","title":"MCP Tool Integration","text":"<p>To use MCP tools with the discovery system:</p> <ol> <li> <p>Configure MCP servers in <code>elysia/mcp.json</code>: <pre><code>{\n  \"servers\": [\n    {\n      \"name\": \"my_server\",\n      \"description\": \"My MCP server\",\n      \"server_script_path\": \"/path/to/server.py\",\n      \"enabled\": true\n    }\n  ]\n}\n</code></pre></p> </li> <li> <p>Restart the service - MCP tools will be automatically discovered and added to trees</p> </li> <li>Verify via <code>/tools/available</code> endpoint</li> </ol> <p>The system is ready for MCP tool integration with zero additional code changes required!</p>"},{"location":"Advanced/tool_discovery/#advanced-customizing-tool-loading","title":"Advanced: Customizing Tool Loading","text":"<p>For advanced use cases, you can customize which tools get loaded:</p> <pre><code>from elysia import Tree\nfrom elysia.tools.ui import DEFAULT_TOOL_CONFIGS, load_default_tools_for_mode\n\n# Get the default config for one_branch mode\nconfig = DEFAULT_TOOL_CONFIGS[\"one_branch\"]\n\n# Modify the config as needed\n# Then use it with a custom tree initialization\n\n# Or load tools selectively\nfrom elysia.util.tool_discovery import discover_tools_from_module\n\nall_tools = discover_tools_from_module()\nretrieval_tools = [cls for name, cls in all_tools.items() \n                   if \"query\" in name.lower() or \"aggregate\" in name.lower()]\n\ntree = Tree(branch_initialisation=\"empty\")\nfor tool_cls in retrieval_tools:\n    tree.add_tool(tool=tool_cls, branch_id=\"base\")\n</code></pre> <p>See also: - Creating Tools - How to create custom tools - MCP Integration - MCP-specific documentation - Advanced Tool Construction - In-depth tool creation guide</p>"},{"location":"Examples/","title":"Index","text":"<ol> <li>Querying a Weaviate database - create an example collection in Weaviate and use Elysia with its core functionality to retrieve data.</li> <li>Basic Linear Regression - creating a custom Elysia tool to perform a basic least squares regression on items retrieved from the <code>query</code> tool.</li> </ol> <p>More examples coming soon!</p>"},{"location":"Examples/data_analysis/","title":"Basic Linear Regression","text":"<p>Let's create a tool which performs linear regression on numeric data retrieved from a Weaviate collection. For this example, we will use the default branch initialisation for the decision tree (<code>\"one_branch\"</code>), which includes tools for querying and retrieving data from a Weaviate collection. We are going to add a <code>fit_linear_regression</code> tool that extracts data from the environment, fits a linear regression model using scikit-learn's linear regression, plots and returns the results.</p> <p>This example requires installing scikit learn as follows (with <code>pip</code>): <pre><code>pip install -U scikit-learn\n</code></pre></p>"},{"location":"Examples/data_analysis/#setup","title":"Setup","text":""},{"location":"Examples/data_analysis/#configuring-elysia","title":"Configuring Elysia","text":"<p>You can set model API keys (e.g. <code>OPENAI_API_KEY</code> or <code>OPENROUTER_API_KEY</code>) and Weaviate API keys (i.e. <code>WEAVIATE_API_KEY</code>, <code>WEAVIATE_URL</code>) in your local <code>.env</code> file, or configure Elysia in python via:</p> <pre><code>configure(\n    base_model=\"gemini-2.0-flash-001\",  # replace models and providers with which ever LM you want to use\n    complex_model=\"gemini-2.0-flash-001\",\n    base_provider=\"openrouter/google\",\n    complex_provider=\"openrouter/google\",\n    wcd_url=\"...\",  # replace with your Weaviate REST endpoint URL\n    wcd_api_key=\"...\",  # replace with your Weaviate cloud API key\n    openrouter_api_key=\"...\",  # replace with whichever API key you will use for your LMs\n)\n</code></pre> <p>This handles which models you will use for preprocessing and decision tree evaluations, as well as sets any API keys if not in your local <code>.env</code> file.</p>"},{"location":"Examples/data_analysis/#downloading-and-processing-data","title":"Downloading and Processing Data","text":"<p>We will use an example diabetes dataset provided by scikit learn for this example, loaded in python via:</p> <pre><code>from sklearn import datasets\n\ndata = datasets.load_diabetes()\nX, Y = data.data, data.target\n</code></pre> <p>And then import to Weaviate as follows:</p> <pre><code>from elysia.util.client import ClientManager\nimport weaviate.classes.config as wvc\n\nwith ClientManager().connect_to_client() as client:\n    collection = client.collections.create(\n        \"Diabetes\", vector_config=wvc.Configure.Vectors.self_provided()\n    )\n\n    with collection.batch.dynamic() as batch:\n        for i in range(len(X)):\n            batch.add_object({\"predictor\": X[i, 0], \"target\": Y[i]})\n</code></pre> <p>Then this collection requires preprocessing with Elysia:</p> <pre><code>from elysia import preprocess\npreprocess(\"Diabetes\")\n</code></pre>"},{"location":"Examples/data_analysis/#tool-construction","title":"Tool Construction","text":"<p>We will follow the follow the basic guideline for creating a tool. The general outline for this tool is going to be as follows:</p> <ul> <li>No LLM calls within the tool, the decision agent is going to be deciding inputs for the regression.</li> <li>The call of the tool is going to use the <code>LinearRegression</code> class from <code>sklearn</code> to fit the model.</li> <li>The tool is going to add to the environment the labelled coefficients from the regression.</li> </ul> <p>To create a tool in Elysia, you can just write a python function and add the <code>@tool</code> decorator.</p> <pre><code>from elysia import tool\nfrom sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\n\n@tool\nasync def fit_linear_regression(env_key, x_var, y_var, collection_name, tree_data):\n    \"\"\"\n    Fit a linear regression model to data. \n    Requires querying for data first.\n\n    Args:\n        env_key: The key of the environment to use (e.g. 'query').\n        x_var: Independent variable field name in environment under the key.\n        y_var: Dependent variable field name in environment under the key.\n    \"\"\"\n    objs = tree_data.environment.find(env_key, collection_name, 0)[\"objects\"]\n    X = [[datum.get(x_var)] for datum in objs]\n    Y = [datum.get(y_var) for datum in objs]\n\n    model = LinearRegression().fit(X, Y)\n\n    plt.scatter(X, Y)\n    plt.plot(X, model.predict(X), color=\"red\")\n    plt.show()\n\n    return {\n        \"intercept\": model.intercept_,\n        \"coef\": model.coef_,\n        \"collection_name\": collection_name\n    }\n</code></pre> <ul> <li>The tool description is taken from the docstring of the function.</li> <li>In this case, the inputs themselves do not receive individual descriptions, but they are provided via the tool description.</li> <li>The <code>env_key</code>, <code>x_var</code>, <code>y_var</code> and <code>collection_name</code> arguments are specific to this tool (so they are decided by the decision agent). But the <code>tree_data</code> argument is a protected argument which will use the data in the decision tree, which is how Elysia interacts with its context and environment.</li> <li>The returned dictionary adds the items returned will be added to Elysia's environment, so the decision tree can read this information on a future run.</li> </ul>"},{"location":"Examples/data_analysis/#running-the-tool","title":"Running the Tool","text":"<p>Let's initialise a tree, add the tool and test it runs correctly. </p> <p><pre><code>from elysia import Tree\ntree = Tree()\ntree.add_tool(fit_linear_regression)\nresponse, objects = tree(\n    \"Fit a linear regression on the Diabetes data\",\n    collection_names = [\"Diabetes\"]\n)\n</code></pre> And some relevant output from the run: <pre><code>DEBUG    Tasks completed (iteration 1):\n            - query (Avg. 6.67 seconds)                                                         \n            - query_postprocessing (Avg. 0.00 seconds)                                          \n\nDEBUG    Tasks completed (iteration 2):\n            - fit_linear_regression (Avg. 0.04 seconds)                                         \n\nDEBUG    Tasks completed (iteration 3):\n            - cited_summarize (Avg. 2.52 seconds)      \n</code></pre></p> <p><pre><code>print(response)\n</code></pre> <code>I will first query the Diabetes data to prepare for fitting the linear regression model. I have queried the Diabetes data to prepare for fitting the linear regression model. I will now fit a linear regression model using 'predictor' as the independent variable and 'target' as the dependent variable from the queried Diabetes data. I have now fitted a linear regression model to the Diabetes data, and will summarise the results. A linear regression model was fitted to the Diabetes dataset using 'predictor' as the independent variable and 'target' as the dependent variable. The model's intercept is approximately 155.82, indicating the expected value of 'target' when 'predictor' is zero. The coefficient for 'predictor' is approximately 468.45, suggesting that for every one-unit increase in 'predictor', the 'target' variable is expected to increase by 468.45 units, on average.</code></p> <p></p> <p>Notice how the model was able to read what the intercept and coefficient values were, based on what was returned from the tool.</p>"},{"location":"Examples/email/","title":"Sending an Automated Email","text":"<p>This example will go through a</p>"},{"location":"Examples/fantasy_adventure/","title":"Example: Creating a Fantasy Adventure Game","text":"<p>Coming soon.</p>"},{"location":"Examples/old_data_analysis/","title":"Old data analysis","text":"<p>Let's create a tool which performs a least squares regression on numeric data retrieved from a Weaviate collection. For this example, we will use the default branch initialisation for the decision tree (<code>\"one_branch\"</code>), which includes tools for querying and retrieving data from a Weaviate collection. We are going to add a <code>BasicLinearRegression</code> tool that extracts data from the environment and solves the least squares problem to obtain coefficients for the regression.</p>"},{"location":"Examples/old_data_analysis/#initial-setup","title":"Initial Setup","text":"<p>Let's follow the basic guideline for creating a tool, and create the skeleton of the tool below.</p> <pre><code>from elysia import Error, Tool, Result\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass BasicLinearRegression(Tool):\n\n    def __init__(self, logger, **kwargs):\n        pass\n\n    async def __call__(\n        self,\n        tree_data,\n        inputs,\n        base_lm,\n        complex_lm,\n        client_manager,\n        **kwargs,\n    ):\n        pass\n\n    async def is_tool_available(self, tree_data, base_lm, complex_lm, client_manager):\n        pass\n</code></pre> <p>The general outline for this tool is going to be as follows:</p> <ul> <li>No LLM calls within the tool, the decision agent is going to be deciding inputs for the regression.</li> <li>The call of the tool is going to transform the environment into matrices to be used for the analysis.</li> <li>The tool is going to add to the environment the labelled coefficients from the regression.</li> <li>The <code>is_tool_available</code> function is going to return <code>True</code> when there is data retrieved from the <code>query</code> tool in the environment.</li> </ul>"},{"location":"Examples/old_data_analysis/#tool-initialisation","title":"Tool Initialisation","text":"<p>Let's set the initialisation parameters to provide a comprehensive tool description and input descriptions.</p> <pre><code>    def __init__(self, logger, **kwargs):\n        super().__init__(\n            name=\"basic_linear_regression_tool\",\n            description=\"\"\"\n            Use this tool to perform linear regression on objects in the environment with numeric data types.\n            \"\"\",\n            status=\"Running linear regression...\",\n            inputs={\n                \"environment_key\": {\n                    \"description\": (\n   \"A single key (string) of the `environment` dictionary that will be used in the analysis. \"\n                        \"Choose the most relevant key for the analysis according to the user prompt. \"\n                        \"All objects under that key will be used to create the dataframe. \"\n                    ),\n                    \"required\": True,\n                    \"type\": str,\n                    \"default\": None,\n                },\n                \"x_variable_fields\": {\n                    \"description\": (\n                        \"The independent variables for the regression. \"\n                        \"Choose one or more fields within the `objects` underneath the specific `environment_key`. \"\n                    ),\n                    \"required\": True,\n                    \"type\": list[str],\n                    \"default\": None,\n                },\n                \"y_variable_field\": {\n                    \"description\": (\n                        \"The dependent variable for the regression. \"\n                        \"Choose one single field within the `objects` underneath the specific `environment_key`. \"\n                    ),\n                    \"required\": True,\n                    \"type\": str,\n                    \"default\": None,\n                },\n            },\n            end=False,\n        )\n</code></pre>"},{"location":"Examples/old_data_analysis/#tool-call","title":"Tool Call","text":"<p>The tool call is the <code>__call__</code> method - this will be called when the tool is selected by the LLM. So this is where the actual tool execution takes place. We need to do several things here:</p> <ol> <li>Convert the environment (within the <code>tree_data</code>) to matrices</li> <li>Perform least squares regression on these matrices</li> <li>Yield relevant objects to the decision tree, so the information goes back to the tree</li> </ol> <p>Let's first start with setting the tool up and collecting the inputs: <pre><code>    async def __call__(\n        self,\n        tree_data,\n        inputs,\n        base_lm,\n        complex_lm,\n        client_manager,\n        **kwargs,\n    ):\n        environment = tree_data.environment.environment\n        environment_key = inputs[\"environment_key\"]\n        x_variable_field = inputs[\"x_variable_field\"]\n        y_variable_field = inputs[\"y_variable_field\"]\n</code></pre></p> <p>Now we have separated the relevant inputs and items we need, let's now proceed with converting the environment (within the <code>tree_data</code>) to matrices:</p> <p><pre><code>        # initialise empty matrices to store all objects\n        X = np.empty((0, 2))\n        y = np.empty((0, 1))\n\n        # iterate over all items in the environment\n        for inner_key in environment[environment_key]:\n\n            # convert all objects under this key to a matrix\n            inner_X = np.array(\n                [\n                    [obj[x_variable_field]]\n                    for environment_list in environment[environment_key][inner_key]\n                    for obj in environment_list[\"objects\"]\n                ]\n            )\n            inner_X = np.hstack([np.ones((inner_X.shape[0], 1)), inner_X])\n            X = np.vstack([X, inner_X])\n\n            # convert all objects under this key to a matrix\n            inner_y = np.array(\n                [\n                    [obj[y_variable_field]]\n                    for environment_list in environment[environment_key][inner_key]\n                    for obj in environment_list[\"objects\"]\n                ]\n            )\n            y = np.vstack([y, inner_y])\n</code></pre> This may look a bit complicated, but its quite straightforward. Since the <code>environment</code> is keyed via two variables: the tool used which added items to the environment and then a sub-key of names within that tool, we require looping over the <code>inner_key</code> within the outer <code>environment_key</code> which was decided by the LLM.</p> <p>We use NumPy to create the matrices, which are initialised to the correct dimension (the independent variable should have matrix \\(X\\) which is \\(n \\times 2\\), since there is only one independent variable and a intercept column, and the dependent variable, \\(y\\), should be \\(n \\times 1\\)). </p> <p>Now we need to perform least squares regression on these matrices to obtain the regression coefficients.</p> <pre><code>        # calculate the beta hat values via least squares\n        beta_hat = np.linalg.inv(X.T @ X + 1e-10 * np.eye(X.shape[1])) @ X.T @ y\n        beta_hat_dict = {\n            \"intercept\": beta_hat[0],\n            \"slope\": beta_hat[1],\n        }\n</code></pre> <p>We follow the least squares solution using linear algebra to get these coefficients: $$     \\hat{\\beta} = (X^\\top X + \\lambda I_n)^{-1} X^\\top y $$ where \\(\\lambda\\) is small (e.g. <code>1e-10</code>). The first value in \\(\\hat{\\beta}\\) will correspond to the intercept and the second value will correspond to the independent variable and its correlation with the dependent variable (the slope). We can plot these values with <code>matplotlib</code>, just for explanatory purposes:</p> <pre><code>        # plot the data\n        pred_y = X @ beta_hat   \n        fig, ax = plt.subplots()\n        ax.scatter(X[:, 1], y)\n        ax.plot(X[:, 1], pred_y, color=\"red\")\n        ax.set_title(\n            f\"Linear regression between {x_variable_field} and {y_variable_field}\"\n        )\n        ax.set_xlabel(x_variable_field)\n        ax.set_ylabel(y_variable_field)\n        fig.show()\n</code></pre> <p>This will plot the independent variable against the dependent one, include the line of best fit, and then show the plot whenever the tool is successfully run.</p> <p>Now we need to give information back to the decision tree about the results of this analysis, so it can be used to continue the conversation and make further decisions. All tools should <code>yield</code> objects, and we use the <code>Result</code> class which automatically will assign objects to the tree's environment.</p> <pre><code>        yield Result(\n            objects=[\n                beta_hat_dict,\n            ],\n            metadata={\n                \"x_variable_field\": x_variable_field,\n                \"y_variable_field\": y_variable_field,\n            },\n            llm_message=(\n                \"Completed linear regression analysis where: \"\n                \"    x variable: {x_variable_field} \"\n                \"    y variable: {y_variable_field} \"\n            ),\n        )\n</code></pre> <p>There is only one object needed to be returned, the coefficient values. The decision agent will use this information to continue the decision tree by reading the objects. We also include what variable corresponds to where by including the <code>x_variable_field</code> and <code>y_variable_field</code> in the metadata, and include an <code>llm_message</code> to custom display that the regression was completed. </p> <pre><code>    async def __call__(\n        self,\n        tree_data,\n        inputs,\n        base_lm,\n        complex_lm,\n        client_manager,\n        **kwargs,\n    ):\n        environment = tree_data.environment.environment\n        environment_key = inputs[\"environment_key\"]\n        x_variable_field = inputs[\"x_variable_field\"]\n        y_variable_field = inputs[\"y_variable_field\"]\n\n        # initialise empty matrices to store all objects\n        X = np.empty((0, 2))\n        y = np.empty((0, 1))\n\n        # iterate over all items in the environment\n        for inner_key in environment[environment_key]:\n\n            # convert all objects under this key to a matrix\n            inner_X = np.array(\n                [\n                    [obj[x_variable_field]]\n                    for environment_list in environment[environment_key][inner_key]\n                    for obj in environment_list[\"objects\"]\n                ]\n            )\n            inner_X = np.hstack([np.ones((inner_X.shape[0], 1)), inner_X])\n            X = np.vstack([X, inner_X])\n\n            # convert all objects under this key to a matrix\n            inner_y = np.array(\n                [\n                    [obj[y_variable_field]]\n                    for environment_list in environment[environment_key][inner_key]\n                    for obj in environment_list[\"objects\"]\n                ]\n            )\n            y = np.vstack([y, inner_y])\n\n        # calculate the beta hat values via least squares\n        beta_hat = np.linalg.inv(X.T @ X + 1e-10 * np.eye(X.shape[1])) @ X.T @ y\n        beta_hat_dict = {\n            \"intercept\": beta_hat[0],\n            \"slope\": beta_hat[1],\n        }\n        pred_y = X @ beta_hat\n\n        # plot the data\n        fig, ax = plt.subplots()\n        ax.scatter(X[:, 1], y)\n        ax.plot(X[:, 1], pred_y, color=\"red\")\n        ax.set_title(\n            f\"Linear regression between {x_variable_field} and {y_variable_field}\"\n        )\n        ax.set_xlabel(x_variable_field)\n        ax.set_ylabel(y_variable_field)\n        fig.show()\n\n        # yield the result to the decision tree\n        yield Result(\n            objects=[\n                beta_hat_dict,\n            ],\n            metadata={\n                \"x_variable_field\": x_variable_field,\n                \"y_variable_field\": y_variable_field,\n            },\n            llm_message=(\n                \"Completed linear regression analysis where: \"\n                \"    x variable: {x_variable_field} \"\n                \"    y variable: {y_variable_field} \"\n            ),\n        )\n</code></pre>"},{"location":"Examples/old_data_analysis/#controlling-when-the-tool-is-available","title":"Controlling when the tool is available","text":"<p>We can also set the <code>is_tool_available</code> method to only return <code>True</code> when something relevant is in the environment. In this case, when information has been returned from the <code>query</code> tool.</p> <pre><code>    async def is_tool_available(self, tree_data, base_lm, complex_lm, client_manager):\n        \"\"\"\n        Available when the 'query' task has been completed and it has added data to the environment.\n        \"\"\"\n        return (\n            \"query\" in tree_data.environment.environment\n            and len(tree_data.environment.environment[\"query\"]) &gt; 0\n        )\n</code></pre> <p>We have also provided a brief description of when the tool will become available, which will be passed to the decision agent any time the tool is unavailable, so that it knows what tasks it needs to accomplish before being able to run this tool. In this case, this should inform the LM that to perform linear regression, it needs to get the data first.</p>"},{"location":"Examples/old_data_analysis/#putting-it-all-together","title":"Putting it all together","text":"<p>Here is the full tool construction, and let's also wrap the call in a <code>try</code>/<code>except</code> block to catch any errors (using the <code>Error</code> class for self-healing errors).</p> <pre><code>class BasicLinearRegression(Tool):\n\n    def __init__(self, logger, **kwargs):\n        super().__init__(\n            name=\"basic_linear_regression_tool\",\n            description=\"\"\"\n            Use this tool to perform linear regression between two numeric variables in the environment.\n            \"\"\",\n            status=\"Running linear regression...\",\n            inputs={\n                \"environment_key\": {\n                    \"description\": (\n                        \"A single key (string) of the `environment` dictionary that will be used in the analysis. \"\n                        \"Choose the most relevant key for the analysis according to the user prompt. \"\n                        \"All objects under that key will be used to create the dataframe. \"\n                    ),\n                    \"required\": True,\n                    \"type\": str,\n                    \"default\": None,\n                },\n                \"x_variable_field\": {\n                    \"description\": (\n                        \"The independent variable for the regression. \"\n                        \"Choose one field title within the `objects` underneath the specific `environment_key`. \"\n                    ),\n                    \"required\": True,\n                    \"type\": str,\n                    \"default\": None,\n                },\n                \"y_variable_field\": {\n                    \"description\": (\n                        \"The dependent variable for the regression. \"\n                        \"Choose one single field within the `objects` underneath the specific `environment_key`. \"\n                    ),\n                    \"required\": True,\n                    \"type\": str,\n                    \"default\": None,\n                },\n            },\n            end=False,\n        )\n\n    async def __call__(\n        self,\n        tree_data,\n        inputs,\n        base_lm,\n        complex_lm,\n        client_manager,\n        **kwargs,\n    ):\n        environment = tree_data.environment.environment\n        environment_key = inputs[\"environment_key\"]\n        x_variable_field = inputs[\"x_variable_field\"]\n        y_variable_field = inputs[\"y_variable_field\"]\n\n        try:\n\n            # initialise empty matrices to store all objects\n            X = np.empty((0, 2))\n            y = np.empty((0, 1))\n\n            # iterate over all items in the environment\n            for inner_key in environment[environment_key]:\n\n                # convert all objects under this key to a matrix\n                inner_X = np.array(\n                    [\n                        [obj[x_variable_field]]\n                        for environment_list in environment[environment_key][inner_key]\n                        for obj in environment_list[\"objects\"]\n                    ]\n                )\n                inner_X = np.hstack([np.ones((inner_X.shape[0], 1)), inner_X])\n                X = np.vstack([X, inner_X])\n\n                # convert all objects under this key to a matrix\n                inner_y = np.array(\n                    [\n                        [obj[y_variable_field]]\n                        for environment_list in environment[environment_key][inner_key]\n                        for obj in environment_list[\"objects\"]\n                    ]\n                )\n                y = np.vstack([y, inner_y])\n\n            # calculate the beta hat values via least squares\n            beta_hat = np.linalg.inv(X.T @ X + 1e-10 * np.eye(X.shape[1])) @ X.T @ y\n            beta_hat_dict = {\n                \"intercept\": beta_hat[0],\n                \"slope\": beta_hat[1],\n            }\n            pred_y = X @ beta_hat\n\n            # plot the data\n            fig, ax = plt.subplots()\n            ax.scatter(X[:, 1], y)\n            ax.plot(X[:, 1], pred_y, color=\"red\")\n            ax.set_title(\n                f\"Linear regression between {x_variable_field} and {y_variable_field}\"\n            )\n            ax.set_xlabel(x_variable_field)\n            ax.set_ylabel(y_variable_field)\n            fig.show()\n\n            # yield the result to the decision tree\n            yield Result(\n                objects=[\n                    beta_hat_dict,\n                ],\n                metadata={\n                    \"x_variable_field\": x_variable_field,\n                    \"y_variable_field\": y_variable_field,\n                },\n                llm_message=(\n                    \"Completed linear regression analysis where: \"\n                    \"    x variable: {x_variable_field} \"\n                    \"    y variable: {y_variable_field} \"\n                ),\n            )\n        except Exception as e:\n            yield Error(str(e))\n\n    async def is_tool_available(self, tree_data, base_lm, complex_lm, client_manager):\n        \"\"\"\n        Available when the 'query' task has been completed and it has added data to the environment.\n        \"\"\"\n        return (\n            \"query\" in tree_data.environment.environment\n            and len(tree_data.environment.environment[\"query\"]) &gt; 0\n        )\n</code></pre>"},{"location":"Examples/old_data_analysis/#experimenting","title":"Experimenting","text":"<p>Let's initialise a tree, add the tool and test it runs correctly. In this example, a Weaviate collection called <code>Ecommerce</code> will be used. The <code>Ecommerce</code> collection has many redundant fields (for this example), but we want to evaluate whether the <code>price</code> variable correlates with the review <code>rating</code>.</p> <p><pre><code>from elysia import Tree\ntree = Tree()\ntree.add_tool(BasicLinearRegression)\nresponse, objects = tree(\n    \"Perform linear regression on the relationship between the price of a product and the review rating it has\",\n    collection_names = [\"Ecommerce\"]\n)\n</code></pre> And some relevant output from the run: <pre><code>INFO     Tasks completed (iteration 1):\n            - query (Avg. 3.90 seconds)\n            - summarise_items (Avg. 0.00 seconds)\nINFO     Tasks completed (iteration 2):\n            - basic_linear_regression_tool (Avg. 0.01 seconds)\nINFO     Tasks completed (iteration 3):\n            - cited_summarize (Avg. 4.59 seconds)\n</code></pre></p> <p><pre><code>print(response)\n</code></pre> <code>To begin, I'm querying the \"Ecommerce\" collection to retrieve the product price and review rating data needed for the linear regression. I am now retrieving all product prices and review ratings from the \"Ecommerce\" collection to prepare for the linear regression analysis. I am now performing a linear regression analysis on the retrieved data, using product price as the independent variable and review rating as the dependent variable. I have completed the linear regression analysis, revealing an intercept of approximately 4.61 and a slope of approximately 0.00044, indicating a very slight positive relationship between product price and review rating. The linear regression analysis performed on product price and review rating reveals a very slight positive relationship between the two variables. The intercept of the regression model is approximately 4.61, and the slope is approximately 0.00044. This indicates that as the product price increases, the review rating tends to increase, but only by a very small margin.</code></p> <p></p>"},{"location":"Examples/query_weaviate/","title":"Querying a Weaviate Database","text":"<p>This example will walk through using Elysia to:</p> <ul> <li>Setting up your API keys with Elysia for using models</li> <li>Setting up your Weaviate collections for usage with Elysia</li> <li>Query or aggregate your Weaviate collections</li> <li>How the decision tree works</li> </ul>"},{"location":"Examples/query_weaviate/#before-you-begin","title":"Before You Begin","text":"<p>Before setting up your environment and connecting to Weaviate, make sure you have the necessary API keys and access credentials for both your language models and your Weaviate instance. This will ensure a smooth setup process in the following steps.</p> <ol> <li>You should have a Weaviate cloud cluster or locally running Weaviate instance - see Step 1.1 in the Weaviate quickstart guide for cloud and see here for running Weaviate locally with Docker.</li> <li>You need to find your REST endpoint URL and Admin API key for your cloud cluster - see Step 1.3 in the Weaviate quickstart guide, or </li> <li>You additionally need API keys for any LLMs you want to use. We recommend OpenRouter to gain access to a range of models, or Ollama for locally running models.</li> </ol>"},{"location":"Examples/query_weaviate/#setting-up","title":"Setting up","text":"<p>Let's use the basic elysia <code>configure</code> to both set up your models and connect to your Weaviate instance.</p> <p><pre><code>from elysia import configure\nconfigure(\n    weaviate_is_local = False, # replace with True if locally running Weaviate\n    wcd_url = \"...\", # replace with your Weaviate REST endpoint URL\n    wcd_api_key = \"...\" # replace with your Weaviate cluster API key,\n    base_model = \"gemini-2.0.flash-001\", # replace with whichever model you are using\n    base_provider = \"gemini\", # replace with your model provider or 'ollama' for locally running ollama models\n    complex_model = \"gemini-2.5.flash-001\",\n    complex_provider = \"gemini\",\n    gemini_api_key = \"...\" # replace with your GEMINI_API_KEY from Google AI studio, or whichever API key you need for Weaviate/your LLMs\n)\n</code></pre> Alternatively, you can use different models, such as <code>gpt-4.1-mini</code>, <code>gpt-4.1</code>, with <code>base_provider=\"openai\"</code> and <code>complex_provider=\"openai\"</code>, as well as an <code>openai_api_key</code>. Or any model/provider combination that you wish, see the full LiteLLM docs for all API keys and models/providers.</p>"},{"location":"Examples/query_weaviate/#optional-add-some-data-to-your-weaviate-cluster","title":"Optional: Add some data to your Weaviate cluster","text":"<p>We're going to create some basic data and an example collection for this demo. This is based on this example in the Weaviate docs.</p> <p>If you want to skip this step and use data from your own collection, simply replace all instances of the collection name \"JeopardyQuestion\" with your true collection name in later steps.</p> <ol> <li> <p>Download the example dataset.     <pre><code>import requests, json\nurl = \"https://raw.githubusercontent.com/weaviate/weaviate-examples/main/jeopardy_small_dataset/jeopardy_tiny.json\"\nresp = requests.get(url)\ndata = json.loads(resp.text)\n</code></pre>     This dataset contains questions, answers and categories from Jeopardy questions. E.g.     <pre><code>{\n    \"Category\": \"SCIENCE\",\n    \"Question\": \"This organ removes excess glucose from the blood &amp; stores it as glycogen\",\n    \"Answer\": \"Liver\"\n}\n</code></pre></p> </li> <li> <p>Import the data into Weaviate     <pre><code>from elysia.util.client import ClientManager\n\nclient_manager = ClientManager()\n\nwith client_manager.connect_to_client() as client:\n\n    if client.collections.exists(\"JeopardyQuestion\"):\n        client.collections.delete(\"JeopardyQuestion\")\n\n    client.collections.create(\n        \"JeopardyQuestion\"\n    )\n\n    jeopardy = client.collections.get(\"JeopardyQuestion\")\n    response = jeopardy.data.insert_many(data)\n\n    if response.has_errors:\n        print(response.errors)\n    else:\n        print(\"Insert complete.\")\n</code></pre>     This will by default use the settings inherited from the earlier <code>configure</code> function, so the Weaviate REST endpoint URL and API key set up previously.</p> </li> </ol>"},{"location":"Examples/query_weaviate/#preprocessing-with-elysia","title":"Preprocessing with Elysia","text":"<p>Now that you are fully set up with models and Weaviate integrations, you can move onto preprocessing your collection for use with Elysia. This is as simple as: <pre><code>from elysia import preprocess\npreprocess(\"JeopardyQuestion\")\n</code></pre></p> View the Preprocessed Data  To view the preprocessing that has been completed, you can run the `view_preprocessed_collection` function on your collection:  <pre><code>from elysia import view_preprocessed_collection\nview_preprocessed_collection(\"JeopardyQuestion\")\n</code></pre> <pre><code>{\n    \"mappings\": {\n        \"document\": {\"content\": \"question\", \"category\": \"category\", \"title\": \"\", \"author\": \"\", \"date\": \"\"},\n        \"table\": {\"category\": \"category\", \"question\": \"question\", \"answer\": \"answer\"}\n    },\n    \"prompts\": [\n        \"What are some questions about DNA?\",\n        \"What questions are in the SCIENCE category?\",\n        \"What questions are in the ANIMALS category?\",\n        \"What are some questions about mammals?\",\n        \"What are some questions about snakes?\",\n        \"What are the answers related to science?\",\n        \"What are the answers related to animals?\",\n        \"What questions involve the atmosphere?\",\n        \"What questions involve metals?\",\n        \"What questions involve organs?\"\n    ],\n    \"fields\": [\n        {\n            \"range\": [1.0, 4.0],\n            \"type\": \"text\",\n            \"groups\": [\n                {\"count\": 1, \"value\": \"DNA\"},\n                {\"count\": 1, \"value\": \"the atmosphere\"},\n                {\"count\": 1, \"value\": \"wire\"},\n                {\"count\": 1, \"value\": \"Elephant\"},\n                {\"count\": 1, \"value\": \"Antelope\"},\n                {\"count\": 1, \"value\": \"species\"},\n                {\"count\": 1, \"value\": \"Liver\"},\n                {\"count\": 1, \"value\": \"Sound barrier\"},\n                {\"count\": 1, \"value\": \"the diamondback rattler\"},\n                {\"count\": 1, \"value\": \"the nose or snout\"}\n            ],\n            \"mean\": 1.7,\n            \"date_range\": None,\n            \"name\": \"answer\",\n            \"date_median\": None,\n            \"description\": \"The correct response to the question posed in the 'question' field. This is a string \ncontaining the answer.\"\n        },\n        {\n            \"range\": [1.0, 1.0],\n            \"type\": \"text\",\n            \"groups\": [{\"count\": 6, \"value\": \"SCIENCE\"}, {\"count\": 4, \"value\": \"ANIMALS\"}],\n            \"mean\": 1.0,\n            \"date_range\": None,\n            \"name\": \"category\",\n            \"date_median\": None,\n            \"description\": \"The subject area or topic to which the question and answer belong. Examples include \n'SCIENCE' and 'ANIMALS'.\"\n        },\n        {\n            \"range\": [10.0, 22.0],\n            \"type\": \"text\",\n            \"groups\": [\n                {\n                    \"count\": 1,\n                    \"value\": \"A metal that is 'ductile' can be pulled into this while cold &amp; under pressure\"\n                },\n                {\n                    \"count\": 1,\n                    \"value\": \"The gavial looks very much like a crocodile except for this bodily feature\"\n                },\n                {\n                    \"count\": 1,\n                    \"value\": \"In 1953 Watson &amp; Crick built a model of the molecular structure of this, the \ngene-carrying substance\"\n                },\n                {\n                    \"count\": 1,\n                    \"value\": \"Weighing around a ton, the eland is the largest species of this animal in Africa\"\n                },\n                {\n                    \"count\": 1,\n                    \"value\": \"2000 news: the Gunnison sage grouse isn't just another northern sage grouse, but a \nnew one of this classification\"\n                },\n                {\"count\": 1, \"value\": \"It's the only living mammal in the order Proboseidea\"},\n                {\"count\": 1, \"value\": \"This organ removes excess glucose from the blood &amp; stores it as glycogen\"},\n                {\n                    \"count\": 1,\n                    \"value\": \"In 70-degree air, a plane traveling at about 1,130 feet per second breaks it\"\n                },\n                {\"count\": 1, \"value\": \"Heaviest of all poisonous snakes is this North American rattlesnake\"},\n                {\"count\": 1, \"value\": \"Changes in the tropospheric layer of this are what gives us weather\"}\n            ],\n            \"mean\": 15.0,\n            \"date_range\": None,\n            \"name\": \"question\",\n            \"date_median\": None,\n            \"description\": \"The question or prompt for which the 'answer' field provides the correct response. This\nis a string containing the question.\"\n        }\n    ],\n    \"summary\": \"This dataset contains questions and answers across various categories, primarily focusing on \nscience and animals. Each entry includes a question, its corresponding answer, and the category to which the \nquestion belongs. The dataset provides a diverse set of trivia-like information suitable for quizzes or educational\npurposes. The sample represents the entire dataset. The 'question' field is related to the 'answer' field, as the \n'answer' provides the correct response to the 'question'. The 'category' field classifies the 'question' and \n'answer' pair into a specific subject area. The category helps to group questions of similar topics together. The \ndata is structured as a list of JSON objects. Each object contains three fields: 'answer', 'category', and \n'question'. No irregularities found. \",\n    \"vectorizer\": None,\n    \"name\": \"JeopardyQuestion\",\n    \"named_vectors\": [\n        {\n            \"source_properties\": None,\n            \"enabled\": True,\n            \"name\": \"default\",\n            \"model\": \"Snowflake/snowflake-arctic-embed-l-v2.0\",\n            \"description\": \"\",\n            \"vectorizer\": \"TEXT2VEC_WEAVIATE\"\n        }\n    ],\n    \"index_properties\": {\"isTimestampIndexed\": False, \"isNullIndexed\": False, \"isLengthIndexed\": False},\n    \"length\": 10.0\n}\n</code></pre>"},{"location":"Examples/query_weaviate/#creating-the-decision-tree","title":"Creating the Decision Tree","text":"<p>Now that the models and Weaviate integrations are set up (with <code>configure</code>), the default parameters to run the decision tree will also work automatically. </p> <p>Let's first create the class and inspect some of the properties. <pre><code>from elysia import Tree\ntree = Tree()\n</code></pre></p> Inspecting the Decision Tree Structure To look at what tools are currently on the tree, we can inspect use the `tree.view()` method:  <pre><code>print(tree.view())\n</code></pre> <pre><code>\ud83d\udcc1 Base (base)\n  \u251c\u2500\u2500 \ud83d\udd27 Cited summarize (cited_summarize)\n      \ud83d\udcac Summarize retrieved information for the user when all relevant data has\n         been gathered. Provides a text response, and may end the conversation, but\n         unlike text_response tool, can be used mid-conversation. Avoid for general\n         questions where text_response is available. Summarisation text is directly\n         displayed to the user. Most of the time, you can choose end_actions to be\n         True to end the conversation with a summary. This is a good way to end the\n         conversation.\n\n\n  \u251c\u2500\u2500 \ud83d\udd27 Text response (text_response)\n      \ud83d\udcac End the conversation. This should be used when the user has finished their\n         query, or you have nothing more to do except reply. You should use this to\n         answer conversational questions not related to other tools. But do not use\n         this as a source of information. All information should be from the\n         environment if answering a complex question or an explanation. If there is\n         an error and you could not complete a task, use this tool to suggest a\n         brief reason why. If, for example, there is a missing API key, then the\n         user needs to add it to the settings (which you should inform them of). Or\n         you cannot connect to weaviate, then the user needs to input their API\n         keys in the settings. If there are no collections available, the user\n         needs to analyze this in the 'data' tab. If there are other problems, and\n         it looks like the user can fix it, then provide a suggestion.\n\n\n  \u251c\u2500\u2500 \ud83d\udd27 Aggregate (aggregate)\n      \ud83d\udcac Query the knowledge base specifically for aggregation queries. Performs\n         calculations (counting, averaging, summing, etc.) and provides summary\n         statistics on data. It can group data by properties and apply filters\n         directly, without needing a prior query. Aggregation queries can be\n         filtered. This can be applied directly on any collections in the schema.\n         Use this tool when you need counts, sums, averages, or other summary\n         statistics on properties in the collections. 'aggregate' should be\n         considered the first choice for tasks involving counting, summing,\n         averaging, or other statistical operations, even when filtering is\n         required.\n\n\n  \u251c\u2500\u2500 \ud83d\udd27 Base.query (base.query)\n      \ud83d\udcac Retrieves and displays specific data entries from the collections. Then,\n         query with semantic search, keyword search, or a combination of both.\n         Queries can be filtered, sorted, and more. Retrieving and displaying\n         specific data entries rather than performing calculations or summaries. Do\n         not use 'query' as a preliminary filtering step when 'aggregate' can\n         achieve the same result more efficiently (if 'aggregate' is available).\n\n    \u2514\u2500\u2500 \ud83d\udd27 Query postprocessing (query_postprocessing)\n        \ud83d\udcac If the user has requested itemised summaries for retrieved objects, this\n           tool summarises each object on an individual basis.\n\n\n  \u2514\u2500\u2500 \ud83d\udd27 Visualise (visualise)\n      \ud83d\udcac Visualise data in a chart from the environment. You can only visualise\n         data that is in the environment. If there is nothing relevant in the\n         environment, do not choose this tool.\n</code></pre>  These are the default tools available in a regular initialisation of the Elysia Tree, as well as their tool descriptions. To change the default tools available on a tree, you can initialise the tree with a different `branch_initialisation`, e.g.  <pre><code>tree = Tree(branch_initialisation=\"empty\")\n</code></pre> will create a tree with no tools, and you can add custom tools via `tree.add_tool()`."},{"location":"Examples/query_weaviate/#running-the-decision-tree","title":"Running the Decision Tree","text":"<p>To run the tool-running pipeline of the Elysia decision tree, you can simply call the class, i.e.</p> <pre><code>response, objects = tree(\n    \"Find a single question about Science\",\n    collection_names = [\"JeopardyQuestion\"]\n)\n</code></pre> Real time updates The default behaviour is that Elysia will print updates on what it is doing. In this example, this is <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 User prompt \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                      \u2502\n\u2502 Find a single question about Science \u2502\n\u2502                                      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Assistant response \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                              \u2502\n\u2502 I will now search for a science question in the JeopardyQuestion collection. \u2502\n\u2502                                                                              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Current Decision \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502 Node: base                                                                                                      \u2502\n\u2502 Decision: query                                                                                                 \u2502\n\u2502 Reasoning: The user is asking for a question about science.                                                     \u2502\n\u2502 The `JeopardyQuestion` collection contains questions and answers, and the category field indicates whether the  \u2502\n\u2502 question is about science.                                                                                      \u2502\n\u2502 Therefore, I should query the `JeopardyQuestion` collection and filter for questions where the category is      \u2502\n\u2502 science.                                                                                                        \u2502\n\u2502 I should use the `query` tool to retrieve the questions.                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Assistant response \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502 I am now retrieving a single science question from the JeopardyQuestion collection by filtering for the         \u2502\n\u2502 'SCIENCE' category.                                                                                             \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 JeopardyQuestion (Weaviate Query) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                         \u2502\n\u2502 collection.query.fetch_objects(                         \u2502\n\u2502     filters=Filter.all_of([                             \u2502\n\u2502         Filter.by_property('category').equal('SCIENCE') \u2502\n\u2502     ]),                                                 \u2502\n\u2502     limit=1                                             \u2502\n\u2502 )                                                       \u2502\n\u2502                                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Current Decision \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                 \u2502\n\u2502 Node: base.query                                                                                \u2502\n\u2502 Decision: query_postprocessing                                                                  \u2502\n\u2502 Reasoning: Only one option available: query_postprocessing (and no function inputs are needed). \u2502\n\u2502                                                                                                 \u2502\n\u2502                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Current Decision \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502 Node: base                                                                                                      \u2502\n\u2502 Decision: text_response                                                                                         \u2502\n\u2502 Reasoning: I have already retrieved a science question from the JeopardyQuestion collection in the previous     \u2502\n\u2502 turn. The question is: \"This organ removes excess glucose from the blood &amp; stores it as glycogen\". The answer   \u2502\n\u2502 is \"Liver\". I should now respond to the user with this question.                                                \u2502\n\u2502                                                                                                                 \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Assistant response \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                \u2502\n\u2502 Here's a science question for you: \"This organ removes excess glucose from the blood &amp; stores it as glycogen?\" \u2502\n\u2502                                                                                                                \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>This will run through the decision tree, where the decision agent will choose tools based on what tools are available, what branch the decision agent is currently on, and context.</p> <p>The <code>response</code> will be a concatenation of all responses that the model output (at each decision step and during any other response outputs): <pre><code>print(response)\n</code></pre> <pre><code>'I will now search for a science question in the JeopardyQuestion collection. I am now retrieving a single science question from the JeopardyQuestion collection by filtering for the \\'SCIENCE\\' category. Here\\'s a science question for you: \"This organ removes excess glucose from the blood &amp; stores it as glycogen?\"'\n</code></pre></p> <p>The <code>objects</code> will be a list of objects that were added to the environment during that tree run: <pre><code>print(objects)\n</code></pre> <pre><code>[\n    [\n        {\n            'category': 'SCIENCE',\n            'question': 'This organ removes excess glucose from the blood &amp; stores it as glycogen',\n            'answer': 'Liver',\n            'uuid': 'b28ca48a-9a8d-417c-9ed1-e487132740ed',\n            'collection_name': 'JeopardyQuestion',\n            'chunk_spans': [],\n            '_REF_ID': 'query_JeopardyQuestion_0_0'\n        }\n    ]\n]\n</code></pre></p> <p>In this case, the model searched Weaviate using a <code>fetch_objects</code> search (no hybrid or semantic search), and used a <code>limit=1</code>, so only returned one object. The decision agent recognised that the query tool was called successfully, added a single object to the environment, and then finally recognised that the task was completed and therefore finalised the process with a <code>text_response</code>, informing the user of what the question was.</p> <p>Note that the query tool was able to correctly use the filter <code>Filter.by_property('category').equal('SCIENCE')</code>, as the preprocessing step identified the unique groups for the property <code>'category'</code>, which was handed down to the query tool.</p>"},{"location":"Examples/query_weaviate/#continuing-the-conversation","title":"Continuing the Conversation","text":"<p>By calling the same <code>Tree</code> class a second time, the conversation history is automatically included in the context of the decision tree. So if you were to ask: <pre><code>tree(\"What about animals?\")\n</code></pre></p> Real time updates <pre><code>\u256d\u2500\u2500\u2500\u2500 User prompt \u2500\u2500\u2500\u2500\u256e\n\u2502                     \u2502\n\u2502 What about animals? \u2502\n\u2502                     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Assistant response \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                    \u2502\n\u2502 I will now search for a question about animals in the JeopardyQuestion collection. \u2502\n\u2502                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Current Decision \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502 Node: base                                                                                                      \u2502\n\u2502 Decision: query                                                                                                 \u2502\n\u2502 Reasoning: The user is now asking about animals, following a previous question about science.                   \u2502\n\u2502 The `JeopardyQuestion` collection contains questions and answers, and the category field indicates whether the  \u2502\n\u2502 question is about animals.                                                                                      \u2502\n\u2502 Therefore, I should query the `JeopardyQuestion` collection and filter for questions where the category is      \u2502\n\u2502 animals.                                                                                                        \u2502\n\u2502 I should use the `query` tool to retrieve the questions.                                                        \u2502\n\u2502                                                                                                                 \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Assistant response \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502 I am now retrieving a single question about animals from the JeopardyQuestion collection by filtering for the   \u2502\n\u2502 'ANIMALS' category.                                                                                             \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 JeopardyQuestion (Weaviate Query) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                         \u2502\n\u2502 collection.query.fetch_objects(                         \u2502\n\u2502     filters=Filter.all_of([                             \u2502\n\u2502         Filter.by_property('category').equal('ANIMALS') \u2502\n\u2502     ]),                                                 \u2502\n\u2502     limit=1                                             \u2502\n\u2502 )                                                       \u2502\n\u2502                                                         \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Current Decision \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                 \u2502\n\u2502 Node: base.query                                                                                \u2502\n\u2502 Decision: query_postprocessing                                                                  \u2502\n\u2502 Reasoning: Only one option available: query_postprocessing (and no function inputs are needed). \u2502\n\u2502                                                                                                 \u2502\n\u2502                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Current Decision \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                                                 \u2502\n\u2502 Node: base                                                                                                      \u2502\n\u2502 Decision: text_response                                                                                         \u2502\n\u2502 Reasoning: I have already retrieved a question about animals from the JeopardyQuestion collection in the        \u2502\n\u2502 previous turn. The question is: \"It's the only living mammal in the order Proboseidea\". The answer is           \u2502\n\u2502 \"Elephant\". I should now respond to the user with this question.                                                \u2502\n\u2502                                                                                                                 \u2502\n\u2502                                                                                                                 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Assistant response \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502                                                                                             \u2502\n\u2502 Here is an animal question for you: \"It's the only living mammal in the order Proboseidea?\" \u2502\n\u2502                                                                                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Then the decision agent is aware that the context of the first question, i.e. \"retrieve a single question\", and uses that to frame its responses and queries for the second question, changing the filter from <code>\"SCIENCE\"</code> to <code>\"ANIMALS\"</code>. The conversation history can be viewed by running:</p> <p><pre><code>print(tree.conversation_history)\n</code></pre> <pre><code>[\n    {'role': 'user', 'content': 'Find a single question about Science'},\n    {\n        'role': 'assistant',\n        'content': 'I will now search for a science question in the JeopardyQuestion collection. I am now \nretrieving a single science question from the JeopardyQuestion collection by filtering for the \\'SCIENCE\\' \ncategory. Here\\'s a science question for you: \"This organ removes excess glucose from the blood &amp; stores it as \nglycogen?\"'\n    },\n    {'role': 'user', 'content': 'What about animals?'},\n    {\n        'role': 'assistant',\n        'content': 'I will now search for a question about animals in the JeopardyQuestion collection. I am now \nretrieving a single question about animals from the JeopardyQuestion collection by filtering for the \\'ANIMALS\\' \ncategory. Here\\'s a question about animals for you: \"It\\'s the only living mammal in the order Proboseidea?\"'\n    }\n]\n</code></pre></p>"},{"location":"Examples/sentiment_analysis/","title":"Example: Sentiment Analysis on Retrieved Data","text":"<p>Coming soon.</p>"},{"location":"MCP/","title":"MCP (Model Context Protocol) Integration","text":"<p>Complete documentation for integrating MCP servers with Elysia.</p>"},{"location":"MCP/#quick-links","title":"Quick Links","text":"<ul> <li>MCP_AS_AGENT Configuration - \u2b50 NEW: Control MCP behavior (Agent vs Individual mode)</li> <li>Overview - High-level summary of MCP integration</li> <li>Quickstart - 30-second setup guide</li> <li>Interaction Model - How MCPTool works and parameter surfacing</li> <li>Architecture - Technical architecture details</li> <li>API Integration - Complete API flow documentation</li> <li>Usage Guide - Configuration and usage instructions</li> <li>Implementation Details - Detailed changes, diagrams, and testing</li> </ul>"},{"location":"MCP/#documentation-structure","title":"Documentation Structure","text":""},{"location":"MCP/#for-users","title":"For Users","text":"<ol> <li>Start with Overview - Understand what was built</li> <li>Read Quickstart - Get up and running quickly</li> <li>Learn Interaction Model - Understand how MCPTool works</li> <li>Review Usage Guide - Learn configuration options</li> </ol>"},{"location":"MCP/#for-developers","title":"For Developers","text":"<ol> <li>Study Interaction Model - Understand the gateway pattern</li> <li>Study Architecture - Understand the implementation</li> <li>Review API Integration - See the complete flow</li> <li>Read Implementation Details - See code changes and testing</li> <li>Refer to source code in <code>elysia/tools/mcp/</code></li> </ol>"},{"location":"MCP/#key-concepts","title":"Key Concepts","text":""},{"location":"MCP/#operating-modes-new","title":"Operating Modes (NEW)","text":"<p>Elysia supports two modes for MCP integration controlled by <code>MCP_AS_AGENT</code> environment variable:</p> <ol> <li>Agent Mode (<code>MCP_AS_AGENT=True</code>) - Default</li> <li>Natural language interface  </li> <li>One AI agent tool per MCP server</li> <li>Autonomous task execution via ReAct agent</li> <li> <p>Example: <code>tree(\"Search for ML papers and analyze them\")</code></p> </li> <li> <p>Individual Tool Mode (<code>MCP_AS_AGENT=False</code>)</p> </li> <li>Structured parameter interface</li> <li>One Elysia tool per MCP server tool</li> <li>Direct tool invocation</li> <li>Example: <code>tree.tools[\"mcp_server_search\"](inputs={...})</code></li> </ol> <p>See MCP_AS_AGENT Configuration for complete details.</p>"},{"location":"MCP/#gateway-pattern","title":"Gateway Pattern","text":"<p>Each <code>MCPTool</code> represents one MCP server that provides access to multiple tools:</p> <ul> <li>Not: Each MCP tool as a separate Elysia tool \u274c</li> <li>Yes: One MCPTool per MCP server, exposing tools via actions \u2705</li> </ul> <p>See Interaction Model for detailed explanation.</p>"},{"location":"MCP/#two-phase-operation","title":"Two-Phase Operation","text":"<ol> <li>Discovery: <code>action='list'</code> - Discover available tools and their schemas</li> <li>Execution: <code>action='execute'</code> - Execute a specific tool by name</li> </ol>"},{"location":"MCP/#parameter-surfacing","title":"Parameter Surfacing","text":"<ul> <li>Level 1 (Elysia): <code>action</code>, <code>tool_name</code>, <code>tool_inputs</code> (always visible)</li> <li>Level 2 (MCP): Each tool's specific parameters (discovered dynamically)</li> </ul>"},{"location":"MCP/#key-features","title":"Key Features","text":"<p>\u2705 Automatic Discovery: MCP tools are automatically discovered and registered \u2705 API Integration: Full integration with Elysia's UserManager and TreeManager \u2705 Configuration-Based: Enable/disable servers via <code>mcp.json</code> \u2705 Production Ready: Error handling, logging, type safety \u2705 Minimal Code: ~200 lines per component, no overengineering  </p>"},{"location":"MCP/#installation","title":"Installation","text":"<p>The MCP integration uses <code>langchain-mcp-adapters</code>:</p> <pre><code>pip install langchain-mcp-adapters\n</code></pre> <p>This is already included in Elysia's dependencies.</p>"},{"location":"MCP/#current-status","title":"Current Status","text":"<ul> <li>\u2705 Integration complete and functional</li> <li>\u2705 API flow tested and verified</li> <li>\u2705 Documentation comprehensive</li> <li>\u23f3 No MCP servers currently configured</li> <li>\ud83c\udfaf Ready for production use</li> </ul>"},{"location":"MCP/#getting-help","title":"Getting Help","text":"<ul> <li>Configuration Issues: See Usage Guide</li> <li>Technical Details: See Architecture</li> <li>API Flow: See API Integration</li> </ul> <p>MCP integration is ready to use! Configure <code>mcp.json</code> to enable servers. \ud83d\ude80</p>"},{"location":"MCP/api_integration/","title":"MCP Integration Summary - API Flow","text":""},{"location":"MCP/api_integration/#overview","title":"Overview","text":"<p>MCP (Model Context Protocol) tools are now fully integrated into the Elysia API flow and can be automatically discovered and used by Trees.</p>"},{"location":"MCP/api_integration/#architecture-flow","title":"Architecture Flow","text":""},{"location":"MCP/api_integration/#1-application-startup-apppy","title":"1. Application Startup (<code>app.py</code>)","text":"<pre><code>app.py (FastAPI application)\n    \u2193\nincludes router: tools.router at /tools\n    \u2193\nimports: elysia.api.custom_tools\n</code></pre>"},{"location":"MCP/api_integration/#2-tool-discovery-custom_toolspy","title":"2. Tool Discovery (<code>custom_tools.py</code>)","text":"<pre><code># elysia/api/custom_tools.py\nfrom elysia.tools.mcp import mcp_loader  # Triggers MCP tool loading\n\n# Import dynamically created MCP tool classes\nfor _mcp_tool_name in mcp_loader.__all__:\n    globals()[_mcp_tool_name] = getattr(mcp_loader, _mcp_tool_name)\n</code></pre> <p>Key Points: - Importing <code>mcp_loader</code> triggers <code>load_mcp_servers_from_config()</code> at module import time - Dynamically created MCP tool classes are injected into <code>custom_tools</code> module namespace - <code>discover_tools_from_module()</code> in <code>util/tool_discovery.py</code> can discover them</p>"},{"location":"MCP/api_integration/#3-mcp-tool-loading-mcp_loaderpy","title":"3. MCP Tool Loading (<code>mcp_loader.py</code>)","text":"<pre><code># elysia/tools/mcp/mcp_loader.py\n\ndef load_mcp_servers_from_config() -&gt; list[type[MCPTool]]:\n    \"\"\"\n    1. Reads elysia/mcp.json\n    2. For each enabled server:\n       - Creates a dynamic MCPTool subclass\n       - Names it: MCP_{server_name}\n    3. Returns list of tool classes\n    \"\"\"\n\n# Executed at module import time\n_loaded_tool_classes = load_mcp_servers_from_config()\n\n# Export to module namespace\nfor tool_class in _loaded_tool_classes:\n    globals()[tool_class.__name__] = tool_class\n</code></pre>"},{"location":"MCP/api_integration/#4-user-initialization-flow","title":"4. User Initialization Flow","text":"<pre><code>POST /init/user/{user_id}\n    \u2193\nUserManager.add_user_local(user_id, config)\n    \u2193\nCreates TreeManager(user_id, config)\n    \u2193\nTreeManager stores Config with Settings\n</code></pre>"},{"location":"MCP/api_integration/#5-tree-initialization-flow","title":"5. Tree Initialization Flow","text":"<pre><code>POST /init/tree/{user_id}/{conversation_id}\n    \u2193\nUserManager.initialise_tree(user_id, conversation_id)\n    \u2193\nTreeManager.add_tree(conversation_id)\n    \u2193\nCreates Tree(settings=TreeManager.settings)\n    \u2193\nTree.__init__:\n    - Initializes with default branch initialization\n    - Loads default tools (Query, Aggregate, etc.)\n    - Ready to accept additional tools\n</code></pre>"},{"location":"MCP/api_integration/#6-tool-registration-with-tree","title":"6. Tool Registration with Tree","text":"<p>MCP tools can be added to a Tree in two ways:</p>"},{"location":"MCP/api_integration/#option-a-programmatic-for-custom-scripts","title":"Option A: Programmatic (for custom scripts)","text":"<pre><code>from elysia.tools.mcp import MCPTool\n\ntree = Tree()\nmcp_tool = MCPTool(\n    server_name=\"my_server\",\n    server_script_path=\"/path/to/server.py\"\n)\ntree.add_tool(mcp_tool, branch_id=\"base\")\n</code></pre>"},{"location":"MCP/api_integration/#option-b-configuration-based-via-mcpjson","title":"Option B: Configuration-based (via mcp.json)","text":"<pre><code>{\n  \"servers\": [\n    {\n      \"name\": \"example_server\",\n      \"description\": \"Example MCP server\",\n      \"server_script_path\": \"/path/to/server.py\",\n      \"enabled\": true\n    }\n  ]\n}\n</code></pre> <p>When enabled in <code>mcp.json</code>: 1. Tool class <code>MCP_example_server</code> is created automatically 2. Available via <code>/tools/available</code> API endpoint 3. Can be instantiated and added to any Tree</p>"},{"location":"MCP/api_integration/#complete-api-flow-diagram","title":"Complete API Flow Diagram","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Application Startup                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1. app.py starts FastAPI                                   \u2502\n\u2502 2. Imports custom_tools module                             \u2502\n\u2502 3. custom_tools imports mcp_loader                         \u2502\n\u2502 4. mcp_loader.load_mcp_servers_from_config() executes     \u2502\n\u2502 5. Reads elysia/mcp.json                                   \u2502\n\u2502 6. Creates MCPTool subclasses for enabled servers          \u2502\n\u2502 7. Injects classes into custom_tools namespace             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 User Initialization (POST /init/user/{user_id})           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1. UserManager.add_user_local(user_id)                    \u2502\n\u2502 2. Creates TreeManager(user_id, config=None)              \u2502\n\u2502 3. TreeManager initializes with default Config            \u2502\n\u2502 4. Config contains Settings (API keys, LLM settings)       \u2502\n\u2502 5. ClientManager created with Settings                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Tree Initialization (POST /init/tree/{user_id}/{conv_id}) \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1. UserManager.initialise_tree(user_id, conversation_id)  \u2502\n\u2502 2. TreeManager.add_tree(conversation_id)                  \u2502\n\u2502 3. Creates Tree(settings=TreeManager.settings)            \u2502\n\u2502 4. Tree.__init__:                                         \u2502\n\u2502    - Sets branch_initialisation (default: \"one_branch\")   \u2502\n\u2502    - Calls one_branch_init()                              \u2502\n\u2502    - Adds default tools to \"base\" branch:                 \u2502\n\u2502      * CitedSummarizer                                     \u2502\n\u2502      * FakeTextResponse                                    \u2502\n\u2502      * Aggregate                                           \u2502\n\u2502      * Query                                               \u2502\n\u2502      * Visualise                                           \u2502\n\u2502      * SummariseItems                                      \u2502\n\u2502 5. Tree ready to process queries                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Tool Availability (GET /tools/available)                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 1. discover_tools_from_module() inspects custom_tools     \u2502\n\u2502 2. Discovers all Tool subclasses including:               \u2502\n\u2502    - Built-in tools (Query, Aggregate, etc.)              \u2502\n\u2502    - Custom tools (TellAJoke, BasicLinearRegression)      \u2502\n\u2502    - MCP tools (MCP_{server_name} for each enabled)       \u2502\n\u2502 3. Returns metadata for all discovered tools               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"MCP/api_integration/#key-files-and-their-roles","title":"Key Files and Their Roles","text":"File Purpose Key Functions <code>elysia/api/app.py</code> FastAPI application entry Mounts routers, starts scheduler <code>elysia/api/routes/init.py</code> User/Tree initialization <code>initialise_user()</code>, <code>initialise_tree()</code> <code>elysia/api/routes/tools.py</code> Tool discovery API <code>/tools/available</code>, uses <code>discover_tools_from_module()</code> <code>elysia/api/custom_tools.py</code> Tool registration point Imports all tools including MCP <code>elysia/api/services/user.py</code> User management <code>UserManager.add_user_local()</code> <code>elysia/api/services/tree.py</code> Tree management <code>TreeManager.add_tree()</code> <code>elysia/tree/tree.py</code> Decision tree core <code>Tree.__init__()</code>, branch/tool management <code>elysia/tools/mcp/mcp_loader.py</code> MCP tool loading <code>load_mcp_servers_from_config()</code> <code>elysia/tools/mcp/mcp_tool.py</code> MCP tool class <code>MCPTool</code> class definition <code>elysia/mcp.json</code> MCP configuration List of MCP servers to load"},{"location":"MCP/api_integration/#default-settings-flow","title":"Default Settings Flow","text":""},{"location":"MCP/api_integration/#settings-initialization-path","title":"Settings Initialization Path","text":"<pre><code>1. Environment Variables (.env file)\n   \u2193\n2. elysia/config.py \u2192 environment_settings (singleton)\n   \u2193\n3. Tree(settings=None) \u2192 uses environment_settings\n   \u2193\n4. TreeManager(config=None) \u2192 creates Config()\n   \u2193\n5. Config() \u2192 creates Settings() \u2192 uses environment_settings\n   \u2193\n6. UserManager.add_user_local(config=None) \u2192 TreeManager gets default Config\n</code></pre>"},{"location":"MCP/api_integration/#default-tools-in-tree","title":"Default Tools in Tree","text":"<p>When <code>Tree()</code> is initialized with <code>branch_initialisation=\"one_branch\"</code> (default):</p> <p>Branch \"base\" gets: - <code>CitedSummarizer</code> - Summarizes content with citations - <code>FakeTextResponse</code> - Simple text response - <code>Aggregate</code> - Aggregate/summarize data from knowledge base - <code>Query</code> - Query knowledge base semantically - <code>Visualise</code> - Create visualizations - <code>SummariseItems</code> - Summarize retrieved items (linked to Query)</p> <p>These tools are always available in a default Tree.</p>"},{"location":"MCP/api_integration/#mcp-tool-availability","title":"MCP Tool Availability","text":""},{"location":"MCP/api_integration/#current-state","title":"Current State","text":"<ul> <li>Configuration: <code>elysia/mcp.json</code> with empty servers array</li> <li>Available MCP Tools: None (no servers enabled)</li> <li>Status: Infrastructure ready, awaiting server configuration</li> </ul>"},{"location":"MCP/api_integration/#to-enable-mcp-tools","title":"To Enable MCP Tools","text":"<ol> <li> <p>Edit <code>elysia/mcp.json</code>: <pre><code>{\n  \"servers\": [\n    {\n      \"name\": \"my_mcp_server\",\n      \"description\": \"My custom MCP server\",\n      \"server_script_path\": \"/path/to/mcp_server.py\",\n      \"enabled\": true\n    }\n  ]\n}\n</code></pre></p> </li> <li> <p>Rebuild container: <pre><code>docker-compose build elysia &amp;&amp; docker-compose up -d elysia\n</code></pre></p> </li> <li> <p>Verify tool is available: <pre><code>curl http://localhost:8000/tools/available\n# Should show \"MCP_my_mcp_server\" in tools list\n</code></pre></p> </li> <li> <p>Use in Tree (programmatically): <pre><code># The dynamically created class is already available\nfrom elysia.api.custom_tools import MCP_my_mcp_server\n\ntree = some_tree  # Get from TreeManager\nmcp_tool = MCP_my_mcp_server()\ntree.add_tool(mcp_tool, branch_id=\"base\")\n</code></pre></p> </li> </ol>"},{"location":"MCP/api_integration/#tool-discovery-mechanism","title":"Tool Discovery Mechanism","text":"<p>The <code>/tools/available</code> endpoint uses <code>discover_tools_from_module()</code> and <code>get_tool_metadata()</code> from <code>elysia/util/tool_discovery.py</code> which:</p> <ol> <li>Inspects <code>custom_tools.__dict__</code></li> <li>Filters for classes that:</li> <li>Are subclasses of <code>Tool</code></li> <li>Are not the base <code>Tool</code> class itself</li> <li>Calls <code>get_metadata()</code> on each class</li> <li>Returns metadata dict for all discovered tools</li> </ol> <p>MCP tools are discovered because: - <code>mcp_loader.__all__</code> lists all dynamically created classes - <code>custom_tools.py</code> imports them into its namespace via:   <pre><code>for _mcp_tool_name in mcp_loader.__all__:\n    globals()[_mcp_tool_name] = getattr(mcp_loader, _mcp_tool_name)\n</code></pre></p>"},{"location":"MCP/api_integration/#verification-tests","title":"Verification Tests","text":""},{"location":"MCP/api_integration/#test-1-tool-discovery","title":"Test 1: Tool Discovery \u2713","text":"<pre><code>curl http://localhost:8000/tools/available\n# Returns: 6 tools (Query, Aggregate, etc.)\n</code></pre>"},{"location":"MCP/api_integration/#test-2-user-initialization","title":"Test 2: User Initialization \u2713","text":"<pre><code>curl -X POST http://localhost:8000/init/user/test_user_123\n# Returns: user_exists=False, config with default settings\n</code></pre>"},{"location":"MCP/api_integration/#test-3-tree-initialization","title":"Test 3: Tree Initialization \u2713","text":"<pre><code>curl -X POST http://localhost:8000/init/tree/test_user_123/test_conv_456 \\\n  -H \"Content-Type: application/json\" -d '{\"low_memory\": false}'\n# Returns: tree initialized successfully\n</code></pre>"},{"location":"MCP/api_integration/#test-4-mcp-tool-loading-ready","title":"Test 4: MCP Tool Loading (Ready)","text":"<pre><code># Edit mcp.json to enable a server, then:\ncurl http://localhost:8000/tools/available\n# Should include MCP_* tool(s)\n</code></pre>"},{"location":"MCP/api_integration/#summary","title":"Summary","text":"<p>\u2705 Complete API flow established: - User initialization creates UserManager \u2192 TreeManager \u2192 Config \u2192 Settings - Tree initialization creates Tree with default tools and Settings - MCP tool loading infrastructure ready at application startup - Tool discovery API exposes all available tools including MCP</p> <p>\u2705 MCP tools ready to be loaded: - Infrastructure in place - Configuration file ready (<code>mcp.json</code>) - No servers currently enabled - When enabled, tools will be automatically discovered</p> <p>\u2705 Default tools available in Tree: - All standard Elysia tools loaded on Tree init - Tools use Settings from TreeManager - ClientManager available for tools needing external connections</p> <p>\ud83c\udfaf Next Steps (when needed): 1. Create or locate MCP server script 2. Enable server in <code>mcp.json</code> 3. Rebuild container 4. MCP tools automatically available to all Trees</p> <p>Architecture Status: Complete and Functional \u2713</p>"},{"location":"MCP/architecture/","title":"MCP Integration - Architecture &amp; Implementation","text":""},{"location":"MCP/architecture/#overview","title":"Overview","text":"<p>Minimal MCP (Model Context Protocol) server integration for Elysia using <code>langchain-mcp-adapters</code>.</p> <p>Philosophy: Use real libraries. No mock code. No overengineering. 300 lines total.</p>"},{"location":"MCP/architecture/#installation","title":"Installation","text":"<pre><code>pip install langchain-mcp-adapters\n</code></pre>"},{"location":"MCP/architecture/#quick-implementation","title":"Quick Implementation","text":"<pre><code>from elysia import Tree\nfrom elysia.tools.additional import MCPServerAdapter\n\ntree = Tree()\nadapter = MCPServerAdapter(server_script_path=\"/path/to/server.py\")\nawait adapter.initialize_and_register_tools(tree, branch_id=\"base\")\n\n# MCP tools now available\nresponse, objects = tree(\"Use the search tool...\")\n</code></pre>"},{"location":"MCP/architecture/#system-architecture","title":"System Architecture","text":"<p>View: MCP Architecture Diagram Source</p> <p>Key Components: - MCPTool Gateway: One instance per MCP server, acts as single entry point - LangChain Adapter: Handles MCP protocol communication - MCP Server: Your custom server (stdio) or remote service (SSE) - Individual Tools: Discovered dynamically from MCP server</p>"},{"location":"MCP/architecture/#component-flow","title":"Component Flow","text":""},{"location":"MCP/architecture/#1-initialization-flow","title":"1. Initialization Flow","text":"<pre><code>User Code\n    \u2193\nMCPServerAdapter.initialize()\n    \u2193\nlangchain-mcp-adapters.load_mcp_tools()\n    \u2193\nStdioServerParameters + ClientSession\n    \u2193\nConnect to MCP Server Script\n    \u2193\nLoad tools as LangChain tools\n    \u2193\nFor each LangChain tool:\n    Create MCPToolWrapper\n    \u2193\nStore in adapter.discovered_tools[]\n</code></pre>"},{"location":"MCP/architecture/#2-registration-flow","title":"2. Registration Flow","text":"<pre><code>adapter.register_tools_with_tree(tree)\n    \u2193\nFor each MCPToolWrapper:\n    \u2193\n    tree.add_tool(wrapper, branch_id)\n        \u2193\n        Tool available in decision tree\n</code></pre>"},{"location":"MCP/architecture/#3-execution-flow","title":"3. Execution Flow","text":"<pre><code>User: tree(\"Use search tool...\")\n    \u2193\nDecisionNode chooses 'mcp_search'\n    \u2193\nMCPToolWrapper.__call__()\n    \u2193\nlangchain_tool.ainvoke(inputs)\n    \u2193\nLangChain \u2192 MCP Server\n    \u2193\nMCP Server executes tool\n    \u2193\nResult returned\n    \u2193\nMCPToolWrapper processes result\n    \u2193\nYield Result/Text/Error objects\n    \u2193\nResponse to user\n</code></pre>"},{"location":"MCP/architecture/#implementation-details","title":"Implementation Details","text":""},{"location":"MCP/architecture/#components-300-lines-total","title":"Components (300 lines total)","text":""},{"location":"MCP/architecture/#1-mcpserveradapter-176-lines","title":"1. MCPServerAdapter (176 lines)","text":"<pre><code>class MCPServerAdapter(Tool):\n    - server_script_path: str\n    - discovered_tools: list[MCPToolWrapper]\n\n    Methods:\n    + initialize()                    # Load tools from MCP server\n    + register_tools_with_tree(tree)  # Register all tools\n    + get_tool_by_name(name)          # Get specific tool\n</code></pre> <p>Responsibilities: - Load tools from MCP server using <code>langchain-mcp-adapters</code> - Create MCPToolWrapper for each tool - Register tools with Elysia Tree</p> <p>Key implementation: <pre><code>async def initialize(self) -&gt; bool:\n    from langchain_mcp_adapters.tools import load_mcp_tools\n    from mcp import ClientSession, StdioServerParameters\n    from mcp.client.stdio import stdio_client\n\n    server_params = StdioServerParameters(\n        command=\"python\",\n        args=[self.server_script_path],\n    )\n\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n            langchain_tools = await load_mcp_tools(session)\n\n            for lc_tool in langchain_tools:\n                wrapper = MCPToolWrapper(\n                    langchain_tool=lc_tool,\n                    logger=self.logger,\n                )\n                self.discovered_tools.append(wrapper)\n\n    return True\n</code></pre></p>"},{"location":"MCP/architecture/#2-mcptoolwrapper-128-lines","title":"2. MCPToolWrapper (128 lines)","text":"<pre><code>class MCPToolWrapper(Tool):\n    - langchain_tool: LangChain Tool\n    - logger: Logger\n\n    Methods:\n    + __call__(inputs)           # Execute tool\n    + is_tool_available()        # Check availability\n</code></pre> <p>Responsibilities: - Wrap LangChain tool as Elysia Tool - Convert input schemas - Execute tool and process results - Return Elysia objects (Result, Text, Error)</p> <p>Key implementation: <pre><code>async def __call__(self, tree_data, inputs, base_lm, complex_lm, client_manager, **kwargs):\n    yield Status(f\"Executing {self.langchain_tool.name}...\")\n\n    try:\n        if hasattr(self.langchain_tool, \"ainvoke\"):\n            result = await self.langchain_tool.ainvoke(inputs)\n        else:\n            result = self.langchain_tool.invoke(inputs)\n\n        # Process result\n        if isinstance(result, str):\n            yield Text(result)\n        elif isinstance(result, dict):\n            yield Result([result], name=self.langchain_tool.name)\n        # ... more processing\n\n    except Exception as e:\n        yield Error(error_message=f\"Error calling {self.langchain_tool.name}: {str(e)}\")\n</code></pre></p>"},{"location":"MCP/architecture/#usage-examples","title":"Usage Examples","text":""},{"location":"MCP/architecture/#basic-register-all-tools","title":"Basic: Register All Tools","text":"<pre><code>adapter = MCPServerAdapter(server_script_path=\"/path/to/server.py\")\nsuccess, tools = await adapter.initialize_and_register_tools(tree)\n\nif success:\n    print(f\"Registered {len(tools)} tools: {tools}\")\n</code></pre>"},{"location":"MCP/architecture/#selective-register-specific-tools","title":"Selective: Register Specific Tools","text":"<pre><code>adapter = MCPServerAdapter(server_script_path=\"/path/to/server.py\")\nawait adapter.initialize()\n\n# List discovered tools\nfor tool in adapter.discovered_tools:\n    print(f\"{tool.name}: {tool.description}\")\n\n# Register only specific tools\nsearch_tool = adapter.get_tool_by_name(\"search\")\nif search_tool:\n    tree.add_tool(search_tool, branch_id=\"base\")\n</code></pre>"},{"location":"MCP/architecture/#multiple-servers","title":"Multiple Servers","text":"<pre><code>servers = [\"/path/to/server1.py\", \"/path/to/server2.py\"]\n\nfor server_path in servers:\n    adapter = MCPServerAdapter(server_script_path=server_path)\n    await adapter.initialize_and_register_tools(tree)\n</code></pre>"},{"location":"MCP/architecture/#design-principles-applied","title":"Design Principles Applied","text":"<p>Following CODING_INSTRUCTIONS.md:</p> <p>\u2705 Radical Minimalism - 300 lines total (was 2,000+ initially) - Deleted 1,700+ lines of overengineered code - No premature architecture</p> <p>\u2705 Use Real Libraries - Uses <code>langchain-mcp-adapters</code> directly - No mock implementations - No custom MCP clients</p> <p>\u2705 No Overengineering - 2 classes with clear responsibilities - No factory patterns - No unnecessary abstractions</p> <p>\u2705 Delete Aggressively - Removed mock MCP client implementations (250 lines) - Removed factory functions (unnecessary abstraction) - Removed HTTP client (not needed) - Removed custom protocol handling (library handles it)</p>"},{"location":"MCP/architecture/#what-was-removed","title":"What Was Removed","text":"<p>During refactoring, deleted: - \u274c <code>langchain_client.py</code> (250 lines) - Mock MCP clients - \u274c Factory functions - Single use abstraction - \u274c HTTP client - Premature feature - \u274c Custom schema conversions - Library handles it - \u274c Complex result processing - Simplified</p> <p>Result: 85% code reduction (2,000 \u2192 300 lines)</p>"},{"location":"MCP/architecture/#file-structure","title":"File Structure","text":"<pre><code>elysia/tools/additional/\n\u251c\u2500\u2500 __init__.py                   # 3 lines - exports\n\u251c\u2500\u2500 mcp_adapter.py                # 176 lines - main adapter\n\u251c\u2500\u2500 mcp_tool_wrapper.py           # 128 lines - tool wrapper\n\u251c\u2500\u2500 ARCHITECTURE.md              # This file\n\u2514\u2500\u2500 QUICKSTART.md                # Quick reference\n\nexamples/\n\u2514\u2500\u2500 mcp_integration_example.py   # Working examples\n</code></pre>"},{"location":"MCP/architecture/#data-flow-example","title":"Data Flow Example","text":""},{"location":"MCP/architecture/#input","title":"Input","text":"<pre><code>tree(\"Search for documents about machine learning\")\n</code></pre>"},{"location":"MCP/architecture/#processing","title":"Processing","text":"<pre><code>1. DecisionNode selects 'mcp_search' tool\n2. Inputs: {\"query\": \"machine learning\"}\n3. MCPToolWrapper.__call__(inputs)\n4. langchain_tool.ainvoke({\"query\": \"machine learning\"})\n5. LangChain \u2192 MCP Server Script\n6. MCP Server processes query\n7. Returns: \"Found 3 documents...\"\n8. MCPToolWrapper yields Text(\"Found 3 documents...\")\n</code></pre>"},{"location":"MCP/architecture/#output","title":"Output","text":"<pre><code>Response: \"Found 3 documents about machine learning\"\nObjects: [Text(\"Found 3 documents...\")]\n</code></pre>"},{"location":"MCP/architecture/#design-patterns","title":"Design Patterns","text":"<ol> <li>Adapter Pattern: MCPServerAdapter adapts MCP server interface to Elysia</li> <li>Wrapper Pattern: MCPToolWrapper wraps LangChain tools as Elysia Tools</li> <li>Delegation Pattern: Delegates to <code>langchain-mcp-adapters</code> for MCP interaction</li> </ol>"},{"location":"MCP/architecture/#error-handling","title":"Error Handling","text":"<pre><code># If langchain-mcp-adapters not installed\n# Returns: False + logs \"pip install langchain-mcp-adapters\"\n\n# If server script not found\n# Returns: False + logs error message\n\n# If tool execution fails\n# Yields: Error object with details\n</code></pre>"},{"location":"MCP/architecture/#testing-connection","title":"Testing Connection","text":"<pre><code>adapter = MCPServerAdapter(server_script_path=\"/path/to/server.py\")\nsuccess = await adapter.initialize()\n\nif success:\n    print(f\"\u2713 Connected! Loaded {len(adapter.discovered_tools)} tools\")\n    for tool in adapter.discovered_tools:\n        print(f\"  - {tool.name}\")\nelse:\n    print(\"\u2717 Failed to connect\")\n</code></pre>"},{"location":"MCP/architecture/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li><code>langchain-mcp-adapters</code> package</li> <li>MCP server script (you implement)</li> </ul>"},{"location":"MCP/architecture/#key-benefits","title":"Key Benefits","text":"<ol> <li>Minimal: 300 lines vs. 2,000+ overengineered</li> <li>Functional: Uses real library, actually works</li> <li>Maintainable: Simple architecture, clear flow</li> <li>Standard: Uses established patterns</li> <li>Honest: Every line has a purpose</li> </ol>"},{"location":"MCP/architecture/#troubleshooting","title":"Troubleshooting","text":"Problem Solution Import error <code>pip install langchain-mcp-adapters</code> Server not found Check <code>server_script_path</code> is correct No tools loaded Verify MCP server implements tools Tool execution fails Check tool inputs match schema"},{"location":"MCP/architecture/#code-stats","title":"Code Stats","text":"Metric Value Total code lines 307 Files 2 main + 1 init Mock code 0 lines Abstraction layers 2 (adapter, wrapper) Dependencies 1 (<code>langchain-mcp-adapters</code>) Lines deleted 1,700+ <p>Simple. Minimal. Actually works. 300 lines. No overengineering.</p>"},{"location":"MCP/implementation_details/","title":"MCP Implementation Details","text":"<p>Comprehensive technical documentation for MCP integration with Elysia.</p>"},{"location":"MCP/implementation_details/#architecture-overview","title":"Architecture Overview","text":""},{"location":"MCP/implementation_details/#mcp_as_agent-modes","title":"MCP_AS_AGENT Modes","text":""},{"location":"MCP/implementation_details/#agent-mode-mcp_as_agenttrue","title":"Agent Mode (<code>MCP_AS_AGENT=True</code>)","text":"<p>One agent tool per MCP server. Accepts natural language, uses ReAct agent.</p> <p></p> <p>Usage: <pre><code>tree(\"Search for ML papers and analyze sentiment\")\n</code></pre></p>"},{"location":"MCP/implementation_details/#individual-mode-mcp_as_agentfalse","title":"Individual Mode (<code>MCP_AS_AGENT=False</code>)","text":"<p>One Elysia tool per MCP server tool. Structured inputs, direct execution.</p> <p></p> <p>Usage: <pre><code>tree.tools[\"mcp_server_search\"](inputs={\"query\": \"ML papers\"})\n</code></pre></p>"},{"location":"MCP/implementation_details/#connection-architecture","title":"Connection Architecture","text":""},{"location":"MCP/implementation_details/#ssehttp-transport","title":"SSE/HTTP Transport","text":"<pre><code>sequenceDiagram\n    participant E as Elysia\n    participant M as MCPTool\n    participant S as SSE Client\n    participant H as Running MCP Server\n\n    E-&gt;&gt;M: Initialize\n    M-&gt;&gt;S: Connect(url, headers)\n    S-&gt;&gt;H: HTTP/SSE Connection\n    H--&gt;&gt;S: Tool List\n    S--&gt;&gt;M: LangChain Tools\n    M--&gt;&gt;E: Ready\n\n    E-&gt;&gt;M: Execute\n    M-&gt;&gt;S: Tool Call\n    S-&gt;&gt;H: HTTP Request\n    H--&gt;&gt;S: Result\n    S--&gt;&gt;M: Tool Output\n    M--&gt;&gt;E: Result/Text/Error</code></pre> <p>Configuration: <pre><code>{\n  \"servers\": [{\n    \"name\": \"api_server\",\n    \"url\": \"http://localhost:8080/mcp\",\n    \"headers\": {\"Authorization\": \"Bearer token\"},\n    \"enabled\": true\n  }]\n}\n</code></pre></p>"},{"location":"MCP/implementation_details/#tool-discovery-loading","title":"Tool Discovery &amp; Loading","text":""},{"location":"MCP/implementation_details/#automatic-tool-discovery","title":"Automatic Tool Discovery","text":"<pre><code>flowchart TD\n    Start([App Start]) --&gt; Load[Load mcp.json]\n    Load --&gt; Check{MCP_AS_AGENT?}\n\n    Check --&gt;|True| Agent[Create MCPTool&lt;br/&gt;Agent Gateway]\n    Check --&gt;|False| Individual[Discover MCP Tools&lt;br/&gt;Create Wrappers]\n\n    Agent --&gt; Register[Auto-register in Tree]\n    Individual --&gt; Register\n\n    Register --&gt; Available[Tools Available]\n\n    style Agent fill:#4A90E2,color:#fff\n    style Individual fill:#F39C12,color:#fff\n    style Available fill:#27AE60,color:#fff</code></pre>"},{"location":"MCP/implementation_details/#files-modified","title":"Files Modified","text":"<ol> <li><code>elysia/tools/mcp/mcp_tool.py</code></li> <li>SSE/HTTP transport only</li> <li>Agent mode: Natural language \u2192 ReAct agent</li> <li> <p>Gateway mode: action/tool_name/tool_inputs</p> </li> <li> <p><code>elysia/tools/mcp/mcp_loader.py</code></p> </li> <li>Reads <code>mcp.json</code></li> <li>Creates tool classes based on <code>MCP_AS_AGENT</code></li> <li> <p>Auto-exports for discovery</p> </li> <li> <p><code>elysia/tools/ui/default_tools.py</code></p> </li> <li>Auto-discovers MCP tools (prefix: <code>MCP_</code>)</li> <li>Adds to root branch during tree init</li> </ol>"},{"location":"MCP/implementation_details/#parameter-surfacing","title":"Parameter Surfacing","text":""},{"location":"MCP/implementation_details/#agent-mode","title":"Agent Mode","text":"<p>Input: <pre><code>{\"query\": \"Natural language task\"}\n</code></pre></p> <p>Flow: 1. MCPTool receives query 2. ReAct agent plans execution 3. Agent selects appropriate MCP tools 4. Returns combined result</p>"},{"location":"MCP/implementation_details/#gateway-mode","title":"Gateway Mode","text":"<p>Input: <pre><code>{\n    \"action\": \"list\" | \"execute\",\n    \"tool_name\": \"tool_name\",  # if execute\n    \"tool_inputs\": {...}        # if execute\n}\n</code></pre></p> <p>Flow: 1. <code>action='list'</code>: Returns available tools 2. <code>action='execute'</code>: Runs specific tool with inputs</p>"},{"location":"MCP/implementation_details/#individual-mode","title":"Individual Mode","text":"<p>Input: Tool-specific schema <pre><code>{\"param1\": \"value1\", \"param2\": \"value2\"}\n</code></pre></p> <p>Flow: 1. Direct connection to MCP server 2. Execute specific tool 3. Return result</p>"},{"location":"MCP/implementation_details/#execution-examples","title":"Execution Examples","text":""},{"location":"MCP/implementation_details/#agent-mode_1","title":"Agent Mode","text":"<pre><code># Natural language query\nresult = tree(\"Search API documentation for authentication methods\")\n\n# Agent will:\n# 1. Call MCP 'search' tool with query\n# 2. Analyze results\n# 3. Return summary\n</code></pre>"},{"location":"MCP/implementation_details/#gateway-mode_1","title":"Gateway Mode","text":"<pre><code># List available tools\nresult = tree.tools[\"mcp_api_server\"]({\"action\": \"list\"})\n\n# Execute specific tool\nresult = tree.tools[\"mcp_api_server\"]({\n    \"action\": \"execute\",\n    \"tool_name\": \"search\",\n    \"tool_inputs\": {\"query\": \"authentication\"}\n})\n</code></pre>"},{"location":"MCP/implementation_details/#individual-mode_1","title":"Individual Mode","text":"<pre><code># Direct tool execution\nresult = tree.tools[\"mcp_api_server_search\"]({\n    \"query\": \"authentication\",\n    \"limit\": 10\n})\n</code></pre>"},{"location":"MCP/implementation_details/#configuration","title":"Configuration","text":""},{"location":"MCP/implementation_details/#mcpjson-schema","title":"mcp.json Schema","text":"<pre><code>{\n  \"servers\": [\n    {\n      \"name\": \"api_server\",\n      \"description\": \"API documentation server\",\n      \"url\": \"http://localhost:8080/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer ${API_TOKEN}\",\n        \"Content-Type\": \"application/json\"\n      },\n      \"enabled\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"MCP/implementation_details/#environment-variables","title":"Environment Variables","text":"<pre><code># Control operating mode\nMCP_AS_AGENT=True  # or False\n\n# Model configuration (for agent mode)\nBASE_MODEL=gpt-4o-mini\nOPENAI_API_KEY=sk-...\n\n# Optional: API tokens for MCP servers\nAPI_TOKEN=your_token_here\n</code></pre>"},{"location":"MCP/implementation_details/#benefits","title":"Benefits","text":""},{"location":"MCP/implementation_details/#ssehttp-transport_1","title":"SSE/HTTP Transport","text":"<p>\u2705 No local server management \u2705 Connect to pre-running services \u2705 Scalable architecture \u2705 Standard HTTP authentication \u2705 Network-accessible  </p>"},{"location":"MCP/implementation_details/#agent-mode_2","title":"Agent Mode","text":"<p>\u2705 Natural language interface \u2705 Autonomous execution \u2705 Multi-step reasoning \u2705 Flexible tool usage  </p>"},{"location":"MCP/implementation_details/#individual-mode_2","title":"Individual Mode","text":"<p>\u2705 Direct control \u2705 Predictable behavior \u2705 Lower latency \u2705 Fine-grained error handling  </p>"},{"location":"MCP/implementation_details/#testing","title":"Testing","text":"<pre><code># Test MCP connection\nfrom elysia.tools.mcp.mcp_tool import MCPTool\n\ntool = MCPTool(\n    server_name=\"test\",\n    url=\"http://localhost:8080/mcp\"\n)\n\nsuccess = await tool.initialize()\nassert success\nassert len(tool._langchain_tools) &gt; 0\n</code></pre>"},{"location":"MCP/implementation_details/#troubleshooting","title":"Troubleshooting","text":"Issue Solution Connection failed Check MCP server is running at URL No tools found Verify server returns tools on connection Agent errors Check BASE_MODEL and OPENAI_API_KEY Import errors <code>pip install langchain langchain-openai langchain-mcp-adapters</code>"},{"location":"MCP/implementation_details/#summary","title":"Summary","text":"<ul> <li>Transport: SSE/HTTP only (assumes running servers)</li> <li>Modes: Agent (natural language) or Individual (structured)</li> <li>Discovery: Automatic tool loading from mcp.json</li> <li>Configuration: Simple JSON + environment variables</li> </ul> <p>Status: Production Ready \u2705</p>"},{"location":"MCP/implementation_details_old/","title":"MCP Implementation Details","text":"<p>This document provides comprehensive implementation information for the MCP integration, including architecture, interaction models, and technical details.</p>"},{"location":"MCP/implementation_details_old/#mcp-tool-interaction-model","title":"MCP Tool Interaction Model","text":""},{"location":"MCP/implementation_details_old/#architecture-decision-mcp-server-as-gateway-tool","title":"Architecture Decision: MCP Server as Gateway Tool","text":"<p>Key Concept: Each <code>MCPTool</code> instance represents one MCP server that acts as a gateway to multiple underlying tools.</p> <p>Design Pattern: Gateway + Two-Phase Execution - Phase 1: Discovery - List available tools from the MCP server - Phase 2: Execution - Execute a specific tool by name</p> <p>NOT: Each MCP server tool as a separate Elysia Tool YES: One Elysia Tool per MCP server, exposing multiple tools via actions</p>"},{"location":"MCP/implementation_details_old/#interaction-flow","title":"Interaction Flow","text":"<pre><code>graph TB\n    subgraph \"User/Agent Layer\"\n        User[User Query]\n        Agent[Elysia Decision Agent]\n    end\n\n    subgraph \"Elysia Tree Layer\"\n        Tree[Tree Instance]\n        DecisionNode[Decision Node: 'base']\n        Options[Available Options]\n    end\n\n    subgraph \"MCPTool Gateway Layer\"\n        MCPTool[MCPTool Instance&lt;br/&gt;name: 'mcp_api_ai_mcp'&lt;br/&gt;One per MCP Server]\n        Action{Action Type}\n        ListAction[Action: 'list'&lt;br/&gt;Discover Tools]\n        ExecAction[Action: 'execute'&lt;br/&gt;Run Specific Tool]\n    end\n\n    subgraph \"LangChain Adapter Layer\"\n        LCAdapter[langchain-mcp-adapters]\n        LCTools[LangChain Tools List&lt;br/&gt;tool1, tool2, tool3...]\n    end\n\n    subgraph \"MCP Server Layer\"\n        MCPServer[MCP Server Process&lt;br/&gt;stdio or SSE]\n        Tool1[MCP Tool: search]\n        Tool2[MCP Tool: analyze]\n        Tool3[MCP Tool: summarize]\n    end\n\n    User --&gt; Agent\n    Agent --&gt; Tree\n    Tree --&gt; DecisionNode\n    DecisionNode --&gt; Options\n    Options --&gt; MCPTool\n\n    MCPTool --&gt; Action\n    Action --&gt;|action='list'| ListAction\n    Action --&gt;|action='execute'| ExecAction\n\n    ListAction --&gt; LCAdapter\n    ExecAction --&gt; LCAdapter\n\n    LCAdapter --&gt; MCPServer\n\n    MCPServer --&gt; Tool1\n    MCPServer --&gt; Tool2\n    MCPServer --&gt; Tool3\n\n    LCTools -.cached in.-&gt; MCPTool\n\n    style MCPTool fill:#4A90E2,stroke:#2E5C8A,color:#fff\n    style Agent fill:#9B59B6,stroke:#7D3C98,color:#fff\n    style LCAdapter fill:#F39C12,stroke:#D68910,color:#fff\n    style MCPServer fill:#E74C3C,stroke:#C0392B,color:#fff</code></pre>"},{"location":"MCP/implementation_details_old/#parameter-surfacing-model","title":"Parameter Surfacing Model","text":"<pre><code>sequenceDiagram\n    participant Agent as Elysia Agent\n    participant MCPTool as MCPTool Gateway\n    participant Init as initialize()\n    participant LC as LangChain Tools\n    participant MCP as MCP Server\n\n    Note over Agent,MCP: Phase 1: Discovery &amp; Initialization\n\n    Agent-&gt;&gt;MCPTool: First call (any action)\n    MCPTool-&gt;&gt;Init: Check _initialized flag\n    Init-&gt;&gt;LC: load_mcp_tools(session)\n    LC-&gt;&gt;MCP: Connect via stdio/SSE\n    MCP--&gt;&gt;LC: Return tool list with schemas\n    LC--&gt;&gt;Init: LangChain tool objects\n    Init-&gt;&gt;MCPTool: Cache tools in _langchain_tools[]\n    Note over MCPTool: Tools cached for reuse\n\n    Note over Agent,MCP: Phase 2a: List Tools (Discovery)\n\n    Agent-&gt;&gt;MCPTool: Call with action='list'\n    MCPTool-&gt;&gt;MCPTool: Extract tool metadata\n    Note over MCPTool: For each cached LangChain tool:&lt;br/&gt;name, description, input schema\n    MCPTool--&gt;&gt;Agent: Result([{name, description, inputs}])\n\n    Note over Agent,MCP: Phase 2b: Execute Tool\n\n    Agent-&gt;&gt;MCPTool: Call with action='execute'&lt;br/&gt;+ tool_name='search'&lt;br/&gt;+ tool_inputs={query: \"...\"}\n    MCPTool-&gt;&gt;MCPTool: get_tool_by_name('search')\n    MCPTool-&gt;&gt;LC: langchain_tool.ainvoke(tool_inputs)\n    LC-&gt;&gt;MCP: Execute 'search' with inputs\n    MCP--&gt;&gt;LC: Tool result\n    LC--&gt;&gt;MCPTool: Result data\n    MCPTool--&gt;&gt;Agent: Result/Text/Error objects</code></pre>"},{"location":"MCP/implementation_details_old/#why-this-design","title":"Why This Design?","text":"<p>Advantages of Gateway Pattern:</p> <ol> <li>Single Tool Registration: One MCPTool per server instead of N separate tools</li> <li>Dynamic Discovery: Tools can change without code regeneration</li> <li>Lazy Loading: Connect to MCP server only when first used</li> <li>Unified Management: Single point for monitoring, logging, error handling</li> <li>Tool Metadata Access: Agent can query available tools before execution</li> </ol> <p>Alternative Rejected: Individual Tool Wrapping <pre><code># \u274c NOT IMPLEMENTED: Each MCP tool as separate Elysia tool\ntree.add_tool(MCPSearchTool())      # Would need separate class\ntree.add_tool(MCPAnalyzeTool())     # Would need separate class\ntree.add_tool(MCPSummarizeTool())   # Would need separate class\n\n# \u2705 IMPLEMENTED: Gateway pattern\ntree.add_tool(MCPTool(server_name=\"api_ai_mcp\"))  # One tool, multiple capabilities\n</code></pre></p>"},{"location":"MCP/implementation_details_old/#changes-summary","title":"Changes Summary","text":""},{"location":"MCP/implementation_details_old/#quick-summary","title":"Quick Summary","text":"<p>Problem: MCP tools from <code>mcp.json</code> were not visible in the tree structure sent to the frontend.</p> <p>Solution: Enhanced tool discovery to include MCP tools and auto-load them during tree initialization.</p> <p>Result: MCP tools now automatically appear in the tree structure and are visible in the UI.</p>"},{"location":"MCP/implementation_details_old/#tool-discovery-loading-flow","title":"Tool Discovery &amp; Loading Flow","text":""},{"location":"MCP/implementation_details_old/#before-fix-tools-not-visible","title":"Before Fix (Tools Not Visible)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Application Startup                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 elysia/tools/mcp/mcp_loader.py                                  \u2502\n\u2502 - Reads mcp.json                                                \u2502\n\u2502 - Creates MCP_api_ai_mcp class                                  \u2502\n\u2502 - Exports to module namespace                                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Tree Initialization: Tree(branch_initialisation=\"one_branch\")   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 set_branch_initialisation(\"one_branch\")                         \u2502\n\u2502   \u2514\u2500&gt; load_default_tools_for_mode()                            \u2502\n\u2502       \u2514\u2500&gt; Adds: Query, Aggregate, Visualise, etc.              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 _load_additional_discovered_tools()                             \u2502\n\u2502   \u2514\u2500&gt; discover_tools_from_module()                             \u2502\n\u2502       \u2514\u2500&gt; Only searches: elysia.api.custom_tools               \u2502\n\u2502           \u274c DOES NOT FIND MCP tools!                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 tree.tree structure                                             \u2502\n\u2502 {                                                               \u2502\n\u2502   \"base\": {                                                     \u2502\n\u2502     \"options\": {                                                \u2502\n\u2502       \"query\": {...},                                           \u2502\n\u2502       \"aggregate\": {...}                                        \u2502\n\u2502       \u274c NO MCP TOOLS                                           \u2502\n\u2502     }                                                           \u2502\n\u2502   }                                                             \u2502\n\u2502 }                                                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Frontend: GET /tree/{user_id}/{conversation_id}                 \u2502\n\u2502   \u2514\u2500&gt; Receives tree.tree                                       \u2502\n\u2502       \u2514\u2500&gt; MCP tools NOT VISIBLE in UI \u274c                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n## Tool Discovery &amp; Loading Flow\n\n### Before Fix (Tools Not Visible)\n\n```mermaid\nflowchart TD\n    Start([Application Startup]) --&gt; LoadMCP[elysia/tools/mcp/mcp_loader.py&lt;br/&gt;Reads mcp.json&lt;br/&gt;Creates MCP classes]\n    LoadMCP --&gt; TreeInit[Tree Initialization&lt;br/&gt;branch_initialisation='one_branch']\n    TreeInit --&gt; SetBranch[set_branch_initialisation]\n    SetBranch --&gt; LoadDefault[load_default_tools_for_mode&lt;br/&gt;Adds: Query, Aggregate, etc.]\n    LoadDefault --&gt; DiscoverAdditional[_load_additional_discovered_tools]\n    DiscoverAdditional --&gt; SearchCustom[discover_tools_from_module&lt;br/&gt;Only searches: elysia.api.custom_tools]\n    SearchCustom --&gt; NoMCP[\u274c DOES NOT FIND MCP tools!]\n    NoMCP --&gt; BuildTree[tree.tree structure&lt;br/&gt;NO MCP TOOLS]\n    BuildTree --&gt; Frontend[Frontend GET /tree/user/conversation&lt;br/&gt;\u274c MCP tools NOT VISIBLE]\n\n    style NoMCP fill:#ffcccc,stroke:#cc0000\n    style Frontend fill:#ffcccc,stroke:#cc0000\n    style BuildTree fill:#ffeecc,stroke:#cc6600\n</code></pre>"},{"location":"MCP/implementation_details_old/#after-fix-tools-visible","title":"After Fix (Tools Visible)","text":"<pre><code>flowchart TD\n    Start([Application Startup]) --&gt; LoadMCP[elysia/tools/mcp/mcp_loader.py&lt;br/&gt;Reads mcp.json&lt;br/&gt;Creates MCP classes]\n    LoadMCP --&gt; TreeInit[Tree Initialization&lt;br/&gt;branch_initialisation='one_branch']\n    TreeInit --&gt; SetBranch[set_branch_initialisation]\n    SetBranch --&gt; LoadDefault[load_default_tools_for_mode \u2728&lt;br/&gt;Adds: Query, Aggregate, etc.]\n    LoadDefault --&gt; AutoDiscover[\ud83c\udd95 Auto-discover MCP tools]\n    AutoDiscover --&gt; SearchMCP[discover_tools_from_module&lt;br/&gt;Searches: elysia.tools.mcp.mcp_loader]\n    SearchMCP --&gt; FoundMCP[\u2705 FINDS: MCP_api_ai_mcp]\n    FoundMCP --&gt; AddTool[tree.add_tool MCP_api_ai_mcp]\n    AddTool --&gt; UpdateNode[tree.decision_nodes base.options&lt;br/&gt;mcp_api_ai_mcp added]\n    UpdateNode --&gt; BuildTree[tree._construct_tree&lt;br/&gt;MCP TOOLS IN STRUCTURE]\n    BuildTree --&gt; Frontend[Frontend GET /tree/user/conversation&lt;br/&gt;\u2705 MCP tools VISIBLE in UI]\n\n    style FoundMCP fill:#ccffcc,stroke:#00cc00\n    style Frontend fill:#ccffcc,stroke:#00cc00\n    style BuildTree fill:#eeffcc,stroke:#66cc00\n    style AutoDiscover fill:#cceeff,stroke:#0066cc</code></pre>"},{"location":"MCP/implementation_details_old/#mcptool-inputoutput-contract","title":"MCPTool Input/Output Contract","text":""},{"location":"MCP/implementation_details_old/#tool-definition-what-agent-sees","title":"Tool Definition (What Agent Sees)","text":"<pre><code>classDiagram\n    class MCPTool {\n        +name: str = \"mcp_server_name\"\n        +description: str = \"MCP server '...' provides access to multiple tools\"\n        +status: str = \"Ready to connect\"\n        +inputs: dict\n        +end: bool = False\n\n        +initialize() bool\n        +__call__(inputs) AsyncGenerator\n        +get_tool_by_name(name) Tool\n        +is_tool_available() bool\n    }\n\n    class ToolInputs {\n        action: str = \"list|execute\"\n        tool_name: Optional~str~\n        tool_inputs: Optional~dict~\n    }\n\n    class ToolOutputs {\n        Status: \"Initializing...\"\n        Result: List~dict~\n        Text: str\n        Error: str\n    }\n\n    MCPTool --&gt; ToolInputs : expects\n    MCPTool --&gt; ToolOutputs : yields\n\n    note for MCPTool \"Single gateway to multiple MCP tools&lt;br/&gt;Agent sees ONE tool per MCP server\"\n    note for ToolInputs \"action='list': Discover tools&lt;br/&gt;action='execute': Run specific tool\"</code></pre>"},{"location":"MCP/implementation_details_old/#input-schema-surfacing","title":"Input Schema Surfacing","text":"<p>The agent receives this schema in <code>DecisionPrompt.available_actions</code>:</p> <pre><code>{\n    \"name\": \"mcp_api_ai_mcp\",\n    \"description\": \"MCP server 'api-ai-mcp' (stdio) - provides access to multiple tools\",\n    \"inputs\": {\n        \"action\": {\n            \"description\": \"Action: 'list' to show tools, 'execute' to run a specific tool\",\n            \"type\": \"&lt;class 'str'&gt;\",\n            \"default\": \"list\"\n        },\n        \"tool_name\": {\n            \"description\": \"Name of the tool to execute (required when action='execute')\",\n            \"type\": \"&lt;class 'str'&gt;\",\n            \"required\": False\n        },\n        \"tool_inputs\": {\n            \"description\": \"Inputs for the tool (required when action='execute')\",\n            \"type\": \"&lt;class 'dict'&gt;\",\n            \"required\": False,\n            \"default\": {}\n        }\n    }\n}\n</code></pre>"},{"location":"MCP/implementation_details_old/#execution-patterns","title":"Execution Patterns","text":""},{"location":"MCP/implementation_details_old/#pattern-1-discovery-first","title":"Pattern 1: Discovery First","text":"<pre><code>sequenceDiagram\n    participant Agent\n    participant MCPTool\n    participant MCPServer\n\n    Agent-&gt;&gt;MCPTool: action='list'\n    activate MCPTool\n    MCPTool-&gt;&gt;MCPServer: Connect &amp; discover\n    MCPServer--&gt;&gt;MCPTool: [search, analyze, summarize]\n    MCPTool--&gt;&gt;Agent: Result([{name: 'search', ...}, ...])\n    deactivate MCPTool\n\n    Note over Agent: Agent sees available tools&lt;br/&gt;and their input schemas\n\n    Agent-&gt;&gt;MCPTool: action='execute'&lt;br/&gt;tool_name='search'&lt;br/&gt;tool_inputs={query: \"ML\"}\n    activate MCPTool\n    MCPTool-&gt;&gt;MCPServer: Execute 'search' tool\n    MCPServer--&gt;&gt;MCPTool: Search results\n    MCPTool--&gt;&gt;Agent: Text(\"Found 3 documents...\")\n    deactivate MCPTool</code></pre>"},{"location":"MCP/implementation_details_old/#pattern-2-direct-execution","title":"Pattern 2: Direct Execution","text":"<pre><code>sequenceDiagram\n    participant Agent\n    participant MCPTool\n    participant MCPServer\n\n    Note over Agent: Agent already knows&lt;br/&gt;tool name from config\n\n    Agent-&gt;&gt;MCPTool: action='execute'&lt;br/&gt;tool_name='analyze'&lt;br/&gt;tool_inputs={text: \"...\"}\n    activate MCPTool\n    MCPTool-&gt;&gt;MCPTool: Initialize if needed\n    MCPTool-&gt;&gt;MCPServer: Execute 'analyze' tool\n    MCPServer--&gt;&gt;MCPTool: Analysis result\n    MCPTool--&gt;&gt;Agent: Result([{sentiment: 0.8, ...}])\n    deactivate MCPTool</code></pre>"},{"location":"MCP/implementation_details_old/#parameter-flow-from-configuration-to-execution","title":"Parameter Flow: From Configuration to Execution","text":"<pre><code>flowchart LR\n    subgraph \"1. Configuration\"\n        JSON[mcp.json&lt;br/&gt;server_script_path&lt;br/&gt;url, headers, etc.]\n    end\n\n    subgraph \"2. Tool Creation\"\n        Loader[mcp_loader.py&lt;br/&gt;Creates MCPTool class]\n        Instance[MCPTool instance&lt;br/&gt;with config params]\n    end\n\n    subgraph \"3. Tree Registration\"\n        Discovery[Tool Discovery&lt;br/&gt;discover_tools_from_module]\n        TreeAdd[tree.add_tool&lt;br/&gt;MCPTool instance]\n    end\n\n    subgraph \"4. Agent Decision\"\n        Schema[DecisionPrompt&lt;br/&gt;available_actions with inputs]\n        AgentChoice[Agent selects tool&lt;br/&gt;+ provides inputs]\n    end\n\n    subgraph \"5. Execution\"\n        Call[MCPTool.__call__&lt;br/&gt;action, tool_name, tool_inputs]\n        MCPExec[MCP Server execution]\n    end\n\n    JSON --&gt; Loader\n    Loader --&gt; Instance\n    Instance --&gt; Discovery\n    Discovery --&gt; TreeAdd\n    TreeAdd --&gt; Schema\n    Schema --&gt; AgentChoice\n    AgentChoice --&gt; Call\n    Call --&gt; MCPExec\n\n    style JSON fill:#e1f5ff\n    style Schema fill:#fff4e6\n    style MCPExec fill:#e8f5e9</code></pre>"},{"location":"MCP/implementation_details_old/#two-phase-parameter-surfacing","title":"Two-Phase Parameter Surfacing","text":""},{"location":"MCP/implementation_details_old/#phase-1-mcptool-parameters-elysia-level","title":"Phase 1: MCPTool Parameters (Elysia Level)","text":"<p>These are always visible to the agent in <code>available_actions</code>:</p> <pre><code>## Two-Phase Parameter Surfacing\n\n### Phase 1: MCPTool Parameters (Elysia Level)\n\nThese are **always** visible to the agent in `available_actions`:\n\n```python\n{\n    \"action\": \"list\" | \"execute\",      # What to do with the MCP server\n    \"tool_name\": \"optional_string\",    # Which MCP tool to run (if execute)\n    \"tool_inputs\": {\"key\": \"value\"}    # Inputs for that specific tool\n}\n</code></pre>"},{"location":"MCP/implementation_details_old/#phase-2-mcp-tool-parameters-mcp-server-level","title":"Phase 2: MCP Tool Parameters (MCP Server Level)","text":"<p>These are discovered dynamically when <code>action='list'</code>:</p> <pre><code># Agent calls: MCPTool(action='list')\n# Returns:\n[\n    {\n        \"name\": \"search\",\n        \"description\": \"Search documents\",\n        \"inputs\": {\n            \"query\": \"string\",\n            \"limit\": \"int\"\n        }\n    },\n    {\n        \"name\": \"analyze\",\n        \"description\": \"Analyze sentiment\",\n        \"inputs\": {\n            \"text\": \"string\"\n        }\n    }\n]\n</code></pre> <p>Then agent uses this info to call: <pre><code>MCPTool(\n    action='execute',\n    tool_name='search',\n    tool_inputs={'query': 'ML papers', 'limit': 10}\n)\n</code></pre></p>"},{"location":"MCP/implementation_details_old/#real-world-execution-example","title":"Real-World Execution Example","text":""},{"location":"MCP/implementation_details_old/#complete-flow-with-parameter-surfacing","title":"Complete Flow with Parameter Surfacing","text":"<pre><code>sequenceDiagram\n    autonumber\n    participant U as User\n    participant A as Agent/Tree\n    participant D as DecisionPrompt\n    participant M as MCPTool\n    participant L as LangChain\n    participant S as MCP Server\n\n    U-&gt;&gt;A: \"Find ML papers and analyze sentiment\"\n\n    Note over A,D: Decision Phase\n    A-&gt;&gt;D: Build DecisionPrompt with available_actions\n    D-&gt;&gt;D: available_actions includes:&lt;br/&gt;{name: \"mcp_api_ai_mcp\"&lt;br/&gt;inputs: {action, tool_name, tool_inputs}}\n    D-&gt;&gt;A: Select \"mcp_api_ai_mcp\" + inputs\n\n    Note over A,M: Discovery Phase\n    A-&gt;&gt;M: __call__(action='list')\n    M-&gt;&gt;M: initialize() if needed\n    M-&gt;&gt;L: Connect to MCP server\n    L-&gt;&gt;S: Discover available tools\n    S--&gt;&gt;L: [search_tool, analyze_tool, ...]\n    L--&gt;&gt;M: LangChain tool objects with schemas\n    M--&gt;&gt;A: Result([{name: \"search\", inputs: {...}}, ...])\n\n    Note over A: Agent now knows MCP tools&lt;br/&gt;and their schemas\n\n    Note over A,M: Execution Phase 1: Search\n    A-&gt;&gt;M: __call__(action='execute',&lt;br/&gt;tool_name='search',&lt;br/&gt;tool_inputs={query: \"ML papers\"})\n    M-&gt;&gt;L: langchain_tool.ainvoke({query: \"ML papers\"})\n    L-&gt;&gt;S: Execute search_tool\n    S--&gt;&gt;L: [\"Paper 1\", \"Paper 2\", ...]\n    L--&gt;&gt;M: Result data\n    M--&gt;&gt;A: Text(\"Found 3 ML papers...\")\n\n    Note over A,M: Execution Phase 2: Analyze\n    A-&gt;&gt;M: __call__(action='execute',&lt;br/&gt;tool_name='analyze',&lt;br/&gt;tool_inputs={text: \"Paper 1 content\"})\n    M-&gt;&gt;L: langchain_tool.ainvoke({text: \"...\"})\n    L-&gt;&gt;S: Execute analyze_tool\n    S--&gt;&gt;L: {sentiment: 0.85, ...}\n    L--&gt;&gt;M: Analysis result\n    M--&gt;&gt;A: Result([{sentiment: 0.85}])\n\n    A--&gt;&gt;U: \"Found 3 papers. Sentiment analysis shows positive...\"</code></pre>"},{"location":"MCP/implementation_details_old/#architecture-comparison","title":"Architecture Comparison","text":""},{"location":"MCP/implementation_details_old/#design-choice-why-gateway-pattern","title":"Design Choice: Why Gateway Pattern?","text":"<pre><code>graph TB\n    subgraph \"\u274c Alternative: Individual Tool Wrapping (NOT USED)\"\n        A1[Tree] --&gt; B1[MCPSearchTool]\n        A1 --&gt; B2[MCPAnalyzeTool]\n        A1 --&gt; B3[MCPSummarizeTool]\n        B1 --&gt; C1[MCP Server 1]\n        B2 --&gt; C1\n        B3 --&gt; C1\n\n        note1[Issues:&lt;br/&gt;- N separate tool classes&lt;br/&gt;- Duplicate connection logic&lt;br/&gt;- No dynamic discovery&lt;br/&gt;- Code regeneration needed]\n    end\n\n    subgraph \"\u2705 Implemented: Gateway Pattern\"\n        A2[Tree] --&gt; B4[MCPTool Gateway]\n        B4 --&gt; |action='list'| C2[Discover Tools]\n        B4 --&gt; |action='execute'&lt;br/&gt;tool_name='search'| C2\n        B4 --&gt; |action='execute'&lt;br/&gt;tool_name='analyze'| C2\n        C2 --&gt; D2[MCP Server]\n\n        note2[Benefits:&lt;br/&gt;+ One tool per server&lt;br/&gt;+ Single connection&lt;br/&gt;+ Dynamic discovery&lt;br/&gt;+ No code generation]\n    end\n\n    style note1 fill:#ffcccc,stroke:#cc0000\n    style note2 fill:#ccffcc,stroke:#00cc00</code></pre>"},{"location":"MCP/implementation_details_old/#1-elysiautiltool_discoverypy","title":"1. <code>elysia/util/tool_discovery.py</code>","text":"<p>Function Modified: <code>discover_tools_from_module()</code></p> <p>Changes: - Added import of <code>elysia.tools.mcp.mcp_loader</code> - Added logic to discover MCP tool classes from the mcp_loader module - Filters for Tool subclasses with names starting with <code>MCP_</code></p> <p>Impact: MCP tools are now discovered alongside custom tools</p>"},{"location":"MCP/implementation_details_old/#2-elysiatoolsuidefault_toolspy","title":"2. <code>elysia/tools/ui/default_tools.py</code>","text":"<p>Function Modified: <code>load_default_tools_for_mode()</code></p> <p>Changes: - Added auto-discovery of MCP tools after loading default tools - Automatically adds discovered MCP tools to the root branch - Logs successful and failed MCP tool additions</p> <p>Impact: MCP tools are automatically added to every tree initialization</p>"},{"location":"MCP/implementation_details_old/#3-elysiatreetreepy","title":"3. <code>elysia/tree/tree.py</code>","text":"<p>Method Modified: <code>_load_additional_discovered_tools()</code></p> <p>Changes: - Removed duplicate MCP loading logic - Converted to empty stub with deprecation notice - Kept for backwards compatibility</p> <p>Impact: Eliminates duplicate code; all MCP loading now centralized</p>"},{"location":"MCP/implementation_details_old/#transport-types-support","title":"Transport Types Support","text":""},{"location":"MCP/implementation_details_old/#transport-architecture","title":"Transport Architecture","text":"<pre><code>graph TB\n    subgraph \"MCPTool Configuration\"\n        Config[MCPTool Instance]\n        TransportType{transport_type}\n    end\n\n    subgraph \"Stdio Transport (Local)\"\n        Stdio[StdioServerParameters]\n        StdioClient[stdio_client]\n        LocalScript[Local Python Script&lt;br/&gt;server.py]\n\n        StdioConfig[Configuration:&lt;br/&gt;- server_script_path: str&lt;br/&gt;- command: 'python']\n    end\n\n    subgraph \"SSE Transport (Remote)\"\n        SSE[SSE Configuration]\n        SSEClient[sse_client]\n        RemoteServer[Remote HTTP Server&lt;br/&gt;http://host:port/mcp]\n\n        SSEConfig[Configuration:&lt;br/&gt;- url: str&lt;br/&gt;- headers: dict]\n    end\n\n    subgraph \"Common MCP Layer\"\n        Session[ClientSession]\n        Protocol[MCP Protocol]\n        Tools[load_mcp_tools]\n    end\n\n    Config --&gt; TransportType\n    TransportType --&gt;|type='stdio'| Stdio\n    TransportType --&gt;|type='sse'| SSE\n\n    Stdio --&gt; StdioConfig\n    StdioConfig --&gt; StdioClient\n    StdioClient --&gt; LocalScript\n    LocalScript --&gt; Session\n\n    SSE --&gt; SSEConfig\n    SSEConfig --&gt; SSEClient\n    SSEClient --&gt; RemoteServer\n    RemoteServer --&gt; Session\n\n    Session --&gt; Protocol\n    Protocol --&gt; Tools\n\n    style Stdio fill:#e8f5e9,stroke:#4CAF50\n    style SSE fill:#e3f2fd,stroke:#2196F3\n    style Session fill:#fff9c4,stroke:#FBC02D</code></pre>"},{"location":"MCP/implementation_details_old/#stdio-transport-local-mcp-server","title":"Stdio Transport (Local MCP Server)","text":"<pre><code>tool = MCPTool(\n    server_name=\"my_server\",\n    transport_type=\"stdio\",\n    server_script_path=\"/path/to/server.py\"\n)\n</code></pre> <p>Use Case: Local development, custom scripts, file system tools</p>"},{"location":"MCP/implementation_details_old/#sse-transport-remote-mcp-server","title":"SSE Transport (Remote MCP Server)","text":"<pre><code>tool = MCPTool(\n    server_name=\"api_server\",\n    transport_type=\"sse\",\n    url=\"http://localhost:8080/mcp\",\n    headers={\"Authorization\": \"Bearer token\"}\n)\n</code></pre> <p>Use Case: API integrations, cloud services, remote tools</p>"},{"location":"MCP/implementation_details_old/#configuration-schema","title":"Configuration Schema","text":""},{"location":"MCP/implementation_details_old/#stdio-transport-configuration","title":"Stdio Transport Configuration","text":"<pre><code>{\n  \"name\": \"server_name\",\n  \"description\": \"Server description\",\n  \"type\": \"stdio\",\n  \"server_script_path\": \"/path/to/script.py\",\n  \"enabled\": true\n}\n</code></pre>"},{"location":"MCP/implementation_details_old/#sse-transport-configuration","title":"SSE Transport Configuration","text":"<pre><code>{\n  \"name\": \"server_name\",\n  \"description\": \"Server description\",\n  \"type\": \"sse\",\n  \"url\": \"http://host:port/path\",\n  \"headers\": {\n    \"Authorization\": \"Bearer token\",\n    \"Custom-Header\": \"value\"\n  },\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"token_id\",\n      \"description\": \"Token description\",\n      \"password\": true\n    }\n  ],\n  \"enabled\": true\n}\n</code></pre>"},{"location":"MCP/implementation_details_old/#testing-checklist","title":"Testing Checklist","text":"<ul> <li>[x] MCP tools are discovered by <code>discover_tools_from_module()</code></li> <li>[x] MCP tools are added to tree during initialization</li> <li>[x] MCP tools appear in <code>tree.tools</code> dictionary</li> <li>[x] MCP tools appear in <code>tree.decision_nodes[root].options</code></li> <li>[x] MCP tools appear in <code>tree.tree</code> structure (what frontend sees)</li> <li>[x] Tool deduplication (<code>tools.py</code> uses <code>tool_discovery.py</code>)</li> <li>[x] Stdio transport support (local MCP servers)</li> <li>[x] SSE transport support (remote MCP servers)</li> <li>[x] Backwards compatibility maintained</li> </ul>"},{"location":"MCP/implementation_details_old/#summary-how-mcptool-works","title":"Summary: How MCPTool Works","text":""},{"location":"MCP/implementation_details_old/#key-points","title":"Key Points","text":"<ol> <li>One Tool Per Server: Each <code>MCPTool</code> instance = one MCP server gateway</li> <li>Two-Phase Operation: </li> <li><code>action='list'</code>: Discover available tools</li> <li><code>action='execute'</code>: Run a specific tool</li> <li>Dynamic Discovery: Tools are discovered at runtime, no code generation</li> <li>Gateway Pattern: Agent interacts with ONE tool that proxies to many MCP tools</li> <li>Parameter Surfacing: </li> <li>Elysia Level: action, tool_name, tool_inputs (always visible)</li> <li>MCP Level: Each tool's specific parameters (discovered dynamically)</li> </ol>"},{"location":"MCP/implementation_details_old/#agents-view","title":"Agent's View","text":"<pre><code># The agent sees this in available_actions:\n{\n    \"name\": \"mcp_api_ai_mcp\",\n    \"description\": \"MCP server providing multiple tools\",\n    \"inputs\": {\n        \"action\": \"list or execute\",\n        \"tool_name\": \"string (optional)\",\n        \"tool_inputs\": \"dict (optional)\"\n    }\n}\n\n# Agent can:\n# 1. List tools: MCPTool(action='list')\n# 2. Execute tool: MCPTool(action='execute', tool_name='search', tool_inputs={...})\n</code></pre>"},{"location":"MCP/implementation_details_old/#developers-view","title":"Developer's View","text":"<pre><code># Configuration (mcp.json)\n{\n    \"name\": \"my_server\",\n    \"type\": \"stdio\",\n    \"server_script_path\": \"/path/to/server.py\"\n}\n\n# Results in MCPTool class creation\nclass MCP_my_server(MCPTool):\n    def __init__(self):\n        super().__init__(\n            server_name=\"my_server\",\n            transport_type=\"stdio\",\n            server_script_path=\"/path/to/server.py\"\n        )\n\n# Automatically discovered and added to tree\n# Agent can now use it without any manual registration\n</code></pre>"},{"location":"MCP/implementation_details_old/#execution-flow-summary","title":"Execution Flow Summary","text":"<pre><code>graph LR\n    A[1. Configuration&lt;br/&gt;mcp.json] --&gt; B[2. MCPTool Class&lt;br/&gt;Auto-generated]\n    B --&gt; C[3. Tool Discovery&lt;br/&gt;Auto-loaded to tree]\n    C --&gt; D[4. Agent Decision&lt;br/&gt;Selects MCPTool]\n    D --&gt; E[5. Initialize&lt;br/&gt;Connect to MCP server]\n    E --&gt; F[6. Action: list/execute&lt;br/&gt;Discover or run tools]\n    F --&gt; G[7. Result&lt;br/&gt;Return to agent]\n\n    style A fill:#e1f5ff\n    style D fill:#fff4e6\n    style F fill:#e8f5e9\n    style G fill:#f3e5f5</code></pre>"},{"location":"MCP/implementation_details_old/#key-benefits","title":"Key Benefits","text":"<ol> <li>Automatic Discovery: MCP tools are automatically discovered and added to trees</li> <li>No Manual Configuration: No need to manually add MCP tools to tree branches</li> <li>Consistent Behavior: All tree initialization modes get MCP tools automatically</li> <li>Frontend Visibility: MCP tools now visible in UI for user selection</li> <li>Centralized Logic: All tool loading logic in one place (<code>default_tools.py</code>)</li> <li>Multiple Transports: Support for both local (stdio) and remote (SSE) MCP servers</li> <li>Type Safety: Strong typing with Literal types for transport validation</li> <li>Extensibility: Easy to add new transport types</li> <li>Gateway Pattern: One tool per server, not N tools per server</li> <li>Dynamic Capabilities: Tools discovered at runtime, adapts to server changes</li> </ol>"},{"location":"MCP/implementation_details_old/#future-enhancements","title":"Future Enhancements","text":"<p>Potential improvements for future consideration:</p> <ol> <li>Selective Loading: Allow configuration to specify which MCP tools to load</li> <li>Branch Placement: Allow MCP tools to be added to specific branches, not just root</li> <li>Tool Ordering: Control the order in which MCP tools appear in the tree</li> <li>Dynamic Reloading: Hot-reload MCP tools when <code>mcp.json</code> changes</li> <li>Tool Metadata: Extract and display MCP tool capabilities in UI</li> <li>Health Checks: Monitor MCP server availability</li> <li>Failover: Support fallback servers for high availability</li> </ol> <p>Implementation Status: Complete and Production-Ready \u2705</p>"},{"location":"MCP/interaction_model/","title":"MCP Tool Interaction Model","text":""},{"location":"MCP/interaction_model/#overview","title":"Overview","text":"<p>This document explains how MCPTool works, how parameters are surfaced, and the design decision to use MCP servers as gateway tools rather than individual tool wrappers.</p>"},{"location":"MCP/interaction_model/#core-design-gateway-pattern","title":"Core Design: Gateway Pattern","text":""},{"location":"MCP/interaction_model/#one-mcptool-one-mcp-server","title":"One MCPTool = One MCP Server","text":"<p>Key Concept: Each <code>MCPTool</code> instance represents one MCP server that provides access to multiple tools.</p> <pre><code>graph LR\n    A[Elysia Agent] --&gt; B[MCPTool Gateway&lt;br/&gt;'mcp_api_ai_mcp']\n    B --&gt; C[MCP Server]\n    C --&gt; D[Tool 1: search]\n    C --&gt; E[Tool 2: analyze]\n    C --&gt; F[Tool 3: summarize]\n\n    style B fill:#4A90E2,stroke:#2E5C8A,color:#fff\n    style C fill:#E74C3C,stroke:#C0392B,color:#fff</code></pre> <p>NOT: Each MCP server tool as separate Elysia Tool \u274c YES: One Elysia Tool per MCP server \u2705</p>"},{"location":"MCP/interaction_model/#why-gateway-pattern","title":"Why Gateway Pattern?","text":""},{"location":"MCP/interaction_model/#rejected-alternative-individual-tool-wrapping","title":"Rejected Alternative: Individual Tool Wrapping","text":"<pre><code># \u274c This approach was NOT implemented\nclass MCPSearchTool(Tool):\n    \"\"\"Separate Elysia tool for MCP search\"\"\"\n    pass\n\nclass MCPAnalyzeTool(Tool):\n    \"\"\"Separate Elysia tool for MCP analyze\"\"\"\n    pass\n\ntree.add_tool(MCPSearchTool())   # Need separate class for each\ntree.add_tool(MCPAnalyzeTool())  # Need separate class for each\n# ... repeat for every MCP tool\n</code></pre> <p>Problems with this approach: 1. Need to create N Elysia tool classes for N MCP tools 2. Code generation required when MCP tools change 3. Duplicate connection/initialization logic for each tool 4. No dynamic discovery - tools hardcoded at build time 5. Changes to MCP server require code regeneration</p>"},{"location":"MCP/interaction_model/#implemented-gateway-pattern","title":"Implemented: Gateway Pattern","text":"<pre><code># \u2705 Implemented approach\nclass MCPTool(Tool):\n    \"\"\"Single gateway to all tools in an MCP server\"\"\"\n\n    def __call__(self, inputs):\n        action = inputs['action']\n\n        if action == 'list':\n            # Discover available tools\n            return list_of_tools_with_schemas\n\n        elif action == 'execute':\n            # Execute specific tool\n            tool_name = inputs['tool_name']\n            tool_inputs = inputs['tool_inputs']\n            return execute_tool(tool_name, tool_inputs)\n\n# One tool represents entire MCP server\ntree.add_tool(MCPTool(server_name=\"api_ai_mcp\"))\n</code></pre> <p>Advantages: 1. \u2705 Single tool per MCP server (not N tools) 2. \u2705 Dynamic tool discovery at runtime 3. \u2705 No code generation needed 4. \u2705 Tools can change without code updates 5. \u2705 Unified connection management 6. \u2705 Agent can query capabilities before use</p>"},{"location":"MCP/interaction_model/#two-phase-operation","title":"Two-Phase Operation","text":""},{"location":"MCP/interaction_model/#phase-1-discovery-actionlist","title":"Phase 1: Discovery (action='list')","text":"<p>Agent discovers what tools are available:</p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MCPTool\n    participant MCPServer\n\n    Agent-&gt;&gt;MCPTool: action='list'\n    activate MCPTool\n    MCPTool-&gt;&gt;MCPServer: Connect &amp; discover tools\n    MCPServer--&gt;&gt;MCPTool: [search, analyze, summarize]\n    MCPTool--&gt;&gt;Agent: Result([&lt;br/&gt;  {name: 'search', inputs: {...}},&lt;br/&gt;  {name: 'analyze', inputs: {...}},&lt;br/&gt;  {name: 'summarize', inputs: {...}}&lt;br/&gt;])\n    deactivate MCPTool</code></pre> <p>Agent now knows: - Available tool names - Each tool's description - Each tool's input schema</p>"},{"location":"MCP/interaction_model/#phase-2-execution-actionexecute","title":"Phase 2: Execution (action='execute')","text":"<p>Agent executes a specific tool:</p> <pre><code>sequenceDiagram\n    participant Agent\n    participant MCPTool\n    participant MCPServer\n\n    Agent-&gt;&gt;MCPTool: action='execute'&lt;br/&gt;tool_name='search'&lt;br/&gt;tool_inputs={query: \"ML\"}\n    activate MCPTool\n    MCPTool-&gt;&gt;MCPServer: Execute 'search' with inputs\n    MCPServer--&gt;&gt;MCPTool: Search results\n    MCPTool--&gt;&gt;Agent: Result/Text/Error\n    deactivate MCPTool</code></pre>"},{"location":"MCP/interaction_model/#parameter-surfacing-model","title":"Parameter Surfacing Model","text":""},{"location":"MCP/interaction_model/#level-1-mcptool-parameters-always-visible","title":"Level 1: MCPTool Parameters (Always Visible)","text":"<p>These are the Elysia-level parameters that the agent always sees:</p> <pre><code>{\n    \"name\": \"mcp_api_ai_mcp\",\n    \"description\": \"MCP server 'api-ai-mcp' (stdio) - provides access to multiple tools\",\n    \"inputs\": {\n        \"action\": {\n            \"description\": \"Action: 'list' to show tools, 'execute' to run a specific tool\",\n            \"type\": \"str\",\n            \"default\": \"list\"\n        },\n        \"tool_name\": {\n            \"description\": \"Name of the tool to execute (required when action='execute')\",\n            \"type\": \"str\",\n            \"required\": False\n        },\n        \"tool_inputs\": {\n            \"description\": \"Inputs for the tool (required when action='execute')\",\n            \"type\": \"dict\",\n            \"required\": False,\n            \"default\": {}\n        }\n    }\n}\n</code></pre> <p>These parameters control the gateway behavior itself.</p>"},{"location":"MCP/interaction_model/#level-2-mcp-tool-parameters-discovered-dynamically","title":"Level 2: MCP Tool Parameters (Discovered Dynamically)","text":"<p>These are the MCP-level parameters discovered when <code>action='list'</code>:</p> <pre><code># After agent calls MCPTool(action='list'), it receives:\n[\n    {\n        \"name\": \"search\",\n        \"description\": \"Search for documents\",\n        \"inputs\": {\n            \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n            \"limit\": {\"type\": \"int\", \"default\": 10}\n        }\n    },\n    {\n        \"name\": \"analyze\",\n        \"description\": \"Analyze text sentiment\",\n        \"inputs\": {\n            \"text\": {\"type\": \"string\", \"description\": \"Text to analyze\"}\n        }\n    }\n]\n</code></pre> <p>These parameters are specific to each MCP tool.</p>"},{"location":"MCP/interaction_model/#complete-parameter-flow","title":"Complete Parameter Flow","text":"<pre><code>flowchart TB\n    Start[Agent Decision] --&gt; CheckAction{What does&lt;br/&gt;agent want?}\n\n    CheckAction --&gt;|Discover tools| List[action='list']\n    CheckAction --&gt;|Use a tool| Execute[action='execute']\n\n    List --&gt; ListCall[MCPTool inputs:&lt;br/&gt;action='list']\n    ListCall --&gt; ListResult[Returns tool list&lt;br/&gt;with schemas]\n    ListResult --&gt; AgentKnows[Agent now knows:&lt;br/&gt;- Tool names&lt;br/&gt;- Tool descriptions&lt;br/&gt;- Tool parameters]\n\n    Execute --&gt; ExecCall[MCPTool inputs:&lt;br/&gt;action='execute'&lt;br/&gt;tool_name='search'&lt;br/&gt;tool_inputs={query: 'ML'}]\n    ExecCall --&gt; ExecResult[Returns tool result]\n\n    style List fill:#e3f2fd,stroke:#2196F3\n    style Execute fill:#e8f5e9,stroke:#4CAF50\n    style AgentKnows fill:#fff9c4,stroke:#FBC02D</code></pre>"},{"location":"MCP/interaction_model/#how-agent-interacts-with-mcptool","title":"How Agent Interacts with MCPTool","text":""},{"location":"MCP/interaction_model/#scenario-1-agent-discovers-then-uses","title":"Scenario 1: Agent Discovers Then Uses","text":"<pre><code># Step 1: Agent discovers available tools\nagent_decision = DecisionNode(\n    function_name=\"mcp_api_ai_mcp\",\n    function_inputs={\"action\": \"list\"}\n)\n# Result: List of tools with schemas\n\n# Step 2: Agent sees 'search' tool is available with 'query' parameter\nagent_decision = DecisionNode(\n    function_name=\"mcp_api_ai_mcp\",\n    function_inputs={\n        \"action\": \"execute\",\n        \"tool_name\": \"search\",\n        \"tool_inputs\": {\"query\": \"machine learning\"}\n    }\n)\n# Result: Search results\n</code></pre>"},{"location":"MCP/interaction_model/#scenario-2-agent-directly-executes-if-known","title":"Scenario 2: Agent Directly Executes (If Known)","text":"<pre><code># Agent already knows tool name from prior experience or config\nagent_decision = DecisionNode(\n    function_name=\"mcp_api_ai_mcp\",\n    function_inputs={\n        \"action\": \"execute\",\n        \"tool_name\": \"analyze\",\n        \"tool_inputs\": {\"text\": \"This is great!\"}\n    }\n)\n# Result: Sentiment analysis\n</code></pre>"},{"location":"MCP/interaction_model/#real-world-example","title":"Real-World Example","text":""},{"location":"MCP/interaction_model/#user-query","title":"User Query","text":"<pre><code>\"Search for machine learning papers and analyze their sentiment\"\n</code></pre>"},{"location":"MCP/interaction_model/#agents-reasoning-process","title":"Agent's Reasoning Process","text":"<pre><code>sequenceDiagram\n    autonumber\n    participant U as User\n    participant A as Agent\n    participant M as MCPTool\n    participant S as MCP Server\n\n    U-&gt;&gt;A: \"Search ML papers and analyze sentiment\"\n\n    Note over A: Decision: Need to use MCP tools\n\n    A-&gt;&gt;M: action='list'\n    M-&gt;&gt;S: Discover tools\n    S--&gt;&gt;M: [search, analyze, ...]\n    M--&gt;&gt;A: Tool list\n\n    Note over A: I see 'search' and 'analyze' tools&lt;br/&gt;I'll use them sequentially\n\n    A-&gt;&gt;M: action='execute'&lt;br/&gt;tool_name='search'&lt;br/&gt;tool_inputs={query: 'ML papers'}\n    M-&gt;&gt;S: Run search tool\n    S--&gt;&gt;M: [\"Paper 1\", \"Paper 2\", ...]\n    M--&gt;&gt;A: Search results\n\n    Note over A: Got papers, now analyze sentiment\n\n    A-&gt;&gt;M: action='execute'&lt;br/&gt;tool_name='analyze'&lt;br/&gt;tool_inputs={text: 'Paper 1...'}\n    M-&gt;&gt;S: Run analyze tool\n    S--&gt;&gt;M: {sentiment: 0.85}\n    M--&gt;&gt;A: Analysis result\n\n    A--&gt;&gt;U: \"Found 3 ML papers.&lt;br/&gt;Sentiment is positive (0.85)\"</code></pre>"},{"location":"MCP/interaction_model/#comparison-gateway-vs-individual-tools","title":"Comparison: Gateway vs Individual Tools","text":""},{"location":"MCP/interaction_model/#gateway-pattern-implemented","title":"Gateway Pattern (Implemented)","text":"<pre><code>graph TB\n    A[Agent] --&gt; B[MCPTool&lt;br/&gt;mcp_api_ai_mcp]\n    B --&gt;|action='list'| C[Discover]\n    B --&gt;|action='execute'&lt;br/&gt;tool_name='search'| D[Execute Search]\n    B --&gt;|action='execute'&lt;br/&gt;tool_name='analyze'| E[Execute Analyze]\n\n    C --&gt; F[MCP Server]\n    D --&gt; F\n    E --&gt; F\n\n    style B fill:#4A90E2,stroke:#2E5C8A,color:#fff\n\n    Note1[Benefits:&lt;br/&gt;\u2713 One tool&lt;br/&gt;\u2713 Dynamic discovery&lt;br/&gt;\u2713 No code gen&lt;br/&gt;\u2713 Flexible]\n\n    style Note1 fill:#ccffcc,stroke:#00cc00</code></pre> <p>Code: <pre><code># Single tool registration\ntree.add_tool(MCPTool(server_name=\"api_ai_mcp\"))\n\n# Agent can discover and use any tool\nresponse = tree(\"Search for ML papers\")  # Uses search tool\nresponse = tree(\"Analyze sentiment\")      # Uses analyze tool\n</code></pre></p>"},{"location":"MCP/interaction_model/#individual-tool-pattern-not-implemented","title":"Individual Tool Pattern (NOT Implemented)","text":"<pre><code>graph TB\n    A[Agent] --&gt; B1[MCPSearchTool]\n    A --&gt; B2[MCPAnalyzeTool]\n    A --&gt; B3[MCPSummarizeTool]\n\n    B1 --&gt; C[MCP Server]\n    B2 --&gt; C\n    B3 --&gt; C\n\n    style B1 fill:#ffcccc,stroke:#cc0000\n    style B2 fill:#ffcccc,stroke:#cc0000\n    style B3 fill:#ffcccc,stroke:#cc0000\n\n    Note1[Issues:&lt;br/&gt;\u2717 N tools&lt;br/&gt;\u2717 Static only&lt;br/&gt;\u2717 Code gen needed&lt;br/&gt;\u2717 Inflexible]\n\n    style Note1 fill:#ffcccc,stroke:#cc0000</code></pre> <p>Code: <pre><code># Would need separate tool classes\ntree.add_tool(MCPSearchTool())\ntree.add_tool(MCPAnalyzeTool())\ntree.add_tool(MCPSummarizeTool())\n# ... and code generation when tools change\n</code></pre></p>"},{"location":"MCP/interaction_model/#key-takeaways","title":"Key Takeaways","text":""},{"location":"MCP/interaction_model/#for-developers","title":"For Developers","text":"<ol> <li>Configuration: Add MCP server to <code>mcp.json</code> \u2192 MCPTool class auto-generated</li> <li>Registration: MCPTool automatically added to tree during initialization</li> <li>No Maintenance: Tools can change in MCP server without code updates</li> </ol>"},{"location":"MCP/interaction_model/#for-users","title":"For Users","text":"<ol> <li>Visibility: MCP tools appear in UI as regular tools</li> <li>Usage: Can select MCP tool and provide action/parameters</li> <li>Discovery: Can list available tools before using them</li> </ol>"},{"location":"MCP/interaction_model/#for-agents","title":"For Agents","text":"<ol> <li>Single Interface: One tool per MCP server with consistent interface</li> <li>Dynamic Capabilities: Can discover tools at runtime</li> <li>Flexible Usage: Can list tools first or execute directly if known</li> </ol>"},{"location":"MCP/interaction_model/#architecture-benefits","title":"Architecture Benefits","text":"Aspect Gateway Pattern Individual Tools Tool Registration 1 per server N per server Code Generation None Required Dynamic Discovery Yes No Maintainability High Low Flexibility High Low Agent Complexity Moderate Low <p>The gateway pattern trades slightly more agent complexity (two-phase operation) for significantly better maintainability, flexibility, and dynamic capabilities.</p> <p>Summary: MCPTool uses the gateway pattern where one Elysia tool represents one MCP server, providing access to multiple tools through a two-phase discover-then-execute model. Parameters are surfaced at two levels: Elysia-level (action, tool_name, tool_inputs) and MCP-level (each tool's specific parameters discovered dynamically).</p>"},{"location":"MCP/mcp_as_agent/","title":"MCP_AS_AGENT Configuration","text":""},{"location":"MCP/mcp_as_agent/#overview","title":"Overview","text":"<p>The <code>MCP_AS_AGENT</code> environment variable controls how MCP (Model Context Protocol) servers are integrated into Elysia.</p> <p>Transport: Connects to running MCP servers via SSE/HTTP.</p>"},{"location":"MCP/mcp_as_agent/#configuration","title":"Configuration","text":"<p>Add to your <code>.env</code> file:</p> <pre><code># True:  Each MCP server becomes ONE agent tool (natural language interface)\n# False: Each tool in the MCP server becomes an individual Elysia Tool\nMCP_AS_AGENT=True\n</code></pre>"},{"location":"MCP/mcp_as_agent/#operating-modes","title":"Operating Modes","text":""},{"location":"MCP/mcp_as_agent/#agent-mode-mcp_as_agenttrue-default","title":"Agent Mode (<code>MCP_AS_AGENT=True</code>) - Default","text":"<p>Behavior: - One <code>MCPTool</code> instance per MCP server - Takes natural language queries as input - Uses ReAct agent with LangChain to execute tasks - Agent has access to all tools in the MCP server - Autonomous decision-making</p> <p>Tool Interface: <pre><code># Input Schema\n{\n    \"query\": \"Natural language query or task description\"\n}\n\n# Example Usage\ntree(\"Use the MCP server to search for machine learning papers\")\n</code></pre></p> <p>Architecture: <pre><code>User Query (Natural Language)\n    \u2193\nMCPTool (ReAct Agent)\n    \u2193\nLangChain Agent Executor\n    \u251c\u2500\u2192 MCP Tool 1\n    \u251c\u2500\u2192 MCP Tool 2\n    \u2514\u2500\u2192 MCP Tool 3\n         \u2193\n    Running MCP Server (HTTP/SSE)\n</code></pre></p>"},{"location":"MCP/mcp_as_agent/#individual-tool-mode-mcp_as_agentfalse","title":"Individual Tool Mode (<code>MCP_AS_AGENT=False</code>)","text":"<p>Behavior: - Each tool in the MCP server becomes a separate Elysia Tool - Direct tool execution (no agent layer) - Explicit tool selection required - Structured inputs per tool</p> <p>Tool Interface: <pre><code># Each MCP tool gets its own Elysia Tool\n# Example: mcp_server_search, mcp_server_analyze, etc.\n\n# Input Schema (specific to each tool)\n{\n    \"param1\": \"value1\",\n    \"param2\": \"value2\"\n}\n\n# Example Usage\ntree.tools[\"mcp_server_search\"](inputs={\"query\": \"ML papers\"})\n</code></pre></p> <p>Architecture: <pre><code>Elysia Decision Tree\n    \u251c\u2500\u2192 mcp_server_tool1 (Individual Elysia Tool)\n    \u251c\u2500\u2192 mcp_server_tool2 (Individual Elysia Tool)\n    \u2514\u2500\u2192 mcp_server_tool3 (Individual Elysia Tool)\n         \u2193\n    Running MCP Server (HTTP/SSE)\n</code></pre></p>"},{"location":"MCP/mcp_as_agent/#comparison","title":"Comparison","text":"Aspect Agent Mode (True) Individual Mode (False) Input Type Natural language Structured parameters Tool Count 1 per server N per server (N = # of MCP tools) Decision Making Autonomous (ReAct agent) Manual (developer/user) Flexibility High (agent decides) Low (explicit calls) Complexity Higher (agent overhead) Lower (direct execution) Use Case Complex tasks, conversational Simple tasks, workflows Dependencies langchain, langchain-openai langchain-mcp-adapters only Transport SSE/HTTP to pre-running servers SSE/HTTP to pre-running servers"},{"location":"MCP/mcp_as_agent/#configuration_1","title":"Configuration","text":""},{"location":"MCP/mcp_as_agent/#mcp-server-configuration-mcpjson","title":"MCP Server Configuration (<code>mcp.json</code>)","text":"<pre><code>{\n  \"research_assistant\": {\n    \"url\": \"http://localhost:8000/sse\",\n    \"headers\": {\n      \"Authorization\": \"Bearer token123\"\n    }\n  },\n  \"data_analyzer\": {\n    \"url\": \"http://localhost:8001/sse\",\n    \"headers\": {}\n  }\n}\n</code></pre> <p>Requirements: - MCP servers must be pre-running (not started by Elysia) - Each server must expose SSE/HTTP endpoint - Configure connection details in <code>mcp.json</code></p>"},{"location":"MCP/mcp_as_agent/#environment-setup","title":"Environment Setup","text":"<pre><code># .env file\nMCP_AS_AGENT=True\nBASE_MODEL=gpt-4\nOPENAI_API_KEY=your_key_here\n</code></pre>"},{"location":"MCP/mcp_as_agent/#implementation-details","title":"Implementation Details","text":""},{"location":"MCP/mcp_as_agent/#agent-mode-implementation","title":"Agent Mode Implementation","text":"<p>MCPTool with Agent: <pre><code>class MCPTool(Tool):\n    def __init__(self, ...):\n        if self.agent_mode:  # MCP_AS_AGENT=True\n            self.inputs = {\n                \"query\": {\n                    \"description\": \"Natural language query\",\n                    \"type\": str,\n                    \"required\": True\n                }\n            }\n\n    async def initialize(self):\n        # Load MCP tools\n        self._langchain_tools = await load_mcp_tools(session)\n\n        if self.agent_mode:\n            # Create ReAct agent\n            from langchain.agents import create_react_agent, AgentExecutor\n            llm = ChatOpenAI(...)\n            agent = create_react_agent(llm, self._langchain_tools, prompt)\n            self._agent = AgentExecutor(agent=agent, tools=self._langchain_tools)\n\n    async def __call__(self, inputs, ...):\n        if self.agent_mode:\n            # Execute with agent\n            result = await self._agent.ainvoke({\"input\": inputs[\"query\"]})\n            yield Text(result[\"output\"])\n</code></pre></p>"},{"location":"MCP/mcp_as_agent/#individual-mode-implementation","title":"Individual Mode Implementation","text":"<p>Individual Tool Wrappers: <pre><code>class IndividualMCPToolWrapper(Tool):\n    \"\"\"Wrapper for a single MCP server tool.\"\"\"\n\n    def __init__(self, tool_name, ...):\n        # Extract input schema from MCP tool\n        super().__init__(\n            name=f\"mcp_{server_name}_{tool_name}\",\n            inputs={...},  # Tool-specific inputs\n        )\n\n    async def __call__(self, inputs, ...):\n        # Connect to MCP server\n        # Execute specific tool\n        result = await self._langchain_tool.ainvoke(inputs)\n        yield Result(result)\n</code></pre></p> <p>Dynamic Tool Discovery: <pre><code>def _create_individual_tool_wrappers(server_config):\n    # Initialize temporary MCP connection\n    temp_tool = MCPTool(...)\n    await temp_tool.initialize()\n\n    # Create wrapper for each discovered tool\n    tool_classes = []\n    for lc_tool in temp_tool._langchain_tools:\n        wrapper = _create_individual_tool_class(\n            tool_name=lc_tool.name,\n            tool_description=lc_tool.description,\n            ...\n        )\n        tool_classes.append(wrapper)\n\n    return tool_classes\n</code></pre></p>"},{"location":"MCP/mcp_as_agent/#usage-examples","title":"Usage Examples","text":""},{"location":"MCP/mcp_as_agent/#agent-mode-usage","title":"Agent Mode Usage","text":"<pre><code>from elysia.tree import Tree\nfrom elysia.tools.mcp.mcp_loader import load_mcp_servers_from_config\nimport os\n\nos.environ[\"MCP_AS_AGENT\"] = \"True\"\n\ntools = load_mcp_servers_from_config()\ntree = Tree(tools=tools)\n\n# Natural language query\nresponse = tree(\"Search for recent AI papers and analyze their sentiment\")\n\n# Agent will:\n# 1. Decide to use 'search' tool\n# 2. Execute search with appropriate parameters\n# 3. Analyze results with 'analyze' tool\n# 4. Return combined response\n</code></pre>"},{"location":"MCP/mcp_as_agent/#individual-mode-usage","title":"Individual Mode Usage","text":"<pre><code>from elysia.tree import Tree\nfrom elysia.tools.mcp.mcp_loader import load_mcp_servers_from_config\nimport os\n\nos.environ[\"MCP_AS_AGENT\"] = \"False\"\n\ntools = load_mcp_servers_from_config()\ntree = Tree(tools=tools)\n\n# Direct tool invocation\nsearch_results = tree.tools[\"api_ai_mcp_search\"](\n    inputs={\"query\": \"AI papers\", \"limit\": 10}\n)\n\nanalysis_results = tree.tools[\"api_ai_mcp_analyze\"](\n    inputs={\"text\": search_results}\n)\n</code></pre>"},{"location":"MCP/mcp_as_agent/#migration-guide","title":"Migration Guide","text":""},{"location":"MCP/mcp_as_agent/#from-individual-to-agent-mode","title":"From Individual to Agent Mode","text":"<p>Before (Individual Mode): <pre><code># Explicit tool calls\ntree.tools[\"mcp_server_tool1\"](inputs={...})\ntree.tools[\"mcp_server_tool2\"](inputs={...})\n</code></pre></p> <p>After (Agent Mode): <pre><code># Natural language query\ntree(\"Use the tools to accomplish task X\")\n</code></pre></p>"},{"location":"MCP/mcp_as_agent/#from-agent-to-individual-mode","title":"From Agent to Individual Mode","text":"<p>Before (Agent Mode): <pre><code>tree(\"Search and analyze data\")\n</code></pre></p> <p>After (Individual Mode): <pre><code># Manual orchestration\nresults = tree.tools[\"mcp_server_search\"](inputs={\"query\": \"data\"})\nanalysis = tree.tools[\"mcp_server_analyze\"](inputs={\"data\": results})\n</code></pre></p>"},{"location":"MCP/mcp_as_agent/#troubleshooting","title":"Troubleshooting","text":""},{"location":"MCP/mcp_as_agent/#agent-mode-issues","title":"Agent Mode Issues","text":"<p>Problem: Agent not making decisions correctly - Solution: Adjust model (set BASE_MODEL env var to gpt-4 or better) - Solution: Improve tool descriptions in MCP server</p> <p>Problem: <code>ImportError: No module named 'langchain'</code> - Solution: <code>pip install langchain langchain-openai</code></p> <p>Problem: Cannot connect to MCP server - Solution: Ensure MCP server is running at configured URL - Solution: Check <code>mcp.json</code> has correct URL and headers - Solution: Verify network connectivity</p>"},{"location":"MCP/mcp_as_agent/#individual-mode-issues","title":"Individual Mode Issues","text":"<p>Problem: Tools not discovered - Solution: Check MCP server is running and accessible - Solution: Verify mcp.json configuration (URL, headers) - Solution: Check logs for initialization errors</p> <p>Problem: Tool inputs don't match schema - Solution: Inspect tool schema: <code>tree.tools[\"server_tool\"].inputs</code> - Solution: Ensure MCP tool has proper input schema definition</p> <p>Problem: Connection timeout - Solution: Increase timeout in MCPTool configuration - Solution: Verify server is responsive (test with curl)</p>"},{"location":"MCP/mcp_as_agent/#best-practices","title":"Best Practices","text":""},{"location":"MCP/mcp_as_agent/#when-to-use-agent-mode","title":"When to Use Agent Mode","text":"<p>\u2705 Complex, multi-step tasks \u2705 Conversational interfaces \u2705 When tool selection logic is unclear \u2705 Autonomous task execution \u2705 Rapid prototyping  </p>"},{"location":"MCP/mcp_as_agent/#when-to-use-individual-mode","title":"When to Use Individual Mode","text":"<p>\u2705 Predictable workflows \u2705 Performance-critical applications \u2705 Fine-grained error handling \u2705 Explicit control requirements \u2705 Debugging MCP tools  </p>"},{"location":"MCP/mcp_as_agent/#server-management","title":"Server Management","text":"<ul> <li>Always start MCP servers before initializing Elysia</li> <li>Use process managers (systemd, PM2) for production</li> <li>Monitor server health and connectivity</li> <li>Configure appropriate timeouts for long-running operations</li> </ul>"},{"location":"MCP/mcp_as_agent/#performance-considerations","title":"Performance Considerations","text":""},{"location":"MCP/mcp_as_agent/#agent-mode","title":"Agent Mode","text":"<ul> <li>Latency: Higher (agent reasoning + tool execution)</li> <li>Cost: Higher (LLM calls for agent reasoning)</li> <li>Reliability: Depends on agent model quality</li> </ul>"},{"location":"MCP/mcp_as_agent/#individual-mode","title":"Individual Mode","text":"<ul> <li>Latency: Lower (direct tool execution)</li> <li>Cost: Lower (no agent LLM calls)</li> <li>Reliability: Higher (deterministic execution)</li> </ul>"},{"location":"MCP/mcp_as_agent/#dependencies","title":"Dependencies","text":""},{"location":"MCP/mcp_as_agent/#agent-mode-requirements","title":"Agent Mode Requirements","text":"<pre><code>pip install langchain-mcp-adapters langchain langchain-openai\n</code></pre>"},{"location":"MCP/mcp_as_agent/#individual-mode-requirements","title":"Individual Mode Requirements","text":"<pre><code>pip install langchain-mcp-adapters\n</code></pre>"},{"location":"MCP/mcp_as_agent/#summary","title":"Summary","text":"<ul> <li><code>MCP_AS_AGENT=True</code>: AI agent interface, natural language, autonomous</li> <li><code>MCP_AS_AGENT=False</code>: Direct tool access, structured inputs, manual control</li> <li>Default is <code>True</code> for ease of use</li> <li>Can switch modes without changing mcp.json</li> <li>Both modes use same MCP server configuration</li> </ul> <p>Choose Agent Mode for: Flexibility, natural language, autonomous execution Choose Individual Mode for: Control, performance, predictable workflows</p>"},{"location":"MCP/overview/","title":"MCP Integration - Complete Summary","text":""},{"location":"MCP/overview/#what-was-built","title":"What Was Built","text":"<p>Minimal, production-ready MCP server integration for Elysia following CODING_INSTRUCTIONS.md principles.</p>"},{"location":"MCP/overview/#core-implementation-300-lines","title":"Core Implementation (300 lines)","text":"<ol> <li><code>mcp_adapter.py</code> (176 lines) - MCPServerAdapter class</li> <li><code>mcp_tool_wrapper.py</code> (128 lines) - MCPToolWrapper class</li> <li><code>__init__.py</code> (3 lines) - Exports</li> </ol>"},{"location":"MCP/overview/#documentation-2-files","title":"Documentation (2 files)","text":"<ol> <li>ARCHITECTURE.md - Complete architecture, implementation details, usage</li> <li>QUICKSTART.md - 30-second setup guide</li> </ol>"},{"location":"MCP/overview/#examples","title":"Examples","text":"<ol> <li><code>examples/mcp_integration_example.py</code> - Working examples</li> </ol>"},{"location":"MCP/overview/#installation","title":"Installation","text":"<pre><code>pip install langchain-mcp-adapters\n</code></pre>"},{"location":"MCP/overview/#usage","title":"Usage","text":"<pre><code>from elysia import Tree\nfrom elysia.tools.additional import MCPServerAdapter\n\ntree = Tree()\nadapter = MCPServerAdapter(server_script_path=\"/path/to/server.py\")\nawait adapter.initialize_and_register_tools(tree, branch_id=\"base\")\n\n# MCP tools now available\nresponse, objects = tree(\"Your query here\")\n</code></pre>"},{"location":"MCP/overview/#refactoring-journey","title":"Refactoring Journey","text":""},{"location":"MCP/overview/#before-initial-implementation","title":"Before (Initial Implementation)","text":"<ul> <li>2,000+ lines of overengineered code</li> <li>Mock MCP client implementations (250 lines)</li> <li>Factory functions (unnecessary abstraction)</li> <li>HTTP client (premature feature)</li> <li>Multiple abstraction layers</li> </ul>"},{"location":"MCP/overview/#after-refactored-following-coding_instructions","title":"After (Refactored Following CODING_INSTRUCTIONS)","text":"<ul> <li>300 lines of production code</li> <li>Uses real <code>langchain-mcp-adapters</code> library</li> <li>2 classes with clear responsibilities</li> <li>No mock code, no unnecessary abstractions</li> <li>85% code reduction</li> </ul>"},{"location":"MCP/overview/#what-was-deleted","title":"What Was Deleted","text":"<p>Following \"Zero Tolerance for Unused Code\": - \u274c <code>langchain_client.py</code> (250 lines) - Mock implementations - \u274c Factory functions - Single use abstraction - \u274c HTTP client - Not needed - \u274c Custom protocol handling - Library handles it - \u274c Complex schema conversions - Library handles it - \u274c 4 redundant documentation files</p> <p>Total deleted: 1,700+ lines</p>"},{"location":"MCP/overview/#design-principles-applied","title":"Design Principles Applied","text":"<p>\u2705 Radical Minimalism: 300 lines total \u2705 Use Real Libraries: No mock code \u2705 No Premature Architecture: Only what's needed \u2705 Delete Aggressively: Removed 85% of code \u2705 Question Abstractions: If used once, inline it</p>"},{"location":"MCP/overview/#file-structure","title":"File Structure","text":"<pre><code>elysia/tools/additional/\n\u251c\u2500\u2500 __init__.py              # 3 lines\n\u251c\u2500\u2500 mcp_adapter.py           # 176 lines\n\u251c\u2500\u2500 mcp_tool_wrapper.py      # 128 lines\n\u251c\u2500\u2500 ARCHITECTURE.md         # Complete guide\n\u251c\u2500\u2500 QUICKSTART.md           # 30-second setup\n\u2514\u2500\u2500 SUMMARY.md              # This file\n\nexamples/\n\u2514\u2500\u2500 mcp_integration_example.py\n</code></pre>"},{"location":"MCP/overview/#answer-to-original-question","title":"Answer to Original Question","text":"<p>\"Is it feasible to attach MCP Server as a tool as a whole?\"</p> <p>YES - Implemented both approaches:</p> <ol> <li>\u2705 MCP Server as Tool: MCPServerAdapter itself is a Tool</li> <li>\u2705 Auto-Register Tools: Adapter registers each MCP tool individually</li> </ol> <p>Both work seamlessly with Elysia's Tree architecture.</p>"},{"location":"MCP/overview/#key-achievements","title":"Key Achievements","text":"<ol> <li>Functional: Uses real <code>langchain-mcp-adapters</code> library</li> <li>Minimal: 85% less code than initial version</li> <li>Simple: 2 classes, clear responsibilities</li> <li>Modular: Easy to extend, follows Elysia patterns</li> <li>Documented: Complete architecture + quickstart</li> <li>Production-Ready: Error handling, logging, type safety</li> </ol>"},{"location":"MCP/overview/#next-steps-for-users","title":"Next Steps for Users","text":"<ol> <li><code>pip install langchain-mcp-adapters</code></li> <li>Point to your MCP server script</li> <li>Run <code>examples/mcp_integration_example.py</code></li> <li>Integrate with your Elysia Tree</li> </ol>"},{"location":"MCP/overview/#metrics","title":"Metrics","text":"Metric Before After Change Code lines 2,000+ 307 -85% Files 6 2 + init -67% Mock code 250 0 -100% Abstractions 5 layers 2 layers -60% Docs 5 files 3 files -40% <p>Mission accomplished: Simple. Minimal. Actually works.</p> <p>Following CODING_INSTRUCTIONS.md: \"The goal is always to write the minimal amount of code that achieves clean, maintainable functionality.\"</p>"},{"location":"MCP/quickstart/","title":"MCP Integration Quick Start","text":""},{"location":"MCP/quickstart/#installation","title":"Installation","text":"<pre><code>pip install langchain-mcp-adapters\n</code></pre>"},{"location":"MCP/quickstart/#30-second-setup","title":"30-Second Setup","text":"<pre><code>from elysia import Tree\nfrom elysia.tools.additional import MCPServerAdapter\n\n# 1. Create tree\ntree = Tree()\n\n# 2. Point to your MCP server script\nadapter = MCPServerAdapter(server_script_path=\"/path/to/your/mcp_server.py\")\n\n# 3. Register all tools\nawait adapter.initialize_and_register_tools(tree, branch_id=\"base\")\n\n# 4. Use it!\nresponse, objects = tree(\"Your query here\")\n</code></pre>"},{"location":"MCP/quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"MCP/quickstart/#1-basic-register-all-tools","title":"1. Basic: Register All Tools","text":"<pre><code>adapter = MCPServerAdapter(server_script_path=\"/path/to/server.py\")\nsuccess, tools = await adapter.initialize_and_register_tools(tree)\nprint(f\"Registered: {tools}\")\n</code></pre>"},{"location":"MCP/quickstart/#2-selective-registration","title":"2. Selective Registration","text":"<pre><code>adapter = MCPServerAdapter(server_script_path=\"/path/to/server.py\")\nawait adapter.initialize()\n\n# Choose specific tools\nfor tool in adapter.discovered_tools:\n    if \"search\" in tool.name:\n        tree.add_tool(tool, branch_id=\"base\")\n</code></pre>"},{"location":"MCP/quickstart/#3-multiple-servers","title":"3. Multiple Servers","text":"<pre><code>servers = [\n    \"/path/to/server1.py\",\n    \"/path/to/server2.py\",\n]\n\nfor server_path in servers:\n    adapter = MCPServerAdapter(server_script_path=server_path)\n    await adapter.initialize_and_register_tools(tree)\n</code></pre>"},{"location":"MCP/quickstart/#how-it-works","title":"How It Works","text":"<pre><code>Your MCP Server Script\n        \u2193\nlangchain-mcp-adapters loads tools\n        \u2193\nMCPServerAdapter wraps them as Elysia Tools\n        \u2193\nRegister with Elysia Tree\n        \u2193\nUse like any other Elysia tool\n</code></pre>"},{"location":"MCP/quickstart/#example-mcp-server-script","title":"Example MCP Server Script","text":"<p>Create a simple MCP server (e.g., <code>my_server.py</code>):</p> <pre><code># This is just an example - implement your actual MCP server here\n# See MCP documentation for server implementation details\n</code></pre>"},{"location":"MCP/quickstart/#testing","title":"Testing","text":"<pre><code>adapter = MCPServerAdapter(server_script_path=\"/path/to/server.py\")\nsuccess = await adapter.initialize()\n\nif success:\n    print(f\"\u2713 Loaded {len(adapter.discovered_tools)} tools\")\nelse:\n    print(\"\u2717 Failed to load tools\")\n</code></pre>"},{"location":"MCP/quickstart/#troubleshooting","title":"Troubleshooting","text":"Problem Solution Import error <code>pip install langchain-mcp-adapters</code> Server not found Check server_script_path is correct No tools loaded Verify MCP server implements tools correctly"},{"location":"MCP/quickstart/#next-steps","title":"Next Steps","text":"<ol> <li>Create or locate your MCP server script</li> <li>Run the example: <code>python examples/mcp_integration_example.py</code></li> <li>Integrate with your tree</li> <li>Start using MCP tools!</li> </ol> <p>Simple. Minimal. No overengineering. \ud83d\ude80</p>"},{"location":"MCP/usage_guide/","title":"MCP Tools Usage Guide","text":""},{"location":"MCP/usage_guide/#overview","title":"Overview","text":"<p>MCP (Model Context Protocol) tools are automatically loaded and made available to all Elysia Trees when configured in <code>elysia/mcp.json</code>.</p>"},{"location":"MCP/usage_guide/#configuration","title":"Configuration","text":""},{"location":"MCP/usage_guide/#location","title":"Location","text":"<ul> <li>File: <code>elysia/mcp.json</code> (at Elysia module root)</li> <li>Format: JSON configuration file</li> </ul>"},{"location":"MCP/usage_guide/#example-configuration","title":"Example Configuration","text":"<pre><code>{\n  \"servers\": [\n    {\n      \"name\": \"example_server\",\n      \"description\": \"Example MCP server providing search tools\",\n      \"server_script_path\": \"/path/to/mcp_server.py\",\n      \"enabled\": true\n    },\n    {\n      \"name\": \"another_server\",\n      \"description\": \"Another MCP server\",\n      \"server_script_path\": \"/path/to/another_server.py\",\n      \"enabled\": false\n    }\n  ]\n}\n</code></pre>"},{"location":"MCP/usage_guide/#configuration-fields","title":"Configuration Fields","text":"Field Required Description <code>name</code> Yes Unique identifier for the MCP server <code>description</code> No Human-readable description <code>server_script_path</code> Yes Path to the MCP server Python script <code>enabled</code> Yes Whether to load this server (true/false)"},{"location":"MCP/usage_guide/#how-it-works","title":"How It Works","text":""},{"location":"MCP/usage_guide/#1-at-application-startup","title":"1. At Application Startup","text":"<pre><code>Application Start\n    \u2193\nImport custom_tools\n    \u2193\nImport mcp_loader\n    \u2193\nload_mcp_servers_from_config()\n    \u2193\nRead elysia/mcp.json\n    \u2193\nFor each enabled server:\n    - Connect to MCP server via langchain-mcp-adapters\n    - Create MCPTool subclass: MCP_{server_name}\n    - Inject into custom_tools namespace\n    \u2193\nTools available via /tools/available API\n</code></pre>"},{"location":"MCP/usage_guide/#2-dynamic-tool-class-creation","title":"2. Dynamic Tool Class Creation","text":"<p>For a server named <code>\"example_server\"</code>, a class <code>MCP_example_server</code> is created:</p> <pre><code>class MCP_example_server(MCPTool):\n    \"\"\"Dynamically created MCP server tool.\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(\n            server_name=\"example_server\",\n            server_script_path=\"/path/to/mcp_server.py\",\n            **kwargs,\n        )\n</code></pre>"},{"location":"MCP/usage_guide/#3-tool-discovery","title":"3. Tool Discovery","text":"<p>The tool is automatically discovered by:</p> <pre><code># In util/tool_discovery.py (called by routes/tools.py)\ndef discover_tools_from_module():\n    # Inspects custom_tools module\n    # Finds MCP_example_server class\n    # Returns tool class dict\n\ndef get_tool_metadata():\n    # Gets metadata from discovered tools\n    # Returns metadata for API\n</code></pre>"},{"location":"MCP/usage_guide/#4-using-mcp-tools","title":"4. Using MCP Tools","text":""},{"location":"MCP/usage_guide/#option-a-automatic-recommended","title":"Option A: Automatic (Recommended)","text":"<p>MCP tools are automatically available to all Trees created via the API:</p> <pre><code># 1. Initialize user\ncurl -X POST http://localhost:8000/init/user/my_user\n\n# 2. Initialize tree\ncurl -X POST http://localhost:8000/init/tree/my_user/my_conversation \\\n  -H \"Content-Type: application/json\" -d '{\"low_memory\": false}'\n\n# 3. MCP tool is available in the tree!\n# Use it by name in queries to the tree\n</code></pre>"},{"location":"MCP/usage_guide/#option-b-programmatic","title":"Option B: Programmatic","text":"<p>Add MCP tool to a specific Tree:</p> <pre><code>from elysia.tree.tree import Tree\nfrom elysia.api.custom_tools import MCP_example_server\n\n# Create tree\ntree = Tree()\n\n# Add MCP tool to a branch\nmcp_tool = MCP_example_server()\ntree.add_tool(mcp_tool, branch_id=\"base\")\n\n# Now the tree can use the MCP server's tools\n</code></pre>"},{"location":"MCP/usage_guide/#mcp-tool-capabilities","title":"MCP Tool Capabilities","text":"<p>Each MCP tool (e.g., <code>MCP_example_server</code>) provides:</p>"},{"location":"MCP/usage_guide/#actions","title":"Actions","text":"<ol> <li> <p>List Tools - See what tools the MCP server provides    <pre><code>inputs = {\"action\": \"list\"}\n# Returns list of available tools from the server\n</code></pre></p> </li> <li> <p>Execute Tool - Run a specific tool from the server    <pre><code>inputs = {\n    \"action\": \"execute\",\n    \"tool_name\": \"search\",\n    \"tool_inputs\": {\"query\": \"machine learning\"}\n}\n# Executes the 'search' tool on the MCP server\n</code></pre></p> </li> </ol>"},{"location":"MCP/usage_guide/#metadata","title":"Metadata","text":"<p>Each MCP tool has metadata accessible via <code>/tools/available</code>:</p> <pre><code>{\n  \"MCP_example_server\": {\n    \"name\": \"mcp_example_server\",\n    \"description\": \"MCP server 'example_server' - provides access to multiple tools via Model Context Protocol\",\n    \"inputs\": {\n      \"action\": {\n        \"description\": \"Action: 'list' to show tools, 'execute' to run a specific tool\",\n        \"type\": \"str\",\n        \"default\": \"list\"\n      },\n      \"tool_name\": {\n        \"description\": \"Name of the tool to execute (required when action='execute')\",\n        \"type\": \"str\",\n        \"required\": false\n      },\n      \"tool_inputs\": {\n        \"description\": \"Inputs for the tool (required when action='execute')\",\n        \"type\": \"dict\",\n        \"required\": false,\n        \"default\": {}\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"MCP/usage_guide/#example-workflow","title":"Example Workflow","text":""},{"location":"MCP/usage_guide/#step-1-create-mcp-server-script","title":"Step 1: Create MCP Server Script","text":"<pre><code># my_mcp_server.py\n# Implement your MCP server following MCP protocol\n# See: https://github.com/modelcontextprotocol/\n</code></pre>"},{"location":"MCP/usage_guide/#step-2-configure-in-mcpjson","title":"Step 2: Configure in mcp.json","text":"<pre><code>{\n  \"servers\": [\n    {\n      \"name\": \"my_search_server\",\n      \"description\": \"Custom search server\",\n      \"server_script_path\": \"/app/my_mcp_server.py\",\n      \"enabled\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"MCP/usage_guide/#step-3-rebuild-container","title":"Step 3: Rebuild Container","text":"<pre><code>docker-compose build elysia\ndocker-compose up -d elysia\n</code></pre>"},{"location":"MCP/usage_guide/#step-4-verify-tool-is-available","title":"Step 4: Verify Tool is Available","text":"<pre><code>curl http://localhost:8000/tools/available | jq '.tools | keys'\n# Should include \"MCP_my_search_server\"\n</code></pre>"},{"location":"MCP/usage_guide/#step-5-use-in-tree","title":"Step 5: Use in Tree","text":"<p>The tool is automatically available to all Trees. The AI can decide to use it based on: - Tool name: <code>mcp_my_search_server</code> - Tool description - Available actions (list, execute)</p>"},{"location":"MCP/usage_guide/#architecture-benefits","title":"Architecture Benefits","text":""},{"location":"MCP/usage_guide/#1-zero-code-changes-required","title":"1. Zero Code Changes Required","text":"<p>Once configured in <code>mcp.json</code>, MCP tools are automatically: - Loaded at startup - Discovered by the API - Available to all Trees</p>"},{"location":"MCP/usage_guide/#2-consistent-interface","title":"2. Consistent Interface","text":"<p>All MCP tools follow the same interface: - Standard inputs (action, tool_name, tool_inputs) - Standard outputs (Status, Result, Error, Text) - Elysia Tool compatibility</p>"},{"location":"MCP/usage_guide/#3-dynamic-loading","title":"3. Dynamic Loading","text":"<ul> <li>Add/remove MCP servers without changing code</li> <li>Enable/disable servers with a config flag</li> <li>Hot-reload by rebuilding container</li> </ul>"},{"location":"MCP/usage_guide/#4-isolation","title":"4. Isolation","text":"<p>Each MCP server runs in isolation: - Separate process - Clear boundaries - No cross-contamination</p>"},{"location":"MCP/usage_guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"MCP/usage_guide/#tool-not-appearing","title":"Tool Not Appearing","text":"<ol> <li>Check <code>mcp.json</code> syntax is valid JSON</li> <li>Ensure <code>enabled: true</code> for the server</li> <li>Verify <code>server_script_path</code> is correct</li> <li>Rebuild container: <code>docker-compose build elysia</code></li> <li>Check logs: <code>docker-compose logs elysia | grep -i mcp</code></li> </ol>"},{"location":"MCP/usage_guide/#tool-execution-fails","title":"Tool Execution Fails","text":"<ol> <li>Verify MCP server script is valid</li> <li>Check tool inputs match MCP server expectations</li> <li>Review error messages in tree execution</li> <li>Test MCP server independently</li> </ol>"},{"location":"MCP/usage_guide/#import-errors","title":"Import Errors","text":"<p>If you see errors about <code>langchain-mcp-adapters</code>:</p> <pre><code># In Dockerfile or requirements, ensure:\npip install langchain-mcp-adapters\n</code></pre>"},{"location":"MCP/usage_guide/#current-status","title":"Current Status","text":""},{"location":"MCP/usage_guide/#installed","title":"Installed","text":"<ul> <li>\u2705 MCP integration framework</li> <li>\u2705 Tool discovery mechanism</li> <li>\u2705 API endpoints</li> <li>\u2705 Configuration file (<code>mcp.json</code>)</li> <li>\u2705 Dynamic class creation</li> <li>\u2705 Automatic registration</li> </ul>"},{"location":"MCP/usage_guide/#configuration_1","title":"Configuration","text":"<ul> <li>Location: <code>elysia/mcp.json</code></li> <li>Current State: Empty (no servers enabled)</li> <li>Status: Ready for configuration</li> </ul>"},{"location":"MCP/usage_guide/#to-enable","title":"To Enable","text":"<ol> <li>Create or locate MCP server script</li> <li>Add configuration to <code>mcp.json</code></li> <li>Set <code>enabled: true</code></li> <li>Rebuild container</li> <li>Tools automatically available!</li> </ol>"},{"location":"MCP/usage_guide/#reference","title":"Reference","text":""},{"location":"MCP/usage_guide/#api-endpoints","title":"API Endpoints","text":"Endpoint Method Purpose <code>/tools/available</code> GET List all available tools (including MCP) <code>/init/user/{user_id}</code> POST Initialize user with TreeManager <code>/init/tree/{user_id}/{conversation_id}</code> POST Initialize Tree with default tools"},{"location":"MCP/usage_guide/#file-locations","title":"File Locations","text":"Path Purpose <code>elysia/mcp.json</code> MCP server configuration <code>elysia/mcp.example.json</code> Example configuration <code>elysia/tools/mcp/mcp_tool.py</code> MCPTool base class <code>elysia/tools/mcp/mcp_loader.py</code> Dynamic tool loader <code>elysia/api/custom_tools.py</code> Tool registration"},{"location":"MCP/usage_guide/#related-documentation","title":"Related Documentation","text":"<ul> <li><code>ARCHITECTURE.md</code> - Technical architecture details</li> <li><code>QUICKSTART.md</code> - Quick setup guide  </li> <li><code>INTEGRATION_SUMMARY.md</code> - API flow documentation</li> <li><code>SUMMARY.md</code> - High-level overview</li> </ul> <p>MCP tools are ready to use! Configure <code>mcp.json</code> and rebuild to enable. \ud83d\ude80</p>"},{"location":"Reference/Client/","title":"WeaviateClient","text":""},{"location":"Reference/Client/#elysia.util.client.ClientManager","title":"<code>ClientManager</code>","text":"<p>Handles the creation and management of the Weaviate client. Handles cases where the client can be used in more than one thread or async operation at a time, via threading and asyncio locks. Also can use methods for restarting client if its been inactive.</p> Source code in <code>elysia/util/client.py</code> <pre><code>class ClientManager:\n    \"\"\"\n    Handles the creation and management of the Weaviate client.\n    Handles cases where the client can be used in more than one thread or async operation at a time,\n    via threading and asyncio locks.\n    Also can use methods for restarting client if its been inactive.\n    \"\"\"\n\n    def __init__(\n        self,\n        wcd_url: str | None = None,\n        wcd_api_key: str | None = None,\n        weaviate_is_local: bool | None = None,\n        local_weaviate_port: int | None = None,\n        local_weaviate_grpc_port: int | None = None,\n        client_timeout: datetime.timedelta | int | None = None,\n        logger: Logger | None = None,\n        settings: Settings | None = None,\n        query_timeout: int = 60,\n        insert_timeout: int = 120,\n        init_timeout: int = 5,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            wcd_url (str): the url of the Weaviate cluster. Defaults to global settings config.\n            wcd_api_key (str): the api key for the Weaviate cluster. Defaults to global settings config.\n            weaviate_is_local (bool): whether the weaviate cluster is local. Defaults to False.\n            client_timeout (datetime.timedelta | int | None): how long (in minutes) means the client should be restarted. Defaults to 3 minutes.\n            logger (Logger | None): a logger object for logging messages. Defaults to None.\n            settings (Settings | None): a settings object for the client manager. Defaults to environment settings.\n            query_timeout (int): the timeout for Weaviate queries. Defaults to 60 seconds (Weaviate default is 30 seconds).\n            insert_timeout (int): the timeout for Weaviate inserts. Defaults to 120 seconds (Weaviate default is 90 seconds).\n            init_timeout (int): the timeout for Weaviate initialisation. Defaults to 5 seconds (Weaviate default is 2 seconds).\n            **kwargs (Any): any other api keys for third party services (formatted as e.g. OPENAI_APIKEY).\n\n        Example:\n        ```python\n        client_manager = ClientManager(\n            wcd_url=\"https://my-weaviate-cluster...\",\n            wcd_api_key=\"my-api-key...\",\n            OPENAI_APIKEY=\"my-openai-api-key...\",\n            HUGGINGFACE_APIKEY=\"my-huggingface-api-key...\",\n        )\n        ```\n        \"\"\"\n\n        self.logger = logger\n\n        if client_timeout is None:\n            self.client_timeout = datetime.timedelta(\n                minutes=int(os.getenv(\"CLIENT_TIMEOUT\", 3))\n            )\n        elif isinstance(client_timeout, int):\n            self.client_timeout = datetime.timedelta(minutes=client_timeout)\n        else:\n            self.client_timeout = client_timeout\n\n        if settings is None:\n            self.settings = environment_settings\n        else:\n            self.settings = settings\n\n        # Set the weaviate url and api key\n        if wcd_url is None:\n            self.wcd_url = self.settings.WCD_URL\n        else:\n            self.wcd_url = wcd_url\n\n        if wcd_api_key is None:\n            self.wcd_api_key = self.settings.WCD_API_KEY\n        else:\n            self.wcd_api_key = wcd_api_key\n\n        if weaviate_is_local is None:\n            self.weaviate_is_local = self.settings.WEAVIATE_IS_LOCAL\n        else:\n            self.weaviate_is_local = weaviate_is_local\n\n        if local_weaviate_port is None:\n            self.local_weaviate_port = self.settings.LOCAL_WEAVIATE_PORT\n        else:\n            self.local_weaviate_port = local_weaviate_port\n\n        if local_weaviate_grpc_port is None:\n            self.local_weaviate_grpc_port = self.settings.LOCAL_WEAVIATE_GRPC_PORT\n        else:\n            self.local_weaviate_grpc_port = local_weaviate_grpc_port\n\n        self.query_timeout = query_timeout\n        self.insert_timeout = insert_timeout\n        self.init_timeout = init_timeout\n\n        if self.weaviate_is_local and (self.wcd_url is None or self.wcd_url == \"\"):\n            self.wcd_url = \"localhost\"\n\n        # Set the api keys for non weaviate cluster (third parties)\n        self.headers = {}\n        for api_key in self.settings.API_KEYS:\n            if api_key.lower() in [a.lower() for a in api_key_map.keys()]:\n                self.headers[api_key_map[api_key.upper()]] = self.settings.API_KEYS[\n                    api_key\n                ]\n\n        # From kwargs\n        for kwarg in kwargs:\n            if kwarg.lower() in [a.lower() for a in api_key_map.keys()]:\n                self.headers[api_key_map[kwarg.upper()]] = kwargs[kwarg]\n\n        # Create locks for client events\n        self.async_lock = asyncio.Lock()\n        self.sync_lock = threading.Lock()\n\n        # In use counter tracks when the client is in use and by how many operations. 0 = can restart\n        self.async_in_use_counter = 0\n        self.sync_in_use_counter = 0\n        self.async_restart_event = asyncio.Event()\n        self.sync_restart_event = threading.Event()\n\n        self.last_used_sync_client = datetime.datetime.now()\n        self.last_used_async_client = datetime.datetime.now()\n\n        self.async_client = None\n        self.async_init_completed = False\n        self.is_client = self.wcd_url != \"\" and (\n            self.wcd_api_key != \"\" or self.weaviate_is_local\n        )\n\n        if self.logger and not self.is_client:\n            if self.wcd_url == \"\" and self.weaviate_is_local:\n                self.logger.warning(\n                    \"WCD_URL not set for local Weaviate (This should probably be localhost). \"\n                    \"All Weaviate functionality will be disabled.\"\n                )\n            elif (\n                not self.weaviate_is_local\n                and self.wcd_api_key == \"\"\n                and self.wcd_url != \"\"\n            ):\n                self.logger.warning(\n                    \"WCD_API_KEY and WCD_URL are not set. \"\n                    \"All Weaviate functionality will be disabled.\"\n                )\n            elif self.wcd_url == \"\" and not self.weaviate_is_local:\n                self.logger.warning(\n                    \"WCD_URL is not set. \"\n                    \"All Weaviate functionality will be disabled.\"\n                )\n            elif self.wcd_api_key == \"\" and not self.weaviate_is_local:\n                self.logger.warning(\n                    \"WCD_API_KEY is not set. \"\n                    \"All Weaviate functionality will be disabled.\"\n                )\n            else:\n                self.logger.debug(\n                    \"Weaviate client initialised. \"\n                    \"All Weaviate functionality will be enabled.\"\n                )\n\n        if not self.is_client:\n            return\n\n        # Start sync client\n        try:\n            self.client = self.get_client()\n        except Exception as e:\n            self.logger.error(\n                \"Error initialising Weaviate client. Please check your Weaviate configuration is set correctly (WCD_URL, WCD_API_KEY, WEAVIATE_IS_LOCAL, LOCAL_WEAVIATE_PORT, LOCAL_WEAVIATE_GRPC_PORT).\"\n            )\n            self.logger.error(f\"Full Weaviate connection error message: {e}\")\n            self.is_client = False\n            return\n        self.sync_restart_event.set()\n\n    def _get_local_host_and_port(self) -&gt; tuple[str, int]:\n        \"\"\"\n        Derive host and port for local connections from wcd_url and configured ports.\n        Accepts full URLs like \"http://localhost:8080\" and extracts hostname/port.\n        \"\"\"\n        host = self.wcd_url if self.wcd_url is not None else \"localhost\"\n        port = self.local_weaviate_port\n        try:\n            parsed = urlparse(host)\n            if parsed.scheme in (\"http\", \"https\"):\n                if parsed.hostname:\n                    host = parsed.hostname\n                if parsed.port:\n                    port = parsed.port\n            # If no scheme, assume the value is a bare hostname (optionally with :port)\n            elif \":\" in host:\n                # Split manually to support host:port form without scheme\n                parts = host.split(\":\")\n                host = parts[0]\n                try:\n                    port = int(parts[1])\n                except Exception:\n                    port = self.local_weaviate_port\n        except Exception:\n            # Fallback to defaults\n            host = \"localhost\" if not host else host\n            port = self.local_weaviate_port\n        return host, port\n\n    async def reset_keys(\n        self,\n        wcd_url: str | None = None,\n        wcd_api_key: str | None = None,\n        api_keys: dict[str, str] = {},\n        weaviate_is_local: bool = False,\n        local_weaviate_port: int = 8080,\n        local_weaviate_grpc_port: int = 50051,\n    ) -&gt; None:\n        \"\"\"\n        Set the API keys, WCD_URL and WCD_API_KEY from the settings object.\n\n        Args:\n            wcd_url (str): the url of the Weaviate cluster.\n            wcd_api_key (str): the api key for the Weaviate cluster.\n            api_keys (dict): a dictionary of api keys for third party services.\n        \"\"\"\n        self.wcd_url = wcd_url\n        self.wcd_api_key = wcd_api_key\n        self.weaviate_is_local = weaviate_is_local\n        self.local_weaviate_port = local_weaviate_port\n        self.local_weaviate_grpc_port = local_weaviate_grpc_port\n\n        # If using a local Weaviate instance and no URL was provided, default to localhost\n        if self.weaviate_is_local and (self.wcd_url is None or self.wcd_url == \"\"):\n            self.wcd_url = \"localhost\"\n\n        self.headers = {}\n\n        for api_key in api_keys:\n            if api_key.lower() in [a.lower() for a in api_key_map.keys()]:\n                self.headers[api_key_map[api_key.upper()]] = api_keys[api_key]\n\n        # Local Weaviate can work without an API key\n        self.is_client = self.wcd_url != \"\" and (\n            self.wcd_api_key != \"\" or self.weaviate_is_local\n        )\n        if self.is_client:\n            await self.restart_client(force=True)\n            await self.restart_async_client(force=True)\n            await self.start_clients()\n\n    async def start_clients(self) -&gt; None:\n        \"\"\"\n        Start the async and sync clients if they are not already running.\n        \"\"\"\n\n        if not self.is_client:\n            raise ValueError(\n                \"Weaviate is not available. Please set the WCD_URL and WCD_API_KEY in the settings.\"\n            )\n\n        if self.async_client is None:\n            self.async_client = await self.get_async_client()\n            self.async_restart_event.set()\n\n        if not self.async_client.is_connected():\n            await self.async_client.connect()\n\n        self.async_init_completed = True\n\n        if not self.client.is_connected():\n            self.client.connect()\n\n    def update_last_user_request(self) -&gt; None:\n        self.last_user_request = datetime.datetime.now()\n\n    def update_last_used_sync_client(self) -&gt; None:\n        self.last_used_sync_client = datetime.datetime.now()\n\n    def update_last_used_async_client(self) -&gt; None:\n        self.last_used_async_client = datetime.datetime.now()\n\n    def get_client(self) -&gt; WeaviateClient:\n        if self.weaviate_is_local and self.wcd_url != \"\":\n            auth_credentials = (\n                Auth.api_key(self.wcd_api_key) if self.wcd_api_key != \"\" else None\n            )\n            host, port = self._get_local_host_and_port()\n            if self.logger:\n                self.logger.debug(\n                    f\"Getting client with weaviate_is_local: {self.weaviate_is_local}, \"\n                    f\"wcd_url: {self.wcd_url}, parsed_host: {host}, api_key_set: {self.wcd_api_key != ''}, \"\n                    f\"http_port: {port}, grpc_port: {self.local_weaviate_grpc_port}\"\n                )\n            return weaviate.connect_to_local(\n                host=host,\n                port=port,\n                grpc_port=self.local_weaviate_grpc_port,\n                auth_credentials=auth_credentials,\n                headers=self.headers,\n                skip_init_checks=True,\n            )\n\n        if self.wcd_url == \"\" or self.wcd_api_key == \"\":\n            raise ValueError(\"WCD_URL and WCD_API_KEY must be set\")\n\n        return weaviate.connect_to_weaviate_cloud(\n            cluster_url=self.wcd_url,\n            auth_credentials=Auth.api_key(self.wcd_api_key),\n            headers=self.headers,\n            skip_init_checks=True,\n            additional_config=AdditionalConfig(\n                timeout=Timeout(\n                    query=self.query_timeout,\n                    insert=self.insert_timeout,\n                    init=self.init_timeout,\n                )\n            ),\n        )\n\n    async def get_async_client(self) -&gt; WeaviateAsyncClient:\n        if self.weaviate_is_local and self.wcd_url != \"\":\n            auth_credentials = (\n                Auth.api_key(self.wcd_api_key) if self.wcd_api_key != \"\" else None\n            )\n            host, port = self._get_local_host_and_port()\n            if self.logger:\n                self.logger.debug(\n                    f\"Getting async client with weaviate_is_local: {self.weaviate_is_local}, \"\n                    f\"wcd_url: {self.wcd_url}, parsed_host: {host}, api_key_set: {self.wcd_api_key != ''}, \"\n                    f\"http_port: {port}, grpc_port: {self.local_weaviate_grpc_port}\"\n                )\n            return weaviate.use_async_with_local(\n                host=host,\n                port=port,\n                grpc_port=self.local_weaviate_grpc_port,\n                auth_credentials=auth_credentials,\n                headers=self.headers,\n                skip_init_checks=True,\n            )\n\n        if self.wcd_url == \"\" or self.wcd_api_key == \"\":\n            raise ValueError(\"WCD_URL and WCD_API_KEY must be set\")\n\n        return weaviate.use_async_with_weaviate_cloud(\n            cluster_url=self.wcd_url,\n            auth_credentials=Auth.api_key(self.wcd_api_key),\n            headers=self.headers,\n            skip_init_checks=True,\n            additional_config=AdditionalConfig(\n                timeout=Timeout(\n                    query=self.query_timeout,\n                    insert=self.insert_timeout,\n                    init=self.init_timeout,\n                )\n            ),\n        )\n\n    @contextmanager\n    def connect_to_client(self) -&gt; Generator[WeaviateClient, Any, None]:\n        \"\"\"\n        A context manager to connect to the _sync_ client.\n\n        E.g.\n\n        ```python\n        with client_manager.connect_to_client():\n            # do stuff with the weaviate client\n            ...\n        ```\n        \"\"\"\n        if not self.is_client:\n            raise ValueError(\n                \"Weaviate is not available. Please set the WCD_URL and WCD_API_KEY in the settings or connect to a local Weaviate instance.\"\n            )\n\n        self.sync_restart_event.wait()\n        with self.sync_lock:\n            self.sync_in_use_counter += 1\n\n        if not self.client.is_connected():\n            self.client.connect()\n\n        connection = _ClientConnection(self, self.client)\n        with connection:\n            yield connection.client\n\n    @asynccontextmanager\n    async def connect_to_async_client(\n        self,\n    ) -&gt; AsyncGenerator[WeaviateAsyncClient, Any]:\n        \"\"\"\n        A context manager to connect to the _async_ client.\n\n        E.g.\n        ```python\n        async with client_manager.connect_to_async_client():\n            # do stuff with the async weaviate client\n            ...\n        ```\n        \"\"\"\n        if not self.is_client:\n            raise ValueError(\n                \"Weaviate is not available. Please set the WCD_URL and WCD_API_KEY in the settings or connect to a local Weaviate instance.\"\n            )\n\n        if not self.async_init_completed:\n            await self.start_clients()\n\n        await self.async_restart_event.wait()\n        async with self.async_lock:\n            self.async_in_use_counter += 1\n\n        if self.async_client is None:\n            raise ValueError(\"Async client not initialised\")\n\n        if not self.async_client.is_connected():\n            await self.async_client.connect()\n\n        connection = _AsyncClientConnection(self, self.async_client)\n        async with connection:\n            yield connection.client\n\n    async def restart_async_client(self, force=False) -&gt; None:\n        \"\"\"\n        Restart the async client if it has not been used in the last client_timeout minutes (set in init).\n        \"\"\"\n        if self.client_timeout == datetime.timedelta(minutes=0) and not force:\n            return\n\n        # First check if the client has been used in the last X minutes\n        if (\n            datetime.datetime.now() - self.last_used_async_client &gt; self.client_timeout\n            or force\n        ):\n            # Acquire lock before modifying shared state to prevent race conditions\n            try:\n                async with self.async_lock:\n                    # Clear the event WHILE holding the lock to prevent new connections from starting\n                    # This ensures no new connections can proceed until we're done\n                    self.async_restart_event.clear()\n\n                    # Record current counter value before waiting\n                    last_recorded_counter = self.async_in_use_counter\n\n                    # Set reasonable timeout values\n                    time_spent = 0\n                    max_wait_time = 10  # seconds\n                    check_interval = 0.1  # seconds\n\n                    # Only wait if there are active connections\n                    if last_recorded_counter &gt; 0:\n                        # Wait for existing connections to complete\n                        while self.async_in_use_counter &gt; 0:\n                            # Release lock during sleep to prevent deadlock\n                            self.async_lock.release()\n\n                            # Only timeout after 10 seconds if nothing is happening\n                            # if the counter is changing, then things are happening and we should wait\n                            if self.async_in_use_counter != last_recorded_counter:\n                                last_recorded_counter = self.async_in_use_counter\n                                time_spent = 0\n\n                            try:\n                                await asyncio.sleep(check_interval)\n                                time_spent += check_interval\n                                if time_spent &gt; max_wait_time:\n                                    if self.logger:\n                                        self.logger.error(\n                                            f\"Async client restart timed out after {max_wait_time} seconds. \"\n                                        )\n                                    break\n                            finally:\n                                # Re-acquire lock after sleep\n                                await self.async_lock.acquire()\n\n                    # Handle timeout case - must reset state regardless of timeout\n                    if self.async_in_use_counter &gt; 0:\n                        if self.logger:\n                            self.logger.error(\n                                \"Force resetting async client state due to timeout\"\n                            )\n                        self.async_in_use_counter = 0\n\n                    # Whether we timed out or not, we need to restart the client\n                    try:\n                        # Only close if client exists and is connected\n                        if (\n                            hasattr(self, \"async_client\")\n                            and self.async_client is not None\n                        ):\n                            await self.async_client.close()\n                        await asyncio.sleep(0.1)\n                        # Create a new client instance\n                        self.async_client = await self.get_async_client()\n                    except Exception as e:\n                        if self.logger:\n                            self.logger.error(\n                                f\"Error during async client restart: {str(e)}\"\n                            )\n                        # Create a new client anyway to ensure we have a valid client\n                        self.async_client = await self.get_async_client()\n                    finally:\n                        # CRITICAL: Always set the event to prevent deadlocks\n                        # This ensures waiting connections can proceed\n                        self.async_restart_event.set()\n\n            except Exception as e:\n                if self.logger:\n                    self.logger.error(\n                        f\"Unexpected error in async client restart: {str(e)}\"\n                    )\n                # Ensure the event is set in all error cases\n                self.async_restart_event.set()\n                # Attempt to create a new client\n                self.async_client = await self.get_async_client()\n\n    async def restart_client(self, force=False) -&gt; None:\n        \"\"\"\n        Restart the sync client if it has not been used in the last client_timeout minutes (set in init).\n        \"\"\"\n        if self.client_timeout == datetime.timedelta(minutes=0) and not force:\n            return\n\n        # First check if the client has been used in the last X minutes\n        if (\n            datetime.datetime.now() - self.last_used_sync_client &gt; self.client_timeout\n            or force\n        ):\n            # Use both locks to prevent any race conditions between sync and async operations\n            try:\n                # Acquire sync lock first\n                with self.sync_lock:\n                    # Clear event while holding the lock to prevent race conditions\n                    self.sync_restart_event.clear()\n\n                    # Record current counter value\n                    last_recorded_counter = self.sync_in_use_counter\n\n                    # Set reasonable timeout values\n                    time_spent = 0\n                    max_wait_time = 10  # seconds\n                    check_interval = 0.1  # seconds\n\n                    # Only wait if there are active connections\n                    if last_recorded_counter &gt; 0:\n                        # Wait for existing connections to complete\n                        while self.sync_in_use_counter &gt; 0:\n                            # Must release lock during async sleep to prevent deadlock\n                            self.sync_lock.release()\n\n                            # Only timeout after 10 seconds if nothing is happening\n                            # if the counter is changing, then things are happening and we should wait\n                            if self.sync_in_use_counter != last_recorded_counter:\n                                last_recorded_counter = self.sync_in_use_counter\n                                time_spent = 0\n\n                            try:\n                                await asyncio.sleep(check_interval)\n                                time_spent += check_interval\n                                if time_spent &gt; max_wait_time:\n                                    if self.logger:\n                                        self.logger.error(\n                                            f\"Sync client restart timed out after {max_wait_time}s. \"\n                                            f\"Initial counter: {last_recorded_counter}, Current: {self.sync_in_use_counter}\"\n                                        )\n                                    break\n                            finally:\n                                # Re-acquire lock\n                                self.sync_lock.acquire()\n\n                    # Handle timeout case - must reset state regardless of timeout\n                    if self.sync_in_use_counter &gt; 0:\n                        if self.logger:\n                            self.logger.error(\n                                \"Force resetting sync client state due to timeout\"\n                            )\n                        self.sync_in_use_counter = 0\n\n                    # Whether we timed out or not, we need to restart the client\n                    try:\n                        # Only close if client exists and is connected\n                        if hasattr(self, \"client\") and self.client is not None:\n                            self.client.close()\n                        await asyncio.sleep(0.1)\n                        # Create a new client instance\n                        self.client = self.get_client()\n                    except Exception as e:\n                        if self.logger:\n                            self.logger.error(\n                                f\"Error during sync client restart: {str(e)}\"\n                            )\n                        # Create a new client anyway\n                        self.client = self.get_client()\n                    finally:\n                        # CRITICAL: Always set the event to prevent deadlocks\n                        self.sync_restart_event.set()\n\n            except Exception as e:\n                if self.logger:\n                    self.logger.error(\n                        f\"Unexpected error in sync client restart: {str(e)}\"\n                    )\n                # Ensure the event is set in all error cases\n                self.sync_restart_event.set()\n                # Attempt to create a new client\n                self.client = self.get_client()\n\n    async def close_clients(self) -&gt; None:\n        \"\"\"\n        Close both the async and sync clients.\n        Should not be called inside a Tool or other function inside the decision tree.\n        \"\"\"\n        if hasattr(self, \"async_client\") and self.async_client is not None:\n            await self.async_client.close()\n        if hasattr(self, \"client\") and self.client is not None:\n            self.client.close()\n</code></pre>"},{"location":"Reference/Client/#elysia.util.client.ClientManager.__init__","title":"<code>__init__(wcd_url=None, wcd_api_key=None, weaviate_is_local=None, local_weaviate_port=None, local_weaviate_grpc_port=None, client_timeout=None, logger=None, settings=None, query_timeout=60, insert_timeout=120, init_timeout=5, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>wcd_url</code> <code>str</code> <p>the url of the Weaviate cluster. Defaults to global settings config.</p> <code>None</code> <code>wcd_api_key</code> <code>str</code> <p>the api key for the Weaviate cluster. Defaults to global settings config.</p> <code>None</code> <code>weaviate_is_local</code> <code>bool</code> <p>whether the weaviate cluster is local. Defaults to False.</p> <code>None</code> <code>client_timeout</code> <code>timedelta | int | None</code> <p>how long (in minutes) means the client should be restarted. Defaults to 3 minutes.</p> <code>None</code> <code>logger</code> <code>Logger | None</code> <p>a logger object for logging messages. Defaults to None.</p> <code>None</code> <code>settings</code> <code>Settings | None</code> <p>a settings object for the client manager. Defaults to environment settings.</p> <code>None</code> <code>query_timeout</code> <code>int</code> <p>the timeout for Weaviate queries. Defaults to 60 seconds (Weaviate default is 30 seconds).</p> <code>60</code> <code>insert_timeout</code> <code>int</code> <p>the timeout for Weaviate inserts. Defaults to 120 seconds (Weaviate default is 90 seconds).</p> <code>120</code> <code>init_timeout</code> <code>int</code> <p>the timeout for Weaviate initialisation. Defaults to 5 seconds (Weaviate default is 2 seconds).</p> <code>5</code> <code>**kwargs</code> <code>Any</code> <p>any other api keys for third party services (formatted as e.g. OPENAI_APIKEY).</p> <code>{}</code> <p>Example: <pre><code>client_manager = ClientManager(\n    wcd_url=\"https://my-weaviate-cluster...\",\n    wcd_api_key=\"my-api-key...\",\n    OPENAI_APIKEY=\"my-openai-api-key...\",\n    HUGGINGFACE_APIKEY=\"my-huggingface-api-key...\",\n)\n</code></pre></p> Source code in <code>elysia/util/client.py</code> <pre><code>def __init__(\n    self,\n    wcd_url: str | None = None,\n    wcd_api_key: str | None = None,\n    weaviate_is_local: bool | None = None,\n    local_weaviate_port: int | None = None,\n    local_weaviate_grpc_port: int | None = None,\n    client_timeout: datetime.timedelta | int | None = None,\n    logger: Logger | None = None,\n    settings: Settings | None = None,\n    query_timeout: int = 60,\n    insert_timeout: int = 120,\n    init_timeout: int = 5,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Args:\n        wcd_url (str): the url of the Weaviate cluster. Defaults to global settings config.\n        wcd_api_key (str): the api key for the Weaviate cluster. Defaults to global settings config.\n        weaviate_is_local (bool): whether the weaviate cluster is local. Defaults to False.\n        client_timeout (datetime.timedelta | int | None): how long (in minutes) means the client should be restarted. Defaults to 3 minutes.\n        logger (Logger | None): a logger object for logging messages. Defaults to None.\n        settings (Settings | None): a settings object for the client manager. Defaults to environment settings.\n        query_timeout (int): the timeout for Weaviate queries. Defaults to 60 seconds (Weaviate default is 30 seconds).\n        insert_timeout (int): the timeout for Weaviate inserts. Defaults to 120 seconds (Weaviate default is 90 seconds).\n        init_timeout (int): the timeout for Weaviate initialisation. Defaults to 5 seconds (Weaviate default is 2 seconds).\n        **kwargs (Any): any other api keys for third party services (formatted as e.g. OPENAI_APIKEY).\n\n    Example:\n    ```python\n    client_manager = ClientManager(\n        wcd_url=\"https://my-weaviate-cluster...\",\n        wcd_api_key=\"my-api-key...\",\n        OPENAI_APIKEY=\"my-openai-api-key...\",\n        HUGGINGFACE_APIKEY=\"my-huggingface-api-key...\",\n    )\n    ```\n    \"\"\"\n\n    self.logger = logger\n\n    if client_timeout is None:\n        self.client_timeout = datetime.timedelta(\n            minutes=int(os.getenv(\"CLIENT_TIMEOUT\", 3))\n        )\n    elif isinstance(client_timeout, int):\n        self.client_timeout = datetime.timedelta(minutes=client_timeout)\n    else:\n        self.client_timeout = client_timeout\n\n    if settings is None:\n        self.settings = environment_settings\n    else:\n        self.settings = settings\n\n    # Set the weaviate url and api key\n    if wcd_url is None:\n        self.wcd_url = self.settings.WCD_URL\n    else:\n        self.wcd_url = wcd_url\n\n    if wcd_api_key is None:\n        self.wcd_api_key = self.settings.WCD_API_KEY\n    else:\n        self.wcd_api_key = wcd_api_key\n\n    if weaviate_is_local is None:\n        self.weaviate_is_local = self.settings.WEAVIATE_IS_LOCAL\n    else:\n        self.weaviate_is_local = weaviate_is_local\n\n    if local_weaviate_port is None:\n        self.local_weaviate_port = self.settings.LOCAL_WEAVIATE_PORT\n    else:\n        self.local_weaviate_port = local_weaviate_port\n\n    if local_weaviate_grpc_port is None:\n        self.local_weaviate_grpc_port = self.settings.LOCAL_WEAVIATE_GRPC_PORT\n    else:\n        self.local_weaviate_grpc_port = local_weaviate_grpc_port\n\n    self.query_timeout = query_timeout\n    self.insert_timeout = insert_timeout\n    self.init_timeout = init_timeout\n\n    if self.weaviate_is_local and (self.wcd_url is None or self.wcd_url == \"\"):\n        self.wcd_url = \"localhost\"\n\n    # Set the api keys for non weaviate cluster (third parties)\n    self.headers = {}\n    for api_key in self.settings.API_KEYS:\n        if api_key.lower() in [a.lower() for a in api_key_map.keys()]:\n            self.headers[api_key_map[api_key.upper()]] = self.settings.API_KEYS[\n                api_key\n            ]\n\n    # From kwargs\n    for kwarg in kwargs:\n        if kwarg.lower() in [a.lower() for a in api_key_map.keys()]:\n            self.headers[api_key_map[kwarg.upper()]] = kwargs[kwarg]\n\n    # Create locks for client events\n    self.async_lock = asyncio.Lock()\n    self.sync_lock = threading.Lock()\n\n    # In use counter tracks when the client is in use and by how many operations. 0 = can restart\n    self.async_in_use_counter = 0\n    self.sync_in_use_counter = 0\n    self.async_restart_event = asyncio.Event()\n    self.sync_restart_event = threading.Event()\n\n    self.last_used_sync_client = datetime.datetime.now()\n    self.last_used_async_client = datetime.datetime.now()\n\n    self.async_client = None\n    self.async_init_completed = False\n    self.is_client = self.wcd_url != \"\" and (\n        self.wcd_api_key != \"\" or self.weaviate_is_local\n    )\n\n    if self.logger and not self.is_client:\n        if self.wcd_url == \"\" and self.weaviate_is_local:\n            self.logger.warning(\n                \"WCD_URL not set for local Weaviate (This should probably be localhost). \"\n                \"All Weaviate functionality will be disabled.\"\n            )\n        elif (\n            not self.weaviate_is_local\n            and self.wcd_api_key == \"\"\n            and self.wcd_url != \"\"\n        ):\n            self.logger.warning(\n                \"WCD_API_KEY and WCD_URL are not set. \"\n                \"All Weaviate functionality will be disabled.\"\n            )\n        elif self.wcd_url == \"\" and not self.weaviate_is_local:\n            self.logger.warning(\n                \"WCD_URL is not set. \"\n                \"All Weaviate functionality will be disabled.\"\n            )\n        elif self.wcd_api_key == \"\" and not self.weaviate_is_local:\n            self.logger.warning(\n                \"WCD_API_KEY is not set. \"\n                \"All Weaviate functionality will be disabled.\"\n            )\n        else:\n            self.logger.debug(\n                \"Weaviate client initialised. \"\n                \"All Weaviate functionality will be enabled.\"\n            )\n\n    if not self.is_client:\n        return\n\n    # Start sync client\n    try:\n        self.client = self.get_client()\n    except Exception as e:\n        self.logger.error(\n            \"Error initialising Weaviate client. Please check your Weaviate configuration is set correctly (WCD_URL, WCD_API_KEY, WEAVIATE_IS_LOCAL, LOCAL_WEAVIATE_PORT, LOCAL_WEAVIATE_GRPC_PORT).\"\n        )\n        self.logger.error(f\"Full Weaviate connection error message: {e}\")\n        self.is_client = False\n        return\n    self.sync_restart_event.set()\n</code></pre>"},{"location":"Reference/Client/#elysia.util.client.ClientManager.close_clients","title":"<code>close_clients()</code>  <code>async</code>","text":"<p>Close both the async and sync clients. Should not be called inside a Tool or other function inside the decision tree.</p> Source code in <code>elysia/util/client.py</code> <pre><code>async def close_clients(self) -&gt; None:\n    \"\"\"\n    Close both the async and sync clients.\n    Should not be called inside a Tool or other function inside the decision tree.\n    \"\"\"\n    if hasattr(self, \"async_client\") and self.async_client is not None:\n        await self.async_client.close()\n    if hasattr(self, \"client\") and self.client is not None:\n        self.client.close()\n</code></pre>"},{"location":"Reference/Client/#elysia.util.client.ClientManager.connect_to_async_client","title":"<code>connect_to_async_client()</code>  <code>async</code>","text":"<p>A context manager to connect to the async client.</p> <p>E.g. <pre><code>async with client_manager.connect_to_async_client():\n    # do stuff with the async weaviate client\n    ...\n</code></pre></p> Source code in <code>elysia/util/client.py</code> <pre><code>@asynccontextmanager\nasync def connect_to_async_client(\n    self,\n) -&gt; AsyncGenerator[WeaviateAsyncClient, Any]:\n    \"\"\"\n    A context manager to connect to the _async_ client.\n\n    E.g.\n    ```python\n    async with client_manager.connect_to_async_client():\n        # do stuff with the async weaviate client\n        ...\n    ```\n    \"\"\"\n    if not self.is_client:\n        raise ValueError(\n            \"Weaviate is not available. Please set the WCD_URL and WCD_API_KEY in the settings or connect to a local Weaviate instance.\"\n        )\n\n    if not self.async_init_completed:\n        await self.start_clients()\n\n    await self.async_restart_event.wait()\n    async with self.async_lock:\n        self.async_in_use_counter += 1\n\n    if self.async_client is None:\n        raise ValueError(\"Async client not initialised\")\n\n    if not self.async_client.is_connected():\n        await self.async_client.connect()\n\n    connection = _AsyncClientConnection(self, self.async_client)\n    async with connection:\n        yield connection.client\n</code></pre>"},{"location":"Reference/Client/#elysia.util.client.ClientManager.connect_to_client","title":"<code>connect_to_client()</code>","text":"<p>A context manager to connect to the sync client.</p> <p>E.g.</p> <pre><code>with client_manager.connect_to_client():\n    # do stuff with the weaviate client\n    ...\n</code></pre> Source code in <code>elysia/util/client.py</code> <pre><code>@contextmanager\ndef connect_to_client(self) -&gt; Generator[WeaviateClient, Any, None]:\n    \"\"\"\n    A context manager to connect to the _sync_ client.\n\n    E.g.\n\n    ```python\n    with client_manager.connect_to_client():\n        # do stuff with the weaviate client\n        ...\n    ```\n    \"\"\"\n    if not self.is_client:\n        raise ValueError(\n            \"Weaviate is not available. Please set the WCD_URL and WCD_API_KEY in the settings or connect to a local Weaviate instance.\"\n        )\n\n    self.sync_restart_event.wait()\n    with self.sync_lock:\n        self.sync_in_use_counter += 1\n\n    if not self.client.is_connected():\n        self.client.connect()\n\n    connection = _ClientConnection(self, self.client)\n    with connection:\n        yield connection.client\n</code></pre>"},{"location":"Reference/Client/#elysia.util.client.ClientManager.reset_keys","title":"<code>reset_keys(wcd_url=None, wcd_api_key=None, api_keys={}, weaviate_is_local=False, local_weaviate_port=8080, local_weaviate_grpc_port=50051)</code>  <code>async</code>","text":"<p>Set the API keys, WCD_URL and WCD_API_KEY from the settings object.</p> <p>Parameters:</p> Name Type Description Default <code>wcd_url</code> <code>str</code> <p>the url of the Weaviate cluster.</p> <code>None</code> <code>wcd_api_key</code> <code>str</code> <p>the api key for the Weaviate cluster.</p> <code>None</code> <code>api_keys</code> <code>dict</code> <p>a dictionary of api keys for third party services.</p> <code>{}</code> Source code in <code>elysia/util/client.py</code> <pre><code>async def reset_keys(\n    self,\n    wcd_url: str | None = None,\n    wcd_api_key: str | None = None,\n    api_keys: dict[str, str] = {},\n    weaviate_is_local: bool = False,\n    local_weaviate_port: int = 8080,\n    local_weaviate_grpc_port: int = 50051,\n) -&gt; None:\n    \"\"\"\n    Set the API keys, WCD_URL and WCD_API_KEY from the settings object.\n\n    Args:\n        wcd_url (str): the url of the Weaviate cluster.\n        wcd_api_key (str): the api key for the Weaviate cluster.\n        api_keys (dict): a dictionary of api keys for third party services.\n    \"\"\"\n    self.wcd_url = wcd_url\n    self.wcd_api_key = wcd_api_key\n    self.weaviate_is_local = weaviate_is_local\n    self.local_weaviate_port = local_weaviate_port\n    self.local_weaviate_grpc_port = local_weaviate_grpc_port\n\n    # If using a local Weaviate instance and no URL was provided, default to localhost\n    if self.weaviate_is_local and (self.wcd_url is None or self.wcd_url == \"\"):\n        self.wcd_url = \"localhost\"\n\n    self.headers = {}\n\n    for api_key in api_keys:\n        if api_key.lower() in [a.lower() for a in api_key_map.keys()]:\n            self.headers[api_key_map[api_key.upper()]] = api_keys[api_key]\n\n    # Local Weaviate can work without an API key\n    self.is_client = self.wcd_url != \"\" and (\n        self.wcd_api_key != \"\" or self.weaviate_is_local\n    )\n    if self.is_client:\n        await self.restart_client(force=True)\n        await self.restart_async_client(force=True)\n        await self.start_clients()\n</code></pre>"},{"location":"Reference/Client/#elysia.util.client.ClientManager.restart_async_client","title":"<code>restart_async_client(force=False)</code>  <code>async</code>","text":"<p>Restart the async client if it has not been used in the last client_timeout minutes (set in init).</p> Source code in <code>elysia/util/client.py</code> <pre><code>async def restart_async_client(self, force=False) -&gt; None:\n    \"\"\"\n    Restart the async client if it has not been used in the last client_timeout minutes (set in init).\n    \"\"\"\n    if self.client_timeout == datetime.timedelta(minutes=0) and not force:\n        return\n\n    # First check if the client has been used in the last X minutes\n    if (\n        datetime.datetime.now() - self.last_used_async_client &gt; self.client_timeout\n        or force\n    ):\n        # Acquire lock before modifying shared state to prevent race conditions\n        try:\n            async with self.async_lock:\n                # Clear the event WHILE holding the lock to prevent new connections from starting\n                # This ensures no new connections can proceed until we're done\n                self.async_restart_event.clear()\n\n                # Record current counter value before waiting\n                last_recorded_counter = self.async_in_use_counter\n\n                # Set reasonable timeout values\n                time_spent = 0\n                max_wait_time = 10  # seconds\n                check_interval = 0.1  # seconds\n\n                # Only wait if there are active connections\n                if last_recorded_counter &gt; 0:\n                    # Wait for existing connections to complete\n                    while self.async_in_use_counter &gt; 0:\n                        # Release lock during sleep to prevent deadlock\n                        self.async_lock.release()\n\n                        # Only timeout after 10 seconds if nothing is happening\n                        # if the counter is changing, then things are happening and we should wait\n                        if self.async_in_use_counter != last_recorded_counter:\n                            last_recorded_counter = self.async_in_use_counter\n                            time_spent = 0\n\n                        try:\n                            await asyncio.sleep(check_interval)\n                            time_spent += check_interval\n                            if time_spent &gt; max_wait_time:\n                                if self.logger:\n                                    self.logger.error(\n                                        f\"Async client restart timed out after {max_wait_time} seconds. \"\n                                    )\n                                break\n                        finally:\n                            # Re-acquire lock after sleep\n                            await self.async_lock.acquire()\n\n                # Handle timeout case - must reset state regardless of timeout\n                if self.async_in_use_counter &gt; 0:\n                    if self.logger:\n                        self.logger.error(\n                            \"Force resetting async client state due to timeout\"\n                        )\n                    self.async_in_use_counter = 0\n\n                # Whether we timed out or not, we need to restart the client\n                try:\n                    # Only close if client exists and is connected\n                    if (\n                        hasattr(self, \"async_client\")\n                        and self.async_client is not None\n                    ):\n                        await self.async_client.close()\n                    await asyncio.sleep(0.1)\n                    # Create a new client instance\n                    self.async_client = await self.get_async_client()\n                except Exception as e:\n                    if self.logger:\n                        self.logger.error(\n                            f\"Error during async client restart: {str(e)}\"\n                        )\n                    # Create a new client anyway to ensure we have a valid client\n                    self.async_client = await self.get_async_client()\n                finally:\n                    # CRITICAL: Always set the event to prevent deadlocks\n                    # This ensures waiting connections can proceed\n                    self.async_restart_event.set()\n\n        except Exception as e:\n            if self.logger:\n                self.logger.error(\n                    f\"Unexpected error in async client restart: {str(e)}\"\n                )\n            # Ensure the event is set in all error cases\n            self.async_restart_event.set()\n            # Attempt to create a new client\n            self.async_client = await self.get_async_client()\n</code></pre>"},{"location":"Reference/Client/#elysia.util.client.ClientManager.restart_client","title":"<code>restart_client(force=False)</code>  <code>async</code>","text":"<p>Restart the sync client if it has not been used in the last client_timeout minutes (set in init).</p> Source code in <code>elysia/util/client.py</code> <pre><code>async def restart_client(self, force=False) -&gt; None:\n    \"\"\"\n    Restart the sync client if it has not been used in the last client_timeout minutes (set in init).\n    \"\"\"\n    if self.client_timeout == datetime.timedelta(minutes=0) and not force:\n        return\n\n    # First check if the client has been used in the last X minutes\n    if (\n        datetime.datetime.now() - self.last_used_sync_client &gt; self.client_timeout\n        or force\n    ):\n        # Use both locks to prevent any race conditions between sync and async operations\n        try:\n            # Acquire sync lock first\n            with self.sync_lock:\n                # Clear event while holding the lock to prevent race conditions\n                self.sync_restart_event.clear()\n\n                # Record current counter value\n                last_recorded_counter = self.sync_in_use_counter\n\n                # Set reasonable timeout values\n                time_spent = 0\n                max_wait_time = 10  # seconds\n                check_interval = 0.1  # seconds\n\n                # Only wait if there are active connections\n                if last_recorded_counter &gt; 0:\n                    # Wait for existing connections to complete\n                    while self.sync_in_use_counter &gt; 0:\n                        # Must release lock during async sleep to prevent deadlock\n                        self.sync_lock.release()\n\n                        # Only timeout after 10 seconds if nothing is happening\n                        # if the counter is changing, then things are happening and we should wait\n                        if self.sync_in_use_counter != last_recorded_counter:\n                            last_recorded_counter = self.sync_in_use_counter\n                            time_spent = 0\n\n                        try:\n                            await asyncio.sleep(check_interval)\n                            time_spent += check_interval\n                            if time_spent &gt; max_wait_time:\n                                if self.logger:\n                                    self.logger.error(\n                                        f\"Sync client restart timed out after {max_wait_time}s. \"\n                                        f\"Initial counter: {last_recorded_counter}, Current: {self.sync_in_use_counter}\"\n                                    )\n                                break\n                        finally:\n                            # Re-acquire lock\n                            self.sync_lock.acquire()\n\n                # Handle timeout case - must reset state regardless of timeout\n                if self.sync_in_use_counter &gt; 0:\n                    if self.logger:\n                        self.logger.error(\n                            \"Force resetting sync client state due to timeout\"\n                        )\n                    self.sync_in_use_counter = 0\n\n                # Whether we timed out or not, we need to restart the client\n                try:\n                    # Only close if client exists and is connected\n                    if hasattr(self, \"client\") and self.client is not None:\n                        self.client.close()\n                    await asyncio.sleep(0.1)\n                    # Create a new client instance\n                    self.client = self.get_client()\n                except Exception as e:\n                    if self.logger:\n                        self.logger.error(\n                            f\"Error during sync client restart: {str(e)}\"\n                        )\n                    # Create a new client anyway\n                    self.client = self.get_client()\n                finally:\n                    # CRITICAL: Always set the event to prevent deadlocks\n                    self.sync_restart_event.set()\n\n        except Exception as e:\n            if self.logger:\n                self.logger.error(\n                    f\"Unexpected error in sync client restart: {str(e)}\"\n                )\n            # Ensure the event is set in all error cases\n            self.sync_restart_event.set()\n            # Attempt to create a new client\n            self.client = self.get_client()\n</code></pre>"},{"location":"Reference/Client/#elysia.util.client.ClientManager.start_clients","title":"<code>start_clients()</code>  <code>async</code>","text":"<p>Start the async and sync clients if they are not already running.</p> Source code in <code>elysia/util/client.py</code> <pre><code>async def start_clients(self) -&gt; None:\n    \"\"\"\n    Start the async and sync clients if they are not already running.\n    \"\"\"\n\n    if not self.is_client:\n        raise ValueError(\n            \"Weaviate is not available. Please set the WCD_URL and WCD_API_KEY in the settings.\"\n        )\n\n    if self.async_client is None:\n        self.async_client = await self.get_async_client()\n        self.async_restart_event.set()\n\n    if not self.async_client.is_connected():\n        await self.async_client.connect()\n\n    self.async_init_completed = True\n\n    if not self.client.is_connected():\n        self.client.connect()\n</code></pre>"},{"location":"Reference/Managers/","title":"Managers","text":""},{"location":"Reference/Managers/#elysia.api.services.user.UserManager","title":"<code>UserManager</code>","text":"<p>The UserManager is designed to manage, for each user:</p> <ul> <li>TreeManager</li> <li>ClientManager</li> <li>FrontendConfig</li> </ul> <p>It contains methods for creating and updating these objects across a range of users, stored in a dictionary. It can be used as a dependency injection container for FastAPI. The user manager/tree manager is decoupled from the core of the Elysia package functionality. It is designed to be used to manage separate Elysia instances, set of trees (via tree managers), configs, etc.</p> Source code in <code>elysia/api/services/user.py</code> <pre><code>class UserManager:\n    \"\"\"\n    The UserManager is designed to manage, for each user:\n\n    - TreeManager\n    - ClientManager\n    - FrontendConfig\n\n    It contains methods for creating and updating these objects across a range of users, stored in a dictionary.\n    It can be used as a dependency injection container for FastAPI.\n    The user manager/tree manager is decoupled from the core of the Elysia package functionality.\n    It is designed to be used to manage separate Elysia instances, set of trees (via tree managers), configs, etc.\n    \"\"\"\n\n    def __init__(\n        self,\n        user_timeout: datetime.timedelta | int | None = None,\n    ):\n        \"\"\"\n        Args:\n            user_timeout (datetime.timedelta | int | None): Optional.\n                The length of time a user can be idle before being timed out.\n                Defaults to 20 minutes or the value of the USER_TIMEOUT environment variable.\n                If an integer is provided, it is interpreted as the number of minutes.\n        \"\"\"\n        if user_timeout is None:\n            self.user_timeout = datetime.timedelta(\n                minutes=int(os.environ.get(\"USER_TIMEOUT\", 20))\n            )\n        elif isinstance(user_timeout, int):\n            self.user_timeout = datetime.timedelta(minutes=user_timeout)\n        else:\n            self.user_timeout = user_timeout\n\n        self.manager_id = random.randint(0, 1000000)\n        self.date_of_reset = None\n        self.users = {}\n\n    def user_exists(self, user_id: str):\n        return user_id in self.users\n\n    async def update_config(\n        self,\n        user_id: str,\n        conversation_id: str | None = None,\n        config_id: str | None = None,\n        config_name: str | None = None,\n        settings: dict[str, Any] | None = None,\n        style: str | None = None,\n        agent_description: str | None = None,\n        end_goal: str | None = None,\n        branch_initialisation: str | None = None,\n    ):\n        local_user = await self.get_user_local(user_id)\n        local_user[\"tree_manager\"].update_config(\n            conversation_id,\n            config_id,\n            config_name,\n            settings,\n            style,\n            agent_description,\n            end_goal,\n            branch_initialisation,\n        )\n\n        await local_user[\"client_manager\"].reset_keys(\n            wcd_url=local_user[\"tree_manager\"].settings.WCD_URL,\n            wcd_api_key=local_user[\"tree_manager\"].settings.WCD_API_KEY,\n            api_keys=local_user[\"tree_manager\"].settings.API_KEYS,\n            weaviate_is_local=local_user[\"tree_manager\"].settings.WEAVIATE_IS_LOCAL,\n            local_weaviate_port=local_user[\"tree_manager\"].settings.LOCAL_WEAVIATE_PORT,\n            local_weaviate_grpc_port=local_user[\"tree_manager\"].settings.LOCAL_WEAVIATE_GRPC_PORT,\n        )\n\n    async def update_frontend_config(\n        self,\n        user_id: str,\n        config: dict[str, Any],\n    ):\n        local_user = await self.get_user_local(user_id)\n        frontend_config: FrontendConfig = local_user[\"frontend_config\"]\n        await frontend_config.configure(**config)\n\n    async def add_user_local(\n        self,\n        user_id: str,\n        config: Config | None = None,\n    ):\n        \"\"\"\n        Add a user to the UserManager.\n\n        Args:\n            user_id (str): Required. The unique identifier for the user.\n            config (Config): Required. The config for the user.\n        \"\"\"\n\n        # add user if it doesn't exist\n        if user_id not in self.users:\n            self.users[user_id] = {}\n\n            fe_config = await load_frontend_config_from_file(user_id, logger)\n\n            self.users[user_id][\"frontend_config\"] = fe_config\n\n            self.users[user_id][\"tree_manager\"] = TreeManager(\n                user_id=user_id,\n                config=config,\n                tree_timeout=fe_config.config[\"tree_timeout\"],\n            )\n\n            # client manager starts with env variables, when config is updated, api keys are updated\n            self.users[user_id][\"client_manager\"] = ClientManager(\n                logger=logger,\n                client_timeout=fe_config.config[\"client_timeout\"],\n                settings=self.users[user_id][\"tree_manager\"].config.settings,\n            )\n\n    async def get_user_local(self, user_id: str):\n        \"\"\"\n        Return a local user object.\n        Will raise a ValueError if the user is not found.\n\n        Args:\n            user_id (str): Required. The unique identifier for the user.\n\n        Returns:\n            (dict): A local user object, containing a TreeManager (\"tree_manager\"),\n                Frontend Config (\"frontend_config\") and ClientManager (\"client_manager\").\n        \"\"\"\n\n        if user_id not in self.users:\n            raise ValueError(\n                f\"User {user_id} not found. Please initialise a user first (by calling `add_user_local`).\"\n            )\n\n        # update last request (adds last_request to user)\n        await self.update_user_last_request(user_id)\n\n        return self.users[user_id]\n\n    async def get_tree(self, user_id: str, conversation_id: str):\n        \"\"\"\n        Get a tree for a user.\n        Will raise a ValueError if the user is not found, or the tree is not found.\n\n        Args:\n            user_id (str): Required. The unique identifier for the user.\n            conversation_id (str): Required. The unique identifier for the conversation.\n\n        Returns:\n            (Tree): The tree.\n        \"\"\"\n        local_user = await self.get_user_local(user_id)\n        return local_user[\"tree_manager\"].get_tree(conversation_id)\n\n    async def check_all_trees_timeout(self):\n        \"\"\"\n        Check all trees in all TreeManagers across all users and remove any that have not been active in the last tree_timeout.\n        \"\"\"\n        for user_id in self.users:\n            self.users[user_id][\"tree_manager\"].check_all_trees_timeout()\n\n    def check_user_timeout(self, user_id: str):\n        \"\"\"\n        Check if a user has been idle for the last user_timeout.\n\n        Args:\n            user_id (str): The user ID which contains the user.\n\n        Returns:\n            (bool): True if the user has been idle for the last user_timeout, False otherwise.\n        \"\"\"\n        # if user not found, return True\n        if user_id not in self.users:\n            return True\n\n        # Remove any trees that have not been active in the last user_timeout\n        # if (\n        #     \"last_request\" in self.users[user_id]\n        #     and datetime.datetime.now() - self.users[user_id][\"last_request\"]\n        #     &gt; self.user_timeout\n        # ):\n        #     return True\n\n        return False\n\n    async def check_all_users_timeout(self):\n        \"\"\"\n        Check all users in the UserManager and remove any that have not been active in the last user_timeout.\n        \"\"\"\n        if self.user_timeout == datetime.timedelta(minutes=0):\n            return\n\n        for user_id in self.users:\n            if self.check_user_timeout(user_id):\n                del self.users[user_id]\n\n    async def check_restart_clients(self):\n        \"\"\"\n        Check all clients in all ClientManagers across all users and run the restart_client() method (for sync and async clients).\n        The restart_client() methods will check if the client has been inactive for the last client_timeout minutes (set in init).\n        \"\"\"\n        for user_id in self.users:\n            if (\n                \"client_manager\" in self.users[user_id]\n                and self.users[user_id][\"client_manager\"].is_client\n            ):\n                await self.users[user_id][\"client_manager\"].restart_client()\n                await self.users[user_id][\"client_manager\"].restart_async_client()\n\n    async def close_all_clients(self):\n        \"\"\"\n        Close all clients in all ClientManagers across all users.\n        \"\"\"\n        for user_id in self.users:\n            if \"client_manager\" in self.users[user_id]:\n                await self.users[user_id][\"client_manager\"].close_clients()\n\n    async def initialise_tree(\n        self,\n        user_id: str,\n        conversation_id: str,\n        low_memory: bool = False,\n    ):\n        \"\"\"\n        Initialises a tree for a user for an existing user at user_id.\n        Requires a user to already exist in the UserManager, via `add_user_local`.\n        This is a wrapper for the TreeManager.add_tree() method.\n\n        Args:\n            user_id (str): Required. The unique identifier for the user.\n            conversation_id (str): Required. The unique identifier for a new conversation within the tree manager.\n            low_memory (bool): Optional. Whether to use low memory mode for the tree.\n                Controls the LM history being saved in the tree, and some other variables.\n                Defaults to False.\n        \"\"\"\n        # self.add_user_local(user_id)\n        local_user = await self.get_user_local(user_id)\n        tree_manager: TreeManager = local_user[\"tree_manager\"]\n        if not tree_manager.tree_exists(conversation_id):\n            tree_manager.add_tree(\n                conversation_id,\n                low_memory,\n            )\n        return tree_manager.get_tree(conversation_id)\n\n    async def save_tree(\n        self,\n        user_id: str,\n        conversation_id: str,\n        wcd_url: str | None = None,\n        wcd_api_key: str | None = None,\n    ):\n        \"\"\"\n        Save a tree to a Weaviate instance (set in the frontend config).\n        This is a wrapper for the TreeManager.save_tree_weaviate() method.\n\n        Args:\n            user_id (str): Required. The unique identifier for the user stored in the UserManager.\n            conversation_id (str): Required. The unique identifier for the conversation for the user.\n        \"\"\"\n\n        local_user = await self.get_user_local(user_id)\n        tree_manager: TreeManager = local_user[\"tree_manager\"]\n\n        if wcd_url is None or wcd_api_key is None:\n            save_location_client_manager = local_user[\n                \"frontend_config\"\n            ].save_location_client_manager\n        else:\n            save_location_client_manager = ClientManager(\n                logger=logger,\n                wcd_url=wcd_url,\n                wcd_api_key=wcd_api_key,\n            )\n\n        await tree_manager.save_tree_weaviate(\n            conversation_id, save_location_client_manager\n        )\n\n    async def check_tree_exists_weaviate(\n        self,\n        user_id: str,\n        conversation_id: str,\n        wcd_url: str | None = None,\n        wcd_api_key: str | None = None,\n    ):\n        \"\"\"\n        Check if a tree exists in a Weaviate instance (set in the frontend config).\n\n        Args:\n            user_id (str): Required. The unique identifier for the user stored in the UserManager.\n            conversation_id (str): Required. The unique identifier for the conversation for the user.\n            wcd_url (str | None): Required. The URL of the Weaviate Cloud Database instance used to save the tree.\n                Defaults to the value of the `wcd_url` setting in the frontend config.\n            wcd_api_key (str | None): Required. The API key for the Weaviate Cloud Database instance used to save the tree.\n                Defaults to the value of the `wcd_api_key` setting in the frontend config.\n\n        Returns:\n            (bool): True if the tree exists in the Weaviate instance, False otherwise.\n        \"\"\"\n\n        local_user = await self.get_user_local(user_id)\n        tree_manager: TreeManager = local_user[\"tree_manager\"]\n\n        if wcd_url is None or wcd_api_key is None:\n            save_location_client_manager = local_user[\n                \"frontend_config\"\n            ].save_location_client_manager\n        else:\n            save_location_client_manager = ClientManager(\n                logger=logger,\n                wcd_url=wcd_url,\n                wcd_api_key=wcd_api_key,\n            )\n\n        return await tree_manager.check_tree_exists_weaviate(\n            conversation_id, save_location_client_manager\n        )\n\n    async def load_tree(\n        self,\n        user_id: str,\n        conversation_id: str,\n        wcd_url: str | None = None,\n        wcd_api_key: str | None = None,\n    ):\n        \"\"\"\n        Load a tree from a Weaviate instance (set in the frontend config).\n        This is a wrapper for the TreeManager.load_tree_weaviate() method.\n\n        Args:\n            user_id (str): Required. The unique identifier for the user stored in the UserManager.\n            conversation_id (str): Required. The unique identifier for the conversation for the user.\n            wcd_url (str | None): Required. The URL of the Weaviate Cloud Database instance used to save the tree.\n                Defaults to the value of the `wcd_url` setting in the frontend config.\n            wcd_api_key (str | None): Required. The API key for the Weaviate Cloud Database instance used to save the tree.\n                Defaults to the value of the `wcd_api_key` setting in the frontend config.\n\n        Returns:\n            (list[dict]): A list of dictionaries, each containing a frontend payload that was used to generate the tree.\n                The list is ordered by the time the payload was originally sent to the frontend (at the time it was saved).\n        \"\"\"\n\n        local_user = await self.get_user_local(user_id)\n        tree_manager: TreeManager = local_user[\"tree_manager\"]\n\n        if wcd_url is None or wcd_api_key is None:\n            save_location_client_manager = local_user[\n                \"frontend_config\"\n            ].save_location_client_manager\n        else:\n            save_location_client_manager = ClientManager(\n                logger=logger,\n                wcd_url=wcd_url,\n                wcd_api_key=wcd_api_key,\n            )\n\n        return await tree_manager.load_tree_weaviate(\n            conversation_id, save_location_client_manager\n        )\n\n    async def delete_tree(\n        self,\n        user_id: str,\n        conversation_id: str,\n        wcd_url: str | None = None,\n        wcd_api_key: str | None = None,\n    ):\n        \"\"\"\n        Delete a saved tree from a Weaviate instance (set in the frontend config).\n        Also delete the tree from the local tree manager.\n        This is a wrapper for the TreeManager.delete_tree_weaviate() method and the TreeManager.delete_tree_local() method.\n\n        Args:\n            user_id (str): Required. The unique identifier for the user stored in the UserManager.\n            conversation_id (str): Required. The unique identifier for the conversation for the user.\n            wcd_url (str | None): Required. The URL of the Weaviate Cloud Database instance used to save the tree.\n                Defaults to the value of the `wcd_url` setting in the frontend config.\n            wcd_api_key (str | None): Required. The API key for the Weaviate Cloud Database instance used to save the tree.\n                Defaults to the value of the `wcd_api_key` setting in the frontend config.\n        \"\"\"\n\n        local_user = await self.get_user_local(user_id)\n        tree_manager: TreeManager = local_user[\"tree_manager\"]\n\n        if wcd_url is None or wcd_api_key is None:\n            save_location_client_manager = local_user[\n                \"frontend_config\"\n            ].save_location_client_manager\n        else:\n            save_location_client_manager = ClientManager(\n                logger=logger,\n                wcd_url=wcd_url,\n                wcd_api_key=wcd_api_key,\n            )\n\n        await tree_manager.delete_tree_weaviate(\n            conversation_id, save_location_client_manager\n        )\n        tree_manager.delete_tree_local(conversation_id)\n\n    async def get_saved_trees(\n        self,\n        user_id: str,\n        wcd_url: str | None = None,\n        wcd_api_key: str | None = None,\n    ):\n        \"\"\"\n        Get all saved trees from a Weaviate instance (set in the frontend config).\n\n        Args:\n            user_id (str): Required. The unique identifier for the user stored in the UserManager.\n            wcd_url (str | None): Required. The URL of the Weaviate Cloud Database instance used to save the tree.\n                Defaults to the value of the `wcd_url` setting in the frontend config.\n            wcd_api_key (str | None): Required. The API key for the Weaviate Cloud Database instance used to save the tree.\n                Defaults to the value of the `wcd_api_key` setting in the frontend config.\n\n        Returns:\n            (dict): A dictionary whose keys are the conversation IDs and whose values are dictionaries containing the title and last update time of the tree.\n                E.g.\n                ```\n                {\n                    \"12345678-XXX-YYYY-ZZZZ\": {\n                        \"title\": \"Query Request\",\n                        \"last_update_time\": \"2025-06-07T10:06:47.376000Z\"\n                    }\n                }\n                ```\n        \"\"\"\n        local_user = await self.get_user_local(user_id)\n\n        if wcd_url is None or wcd_api_key is None:\n            save_location_client_manager = local_user[\n                \"frontend_config\"\n            ].save_location_client_manager\n        else:\n            save_location_client_manager = ClientManager(\n                logger=logger,\n                wcd_url=wcd_url,\n                wcd_api_key=wcd_api_key,\n            )\n\n        return await get_saved_trees_weaviate(\n            \"ELYSIA_TREES__\", save_location_client_manager, user_id\n        )\n\n    async def update_user_last_request(self, user_id: str):\n        self.users[user_id][\"last_request\"] = datetime.datetime.now()\n\n    def check_tree_timeout(self, user_id: str, conversation_id: str):\n        if user_id not in self.users:\n            return True\n        elif conversation_id not in self.users[user_id][\"tree_manager\"].trees:\n            return True\n        return False\n\n    async def process_tree(\n        self,\n        query: str,\n        user_id: str,\n        conversation_id: str,\n        query_id: str,\n        training_route: str = \"\",\n        collection_names: list[str] = [],\n        save_trees_to_weaviate: bool | None = None,\n        wcd_url: str | None = None,\n        wcd_api_key: str | None = None,\n    ):\n        \"\"\"\n        Wrapper for the TreeManager.process_tree() method.\n        Which itself is a wrapper for the Tree.async_run() method.\n        This is an async generator which yields results from the tree.async_run() method.\n        Automatically sends error payloads if the user or tree has been timed out.\n\n        Args:\n            query (str): Required. The user input/prompt to process in the decision tree.\n            user_id (str): Required. The unique identifier for the user.\n            conversation_id (str): Required. The conversation ID which contains the tree.\n                This should be the same conversation ID as the one used to initialise the tree (see `initialise_tree`).\n            query_id (str): Required. A unique identifier for the query.\n            training_route (str): Optional. The training route, a string of the form \"tool1/tool2/tool1\" etc.\n                See the `tree.async_run()` method for more details.\n            collection_names (list[str]): Optional. A list of collection names to use in the query.\n                If not supplied, all collections will be used.\n            save_trees_to_weaviate (bool | None): Optional. Whether to save the trees to a Weaviate instance,\n                after the process_tree() method has finished.\n                Defaults to the value of the `save_trees_to_weaviate` setting in the frontend config.\n            wcd_url (str | None): Required. The URL of the Weaviate Cloud Database instance used to save the tree.\n                Defaults to the value of the `wcd_url` setting in the frontend config.\n            wcd_api_key (str | None): Required. The API key for the Weaviate Cloud Database instance used to save the tree.\n                Defaults to the value of the `wcd_api_key` setting in the frontend config.\n        \"\"\"\n\n        if self.check_user_timeout(user_id):\n            user_timeout_error = UserTimeoutError()\n            error_payload = await user_timeout_error.to_frontend(\n                user_id, conversation_id, query_id\n            )\n            yield error_payload\n            return\n\n        if self.check_tree_timeout(user_id, conversation_id):\n            if await self.check_tree_exists_weaviate(user_id, conversation_id):\n                await self.load_tree(user_id, conversation_id)\n            else:\n                tree_timeout_error = TreeTimeoutError()\n                error_payload = await tree_timeout_error.to_frontend(\n                    user_id, conversation_id, query_id\n                )\n                yield error_payload\n                return\n\n        local_user = await self.get_user_local(user_id)\n        await self.update_user_last_request(user_id)\n\n        tree_manager: TreeManager = local_user[\"tree_manager\"]\n\n        async for yielded_result in tree_manager.process_tree(\n            query,\n            conversation_id,\n            query_id,\n            training_route,\n            collection_names,\n            local_user[\"client_manager\"],\n        ):\n            yield yielded_result\n            await self.update_user_last_request(user_id)\n\n        if save_trees_to_weaviate is None:\n            frontend_config: FrontendConfig = local_user[\"frontend_config\"]\n            save_trees_to_weaviate = frontend_config.config[\"save_trees_to_weaviate\"]\n\n        if save_trees_to_weaviate:\n            await self.save_tree(user_id, conversation_id, wcd_url, wcd_api_key)\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.__init__","title":"<code>__init__(user_timeout=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>user_timeout</code> <code>timedelta | int | None</code> <p>Optional. The length of time a user can be idle before being timed out. Defaults to 20 minutes or the value of the USER_TIMEOUT environment variable. If an integer is provided, it is interpreted as the number of minutes.</p> <code>None</code> Source code in <code>elysia/api/services/user.py</code> <pre><code>def __init__(\n    self,\n    user_timeout: datetime.timedelta | int | None = None,\n):\n    \"\"\"\n    Args:\n        user_timeout (datetime.timedelta | int | None): Optional.\n            The length of time a user can be idle before being timed out.\n            Defaults to 20 minutes or the value of the USER_TIMEOUT environment variable.\n            If an integer is provided, it is interpreted as the number of minutes.\n    \"\"\"\n    if user_timeout is None:\n        self.user_timeout = datetime.timedelta(\n            minutes=int(os.environ.get(\"USER_TIMEOUT\", 20))\n        )\n    elif isinstance(user_timeout, int):\n        self.user_timeout = datetime.timedelta(minutes=user_timeout)\n    else:\n        self.user_timeout = user_timeout\n\n    self.manager_id = random.randint(0, 1000000)\n    self.date_of_reset = None\n    self.users = {}\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.add_user_local","title":"<code>add_user_local(user_id, config=None)</code>  <code>async</code>","text":"<p>Add a user to the UserManager.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Required. The unique identifier for the user.</p> required <code>config</code> <code>Config</code> <p>Required. The config for the user.</p> <code>None</code> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def add_user_local(\n    self,\n    user_id: str,\n    config: Config | None = None,\n):\n    \"\"\"\n    Add a user to the UserManager.\n\n    Args:\n        user_id (str): Required. The unique identifier for the user.\n        config (Config): Required. The config for the user.\n    \"\"\"\n\n    # add user if it doesn't exist\n    if user_id not in self.users:\n        self.users[user_id] = {}\n\n        fe_config = await load_frontend_config_from_file(user_id, logger)\n\n        self.users[user_id][\"frontend_config\"] = fe_config\n\n        self.users[user_id][\"tree_manager\"] = TreeManager(\n            user_id=user_id,\n            config=config,\n            tree_timeout=fe_config.config[\"tree_timeout\"],\n        )\n\n        # client manager starts with env variables, when config is updated, api keys are updated\n        self.users[user_id][\"client_manager\"] = ClientManager(\n            logger=logger,\n            client_timeout=fe_config.config[\"client_timeout\"],\n            settings=self.users[user_id][\"tree_manager\"].config.settings,\n        )\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.check_all_trees_timeout","title":"<code>check_all_trees_timeout()</code>  <code>async</code>","text":"<p>Check all trees in all TreeManagers across all users and remove any that have not been active in the last tree_timeout.</p> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def check_all_trees_timeout(self):\n    \"\"\"\n    Check all trees in all TreeManagers across all users and remove any that have not been active in the last tree_timeout.\n    \"\"\"\n    for user_id in self.users:\n        self.users[user_id][\"tree_manager\"].check_all_trees_timeout()\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.check_all_users_timeout","title":"<code>check_all_users_timeout()</code>  <code>async</code>","text":"<p>Check all users in the UserManager and remove any that have not been active in the last user_timeout.</p> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def check_all_users_timeout(self):\n    \"\"\"\n    Check all users in the UserManager and remove any that have not been active in the last user_timeout.\n    \"\"\"\n    if self.user_timeout == datetime.timedelta(minutes=0):\n        return\n\n    for user_id in self.users:\n        if self.check_user_timeout(user_id):\n            del self.users[user_id]\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.check_restart_clients","title":"<code>check_restart_clients()</code>  <code>async</code>","text":"<p>Check all clients in all ClientManagers across all users and run the restart_client() method (for sync and async clients). The restart_client() methods will check if the client has been inactive for the last client_timeout minutes (set in init).</p> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def check_restart_clients(self):\n    \"\"\"\n    Check all clients in all ClientManagers across all users and run the restart_client() method (for sync and async clients).\n    The restart_client() methods will check if the client has been inactive for the last client_timeout minutes (set in init).\n    \"\"\"\n    for user_id in self.users:\n        if (\n            \"client_manager\" in self.users[user_id]\n            and self.users[user_id][\"client_manager\"].is_client\n        ):\n            await self.users[user_id][\"client_manager\"].restart_client()\n            await self.users[user_id][\"client_manager\"].restart_async_client()\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.check_tree_exists_weaviate","title":"<code>check_tree_exists_weaviate(user_id, conversation_id, wcd_url=None, wcd_api_key=None)</code>  <code>async</code>","text":"<p>Check if a tree exists in a Weaviate instance (set in the frontend config).</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Required. The unique identifier for the user stored in the UserManager.</p> required <code>conversation_id</code> <code>str</code> <p>Required. The unique identifier for the conversation for the user.</p> required <code>wcd_url</code> <code>str | None</code> <p>Required. The URL of the Weaviate Cloud Database instance used to save the tree. Defaults to the value of the <code>wcd_url</code> setting in the frontend config.</p> <code>None</code> <code>wcd_api_key</code> <code>str | None</code> <p>Required. The API key for the Weaviate Cloud Database instance used to save the tree. Defaults to the value of the <code>wcd_api_key</code> setting in the frontend config.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the tree exists in the Weaviate instance, False otherwise.</p> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def check_tree_exists_weaviate(\n    self,\n    user_id: str,\n    conversation_id: str,\n    wcd_url: str | None = None,\n    wcd_api_key: str | None = None,\n):\n    \"\"\"\n    Check if a tree exists in a Weaviate instance (set in the frontend config).\n\n    Args:\n        user_id (str): Required. The unique identifier for the user stored in the UserManager.\n        conversation_id (str): Required. The unique identifier for the conversation for the user.\n        wcd_url (str | None): Required. The URL of the Weaviate Cloud Database instance used to save the tree.\n            Defaults to the value of the `wcd_url` setting in the frontend config.\n        wcd_api_key (str | None): Required. The API key for the Weaviate Cloud Database instance used to save the tree.\n            Defaults to the value of the `wcd_api_key` setting in the frontend config.\n\n    Returns:\n        (bool): True if the tree exists in the Weaviate instance, False otherwise.\n    \"\"\"\n\n    local_user = await self.get_user_local(user_id)\n    tree_manager: TreeManager = local_user[\"tree_manager\"]\n\n    if wcd_url is None or wcd_api_key is None:\n        save_location_client_manager = local_user[\n            \"frontend_config\"\n        ].save_location_client_manager\n    else:\n        save_location_client_manager = ClientManager(\n            logger=logger,\n            wcd_url=wcd_url,\n            wcd_api_key=wcd_api_key,\n        )\n\n    return await tree_manager.check_tree_exists_weaviate(\n        conversation_id, save_location_client_manager\n    )\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.check_user_timeout","title":"<code>check_user_timeout(user_id)</code>","text":"<p>Check if a user has been idle for the last user_timeout.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>The user ID which contains the user.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the user has been idle for the last user_timeout, False otherwise.</p> Source code in <code>elysia/api/services/user.py</code> <pre><code>def check_user_timeout(self, user_id: str):\n    \"\"\"\n    Check if a user has been idle for the last user_timeout.\n\n    Args:\n        user_id (str): The user ID which contains the user.\n\n    Returns:\n        (bool): True if the user has been idle for the last user_timeout, False otherwise.\n    \"\"\"\n    # if user not found, return True\n    if user_id not in self.users:\n        return True\n\n    # Remove any trees that have not been active in the last user_timeout\n    # if (\n    #     \"last_request\" in self.users[user_id]\n    #     and datetime.datetime.now() - self.users[user_id][\"last_request\"]\n    #     &gt; self.user_timeout\n    # ):\n    #     return True\n\n    return False\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.close_all_clients","title":"<code>close_all_clients()</code>  <code>async</code>","text":"<p>Close all clients in all ClientManagers across all users.</p> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def close_all_clients(self):\n    \"\"\"\n    Close all clients in all ClientManagers across all users.\n    \"\"\"\n    for user_id in self.users:\n        if \"client_manager\" in self.users[user_id]:\n            await self.users[user_id][\"client_manager\"].close_clients()\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.delete_tree","title":"<code>delete_tree(user_id, conversation_id, wcd_url=None, wcd_api_key=None)</code>  <code>async</code>","text":"<p>Delete a saved tree from a Weaviate instance (set in the frontend config). Also delete the tree from the local tree manager. This is a wrapper for the TreeManager.delete_tree_weaviate() method and the TreeManager.delete_tree_local() method.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Required. The unique identifier for the user stored in the UserManager.</p> required <code>conversation_id</code> <code>str</code> <p>Required. The unique identifier for the conversation for the user.</p> required <code>wcd_url</code> <code>str | None</code> <p>Required. The URL of the Weaviate Cloud Database instance used to save the tree. Defaults to the value of the <code>wcd_url</code> setting in the frontend config.</p> <code>None</code> <code>wcd_api_key</code> <code>str | None</code> <p>Required. The API key for the Weaviate Cloud Database instance used to save the tree. Defaults to the value of the <code>wcd_api_key</code> setting in the frontend config.</p> <code>None</code> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def delete_tree(\n    self,\n    user_id: str,\n    conversation_id: str,\n    wcd_url: str | None = None,\n    wcd_api_key: str | None = None,\n):\n    \"\"\"\n    Delete a saved tree from a Weaviate instance (set in the frontend config).\n    Also delete the tree from the local tree manager.\n    This is a wrapper for the TreeManager.delete_tree_weaviate() method and the TreeManager.delete_tree_local() method.\n\n    Args:\n        user_id (str): Required. The unique identifier for the user stored in the UserManager.\n        conversation_id (str): Required. The unique identifier for the conversation for the user.\n        wcd_url (str | None): Required. The URL of the Weaviate Cloud Database instance used to save the tree.\n            Defaults to the value of the `wcd_url` setting in the frontend config.\n        wcd_api_key (str | None): Required. The API key for the Weaviate Cloud Database instance used to save the tree.\n            Defaults to the value of the `wcd_api_key` setting in the frontend config.\n    \"\"\"\n\n    local_user = await self.get_user_local(user_id)\n    tree_manager: TreeManager = local_user[\"tree_manager\"]\n\n    if wcd_url is None or wcd_api_key is None:\n        save_location_client_manager = local_user[\n            \"frontend_config\"\n        ].save_location_client_manager\n    else:\n        save_location_client_manager = ClientManager(\n            logger=logger,\n            wcd_url=wcd_url,\n            wcd_api_key=wcd_api_key,\n        )\n\n    await tree_manager.delete_tree_weaviate(\n        conversation_id, save_location_client_manager\n    )\n    tree_manager.delete_tree_local(conversation_id)\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.get_saved_trees","title":"<code>get_saved_trees(user_id, wcd_url=None, wcd_api_key=None)</code>  <code>async</code>","text":"<p>Get all saved trees from a Weaviate instance (set in the frontend config).</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Required. The unique identifier for the user stored in the UserManager.</p> required <code>wcd_url</code> <code>str | None</code> <p>Required. The URL of the Weaviate Cloud Database instance used to save the tree. Defaults to the value of the <code>wcd_url</code> setting in the frontend config.</p> <code>None</code> <code>wcd_api_key</code> <code>str | None</code> <p>Required. The API key for the Weaviate Cloud Database instance used to save the tree. Defaults to the value of the <code>wcd_api_key</code> setting in the frontend config.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary whose keys are the conversation IDs and whose values are dictionaries containing the title and last update time of the tree. E.g. <pre><code>{\n    \"12345678-XXX-YYYY-ZZZZ\": {\n        \"title\": \"Query Request\",\n        \"last_update_time\": \"2025-06-07T10:06:47.376000Z\"\n    }\n}\n</code></pre></p> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def get_saved_trees(\n    self,\n    user_id: str,\n    wcd_url: str | None = None,\n    wcd_api_key: str | None = None,\n):\n    \"\"\"\n    Get all saved trees from a Weaviate instance (set in the frontend config).\n\n    Args:\n        user_id (str): Required. The unique identifier for the user stored in the UserManager.\n        wcd_url (str | None): Required. The URL of the Weaviate Cloud Database instance used to save the tree.\n            Defaults to the value of the `wcd_url` setting in the frontend config.\n        wcd_api_key (str | None): Required. The API key for the Weaviate Cloud Database instance used to save the tree.\n            Defaults to the value of the `wcd_api_key` setting in the frontend config.\n\n    Returns:\n        (dict): A dictionary whose keys are the conversation IDs and whose values are dictionaries containing the title and last update time of the tree.\n            E.g.\n            ```\n            {\n                \"12345678-XXX-YYYY-ZZZZ\": {\n                    \"title\": \"Query Request\",\n                    \"last_update_time\": \"2025-06-07T10:06:47.376000Z\"\n                }\n            }\n            ```\n    \"\"\"\n    local_user = await self.get_user_local(user_id)\n\n    if wcd_url is None or wcd_api_key is None:\n        save_location_client_manager = local_user[\n            \"frontend_config\"\n        ].save_location_client_manager\n    else:\n        save_location_client_manager = ClientManager(\n            logger=logger,\n            wcd_url=wcd_url,\n            wcd_api_key=wcd_api_key,\n        )\n\n    return await get_saved_trees_weaviate(\n        \"ELYSIA_TREES__\", save_location_client_manager, user_id\n    )\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.get_tree","title":"<code>get_tree(user_id, conversation_id)</code>  <code>async</code>","text":"<p>Get a tree for a user. Will raise a ValueError if the user is not found, or the tree is not found.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Required. The unique identifier for the user.</p> required <code>conversation_id</code> <code>str</code> <p>Required. The unique identifier for the conversation.</p> required <p>Returns:</p> Type Description <code>Tree</code> <p>The tree.</p> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def get_tree(self, user_id: str, conversation_id: str):\n    \"\"\"\n    Get a tree for a user.\n    Will raise a ValueError if the user is not found, or the tree is not found.\n\n    Args:\n        user_id (str): Required. The unique identifier for the user.\n        conversation_id (str): Required. The unique identifier for the conversation.\n\n    Returns:\n        (Tree): The tree.\n    \"\"\"\n    local_user = await self.get_user_local(user_id)\n    return local_user[\"tree_manager\"].get_tree(conversation_id)\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.get_user_local","title":"<code>get_user_local(user_id)</code>  <code>async</code>","text":"<p>Return a local user object. Will raise a ValueError if the user is not found.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Required. The unique identifier for the user.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A local user object, containing a TreeManager (\"tree_manager\"), Frontend Config (\"frontend_config\") and ClientManager (\"client_manager\").</p> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def get_user_local(self, user_id: str):\n    \"\"\"\n    Return a local user object.\n    Will raise a ValueError if the user is not found.\n\n    Args:\n        user_id (str): Required. The unique identifier for the user.\n\n    Returns:\n        (dict): A local user object, containing a TreeManager (\"tree_manager\"),\n            Frontend Config (\"frontend_config\") and ClientManager (\"client_manager\").\n    \"\"\"\n\n    if user_id not in self.users:\n        raise ValueError(\n            f\"User {user_id} not found. Please initialise a user first (by calling `add_user_local`).\"\n        )\n\n    # update last request (adds last_request to user)\n    await self.update_user_last_request(user_id)\n\n    return self.users[user_id]\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.initialise_tree","title":"<code>initialise_tree(user_id, conversation_id, low_memory=False)</code>  <code>async</code>","text":"<p>Initialises a tree for a user for an existing user at user_id. Requires a user to already exist in the UserManager, via <code>add_user_local</code>. This is a wrapper for the TreeManager.add_tree() method.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Required. The unique identifier for the user.</p> required <code>conversation_id</code> <code>str</code> <p>Required. The unique identifier for a new conversation within the tree manager.</p> required <code>low_memory</code> <code>bool</code> <p>Optional. Whether to use low memory mode for the tree. Controls the LM history being saved in the tree, and some other variables. Defaults to False.</p> <code>False</code> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def initialise_tree(\n    self,\n    user_id: str,\n    conversation_id: str,\n    low_memory: bool = False,\n):\n    \"\"\"\n    Initialises a tree for a user for an existing user at user_id.\n    Requires a user to already exist in the UserManager, via `add_user_local`.\n    This is a wrapper for the TreeManager.add_tree() method.\n\n    Args:\n        user_id (str): Required. The unique identifier for the user.\n        conversation_id (str): Required. The unique identifier for a new conversation within the tree manager.\n        low_memory (bool): Optional. Whether to use low memory mode for the tree.\n            Controls the LM history being saved in the tree, and some other variables.\n            Defaults to False.\n    \"\"\"\n    # self.add_user_local(user_id)\n    local_user = await self.get_user_local(user_id)\n    tree_manager: TreeManager = local_user[\"tree_manager\"]\n    if not tree_manager.tree_exists(conversation_id):\n        tree_manager.add_tree(\n            conversation_id,\n            low_memory,\n        )\n    return tree_manager.get_tree(conversation_id)\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.load_tree","title":"<code>load_tree(user_id, conversation_id, wcd_url=None, wcd_api_key=None)</code>  <code>async</code>","text":"<p>Load a tree from a Weaviate instance (set in the frontend config). This is a wrapper for the TreeManager.load_tree_weaviate() method.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Required. The unique identifier for the user stored in the UserManager.</p> required <code>conversation_id</code> <code>str</code> <p>Required. The unique identifier for the conversation for the user.</p> required <code>wcd_url</code> <code>str | None</code> <p>Required. The URL of the Weaviate Cloud Database instance used to save the tree. Defaults to the value of the <code>wcd_url</code> setting in the frontend config.</p> <code>None</code> <code>wcd_api_key</code> <code>str | None</code> <p>Required. The API key for the Weaviate Cloud Database instance used to save the tree. Defaults to the value of the <code>wcd_api_key</code> setting in the frontend config.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries, each containing a frontend payload that was used to generate the tree. The list is ordered by the time the payload was originally sent to the frontend (at the time it was saved).</p> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def load_tree(\n    self,\n    user_id: str,\n    conversation_id: str,\n    wcd_url: str | None = None,\n    wcd_api_key: str | None = None,\n):\n    \"\"\"\n    Load a tree from a Weaviate instance (set in the frontend config).\n    This is a wrapper for the TreeManager.load_tree_weaviate() method.\n\n    Args:\n        user_id (str): Required. The unique identifier for the user stored in the UserManager.\n        conversation_id (str): Required. The unique identifier for the conversation for the user.\n        wcd_url (str | None): Required. The URL of the Weaviate Cloud Database instance used to save the tree.\n            Defaults to the value of the `wcd_url` setting in the frontend config.\n        wcd_api_key (str | None): Required. The API key for the Weaviate Cloud Database instance used to save the tree.\n            Defaults to the value of the `wcd_api_key` setting in the frontend config.\n\n    Returns:\n        (list[dict]): A list of dictionaries, each containing a frontend payload that was used to generate the tree.\n            The list is ordered by the time the payload was originally sent to the frontend (at the time it was saved).\n    \"\"\"\n\n    local_user = await self.get_user_local(user_id)\n    tree_manager: TreeManager = local_user[\"tree_manager\"]\n\n    if wcd_url is None or wcd_api_key is None:\n        save_location_client_manager = local_user[\n            \"frontend_config\"\n        ].save_location_client_manager\n    else:\n        save_location_client_manager = ClientManager(\n            logger=logger,\n            wcd_url=wcd_url,\n            wcd_api_key=wcd_api_key,\n        )\n\n    return await tree_manager.load_tree_weaviate(\n        conversation_id, save_location_client_manager\n    )\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.process_tree","title":"<code>process_tree(query, user_id, conversation_id, query_id, training_route='', collection_names=[], save_trees_to_weaviate=None, wcd_url=None, wcd_api_key=None)</code>  <code>async</code>","text":"<p>Wrapper for the TreeManager.process_tree() method. Which itself is a wrapper for the Tree.async_run() method. This is an async generator which yields results from the tree.async_run() method. Automatically sends error payloads if the user or tree has been timed out.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Required. The user input/prompt to process in the decision tree.</p> required <code>user_id</code> <code>str</code> <p>Required. The unique identifier for the user.</p> required <code>conversation_id</code> <code>str</code> <p>Required. The conversation ID which contains the tree. This should be the same conversation ID as the one used to initialise the tree (see <code>initialise_tree</code>).</p> required <code>query_id</code> <code>str</code> <p>Required. A unique identifier for the query.</p> required <code>training_route</code> <code>str</code> <p>Optional. The training route, a string of the form \"tool1/tool2/tool1\" etc. See the <code>tree.async_run()</code> method for more details.</p> <code>''</code> <code>collection_names</code> <code>list[str]</code> <p>Optional. A list of collection names to use in the query. If not supplied, all collections will be used.</p> <code>[]</code> <code>save_trees_to_weaviate</code> <code>bool | None</code> <p>Optional. Whether to save the trees to a Weaviate instance, after the process_tree() method has finished. Defaults to the value of the <code>save_trees_to_weaviate</code> setting in the frontend config.</p> <code>None</code> <code>wcd_url</code> <code>str | None</code> <p>Required. The URL of the Weaviate Cloud Database instance used to save the tree. Defaults to the value of the <code>wcd_url</code> setting in the frontend config.</p> <code>None</code> <code>wcd_api_key</code> <code>str | None</code> <p>Required. The API key for the Weaviate Cloud Database instance used to save the tree. Defaults to the value of the <code>wcd_api_key</code> setting in the frontend config.</p> <code>None</code> Source code in <code>elysia/api/services/user.py</code> <pre><code>async def process_tree(\n    self,\n    query: str,\n    user_id: str,\n    conversation_id: str,\n    query_id: str,\n    training_route: str = \"\",\n    collection_names: list[str] = [],\n    save_trees_to_weaviate: bool | None = None,\n    wcd_url: str | None = None,\n    wcd_api_key: str | None = None,\n):\n    \"\"\"\n    Wrapper for the TreeManager.process_tree() method.\n    Which itself is a wrapper for the Tree.async_run() method.\n    This is an async generator which yields results from the tree.async_run() method.\n    Automatically sends error payloads if the user or tree has been timed out.\n\n    Args:\n        query (str): Required. The user input/prompt to process in the decision tree.\n        user_id (str): Required. The unique identifier for the user.\n        conversation_id (str): Required. The conversation ID which contains the tree.\n            This should be the same conversation ID as the one used to initialise the tree (see `initialise_tree`).\n        query_id (str): Required. A unique identifier for the query.\n        training_route (str): Optional. The training route, a string of the form \"tool1/tool2/tool1\" etc.\n            See the `tree.async_run()` method for more details.\n        collection_names (list[str]): Optional. A list of collection names to use in the query.\n            If not supplied, all collections will be used.\n        save_trees_to_weaviate (bool | None): Optional. Whether to save the trees to a Weaviate instance,\n            after the process_tree() method has finished.\n            Defaults to the value of the `save_trees_to_weaviate` setting in the frontend config.\n        wcd_url (str | None): Required. The URL of the Weaviate Cloud Database instance used to save the tree.\n            Defaults to the value of the `wcd_url` setting in the frontend config.\n        wcd_api_key (str | None): Required. The API key for the Weaviate Cloud Database instance used to save the tree.\n            Defaults to the value of the `wcd_api_key` setting in the frontend config.\n    \"\"\"\n\n    if self.check_user_timeout(user_id):\n        user_timeout_error = UserTimeoutError()\n        error_payload = await user_timeout_error.to_frontend(\n            user_id, conversation_id, query_id\n        )\n        yield error_payload\n        return\n\n    if self.check_tree_timeout(user_id, conversation_id):\n        if await self.check_tree_exists_weaviate(user_id, conversation_id):\n            await self.load_tree(user_id, conversation_id)\n        else:\n            tree_timeout_error = TreeTimeoutError()\n            error_payload = await tree_timeout_error.to_frontend(\n                user_id, conversation_id, query_id\n            )\n            yield error_payload\n            return\n\n    local_user = await self.get_user_local(user_id)\n    await self.update_user_last_request(user_id)\n\n    tree_manager: TreeManager = local_user[\"tree_manager\"]\n\n    async for yielded_result in tree_manager.process_tree(\n        query,\n        conversation_id,\n        query_id,\n        training_route,\n        collection_names,\n        local_user[\"client_manager\"],\n    ):\n        yield yielded_result\n        await self.update_user_last_request(user_id)\n\n    if save_trees_to_weaviate is None:\n        frontend_config: FrontendConfig = local_user[\"frontend_config\"]\n        save_trees_to_weaviate = frontend_config.config[\"save_trees_to_weaviate\"]\n\n    if save_trees_to_weaviate:\n        await self.save_tree(user_id, conversation_id, wcd_url, wcd_api_key)\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.user.UserManager.save_tree","title":"<code>save_tree(user_id, conversation_id, wcd_url=None, wcd_api_key=None)</code>  <code>async</code>","text":"<p>Save a tree to a Weaviate instance (set in the frontend config). This is a wrapper for the TreeManager.save_tree_weaviate() method.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Required. The unique identifier for the user stored in the UserManager.</p> required <code>conversation_id</code> <code>str</code> <p>Required. The unique identifier for the conversation for the user.</p> required Source code in <code>elysia/api/services/user.py</code> <pre><code>async def save_tree(\n    self,\n    user_id: str,\n    conversation_id: str,\n    wcd_url: str | None = None,\n    wcd_api_key: str | None = None,\n):\n    \"\"\"\n    Save a tree to a Weaviate instance (set in the frontend config).\n    This is a wrapper for the TreeManager.save_tree_weaviate() method.\n\n    Args:\n        user_id (str): Required. The unique identifier for the user stored in the UserManager.\n        conversation_id (str): Required. The unique identifier for the conversation for the user.\n    \"\"\"\n\n    local_user = await self.get_user_local(user_id)\n    tree_manager: TreeManager = local_user[\"tree_manager\"]\n\n    if wcd_url is None or wcd_api_key is None:\n        save_location_client_manager = local_user[\n            \"frontend_config\"\n        ].save_location_client_manager\n    else:\n        save_location_client_manager = ClientManager(\n            logger=logger,\n            wcd_url=wcd_url,\n            wcd_api_key=wcd_api_key,\n        )\n\n    await tree_manager.save_tree_weaviate(\n        conversation_id, save_location_client_manager\n    )\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager","title":"<code>TreeManager</code>","text":"<p>Manages trees (different conversations) for a single user. Designed to be used with the Elysia API, or a manager for multiple Elysia decision trees.</p> <p>You can initialise the TreeManager with particular config options. Or, upon adding a tree, you can set a specific style, description and end goal for that tree.</p> <p>Each tree has separate elements: the tree itself, the last request time, and the asyncio event.</p> Source code in <code>elysia/api/services/tree.py</code> <pre><code>class TreeManager:\n    \"\"\"\n    Manages trees (different conversations) for a single user.\n    Designed to be used with the Elysia API, or a manager for multiple Elysia decision trees.\n\n    You can initialise the TreeManager with particular config options.\n    Or, upon adding a tree, you can set a specific style, description and end goal for that tree.\n\n    Each tree has separate elements: the tree itself, the last request time, and the asyncio event.\n    \"\"\"\n\n    def __init__(\n        self,\n        user_id: str,\n        config: Config | None = None,\n        tree_timeout: datetime.timedelta | int | None = None,\n    ):\n        \"\"\"\n        Args:\n            user_id (str): Required. A unique identifier for the user being managed within this TreeManager.\n            config (Config | None): Optional. A config for all trees managed by this TreeManager.\n                Defaults to a new config with default settings.\n                If `settings` is not provided, it will be set to the default settings (smart_setup).\n            tree_timeout (datetime.timedelta | int | None): Optional. A timeout for all trees managed by this TreeManager.\n                Defaults to the value of the `TREE_TIMEOUT` environment variable.\n                If an integer is passed, it will be interpreted as minutes.\n                If set to 0, trees will not be automatically removed.\n        \"\"\"\n        self.trees = {}\n        self.user_id = user_id\n\n        if tree_timeout is None:\n            self.tree_timeout = datetime.timedelta(\n                minutes=int(os.environ.get(\"TREE_TIMEOUT\", 10))\n            )\n        elif isinstance(tree_timeout, int):\n            self.tree_timeout = datetime.timedelta(minutes=tree_timeout)\n        else:\n            self.tree_timeout = tree_timeout\n\n        if config is None:\n            self.config = Config()\n        else:\n            self.config = config\n\n        self.settings = self.config.settings\n\n    def update_config(\n        self,\n        conversation_id: str | None = None,\n        config_id: str | None = None,\n        config_name: str | None = None,\n        settings: dict[str, Any] | None = None,\n        style: str | None = None,\n        agent_description: str | None = None,\n        end_goal: str | None = None,\n        branch_initialisation: BranchInitType | None = None,\n    ):\n        if config_id is not None:\n            self.config.id = config_id\n\n        if config_name is not None:\n            self.config.name = config_name\n\n        if settings is not None:\n            self.configure(conversation_id=conversation_id, replace=True, **settings)\n\n        if style is not None:\n            self.change_style(style, conversation_id)\n\n        if agent_description is not None:\n            self.change_agent_description(agent_description, conversation_id)\n\n        if end_goal is not None:\n            self.change_end_goal(end_goal, conversation_id)\n\n        if branch_initialisation is not None:\n            self.change_branch_initialisation(branch_initialisation, conversation_id)\n\n    def add_tree(\n        self,\n        conversation_id: str,\n        low_memory: bool = False,\n    ):\n        \"\"\"\n        Add a tree to the TreeManager.\n        The decision tree can be initialised with specific config options, such as style, agent description and end goal.\n        As well as a Settings object, which chooses options such as the LLM models, API keys and more.\n\n        Args:\n            conversation_id (str): Required. A unique identifier for the conversation.\n            low_memory (bool): Optional. Whether to use low memory mode for the tree.\n                Controls the LM history being saved in the tree, and some other variables.\n                Defaults to False.\n        \"\"\"\n\n        if not self.tree_exists(conversation_id):\n            self.trees[conversation_id] = {\n                \"tree\": Tree(\n                    conversation_id=conversation_id,\n                    user_id=self.user_id,\n                    settings=self.settings,\n                    style=self.config.style,\n                    agent_description=self.config.agent_description,\n                    end_goal=self.config.end_goal,\n                    branch_initialisation=self.config.branch_initialisation,\n                    low_memory=low_memory,\n                    use_elysia_collections=self.config.use_elysia_collections,\n                ),\n                \"last_request\": datetime.datetime.now(),\n                \"event\": asyncio.Event(),\n            }\n            self.trees[conversation_id][\"event\"].set()\n\n    async def save_tree_weaviate(\n        self, conversation_id: str, client_manager: ClientManager\n    ):\n        \"\"\"\n        Save a tree to Weaviate to collection ELYSIA_TREES__.\n        Creates the collection if it doesn't exist.\n\n        Args:\n            conversation_id (str): The conversation ID which contains the tree.\n            client_manager (ClientManager): The client manager to use for the tree.\n        \"\"\"\n        tree: Tree = self.get_tree(conversation_id)\n        await tree.export_to_weaviate(\"ELYSIA_TREES__\", client_manager)\n\n    async def check_tree_exists_weaviate(\n        self, conversation_id: str, client_manager: ClientManager\n    ):\n        \"\"\"\n        Check if a tree exists in a Weaviate instance.\n        The collection ELYSIA_TREES__ must exist, returns False if it doesn't.\n\n        Args:\n            conversation_id (str): The conversation ID which contains the tree.\n            client_manager (ClientManager): The client manager to use for the tree.\n\n        Returns:\n            (bool): True if the tree exists in the Weaviate instance, False otherwise.\n        \"\"\"\n        async with client_manager.connect_to_async_client() as client:\n            if not await client.collections.exists(\"ELYSIA_TREES__\"):\n                return False\n\n            collection = client.collections.get(\"ELYSIA_TREES__\")\n            uuid = generate_uuid5(conversation_id)\n            return await collection.data.exists(uuid)\n\n    async def load_tree_weaviate(\n        self, conversation_id: str, client_manager: ClientManager\n    ):\n        \"\"\"\n        Load a tree from Weaviate.\n        The conversation ID from the loaded tree is placed into the tree manager\n        (possibly overwriting an existing tree with the same conversation ID).\n        Then the tree itself is not returned - instead the list of frontend payloads\n        that were yielded to the frontend by the tree is returned.\n\n        Args:\n            conversation_id (str): The conversation ID which contains the tree.\n            client_manager (ClientManager): The client manager to use for the tree.\n\n        Returns:\n            (list): A list of dictionaries, each containing a frontend payload that was used to generate the tree.\n                The list is ordered by the time the payload was originally sent to the frontend (at the time it was saved).\n        \"\"\"\n        tree = await Tree.import_from_weaviate(\n            \"ELYSIA_TREES__\", conversation_id, client_manager\n        )\n        if conversation_id not in self.trees:\n            self.trees[conversation_id] = {\n                \"tree\": None,\n                \"event\": asyncio.Event(),\n                \"last_request\": datetime.datetime.now(),\n            }\n        self.trees[conversation_id][\"tree\"] = tree\n        self.trees[conversation_id][\"event\"].set()\n        self.update_tree_last_request(conversation_id)\n        return tree.returner.store\n\n    async def delete_tree_weaviate(\n        self, conversation_id: str, client_manager: ClientManager\n    ):\n        \"\"\"\n        Delete a tree from the stored trees in Weaviate.\n\n        Args:\n            conversation_id (str): The conversation ID of the tree to be deleted.\n            client_manager (ClientManager): The client manager pointing to the Weaviate instance containing the tree.\n        \"\"\"\n        await delete_tree_from_weaviate(\n            conversation_id, \"ELYSIA_TREES__\", client_manager\n        )\n\n    def delete_tree_local(self, conversation_id: str):\n        \"\"\"\n        Delete a tree from the TreeManager.\n\n        Args:\n            conversation_id (str): The conversation ID of the tree to be deleted.\n        \"\"\"\n        if conversation_id in self.trees:\n            del self.trees[conversation_id]\n\n    def tree_exists(self, conversation_id: str):\n        \"\"\"\n        Check if a tree exists in the TreeManager.\n\n        Args:\n            conversation_id (str): The conversation ID which may contain the tree.\n\n        Returns:\n            (bool): True if the tree exists, False otherwise.\n        \"\"\"\n        return conversation_id in self.trees\n\n    def get_tree(self, conversation_id: str):\n        \"\"\"\n        Get a tree from the TreeManager.\n        Will raise a ValueError if the tree is not found.\n\n        Args:\n            conversation_id (str): The conversation ID which contains the tree.\n\n        Returns:\n            (Tree): The tree associated with the conversation ID.\n        \"\"\"\n        if conversation_id not in self.trees:\n            raise ValueError(\n                f\"Tree {conversation_id} not found. Please initialise a tree first (by calling `add_tree`).\"\n            )\n\n        return self.trees[conversation_id][\"tree\"]\n\n    def get_event(self, conversation_id: str):\n        \"\"\"\n        Get the asyncio.Event for a tree in the TreeManager.\n        This is cleared when the tree is processing, and set when the tree is idle.\n        This is used to block the API from sending multiple requests to the tree at once.\n\n        Args:\n            conversation_id (str): The conversation ID which contains the tree.\n\n        Returns:\n            (asyncio.Event): The event for the tree.\n        \"\"\"\n        return self.trees[conversation_id][\"event\"]\n\n    def configure(\n        self, conversation_id: str | None = None, replace: bool = False, **kwargs: Any\n    ):\n        \"\"\"\n        Configure the settings for a tree in the TreeManager.\n\n        Args:\n            conversation_id (str | None): The conversation ID which contains the tree.\n            replace (bool): Whether to override the current settings with the new settings.\n                When this is True, all existing settings are removed, and only the new settings are used.\n                Defaults to False.\n            **kwargs (Any): The keyword arguments to pass to the Settings.configure() method.\n        \"\"\"\n        if conversation_id is None:\n            self.settings.configure(replace=replace, **kwargs)\n            self.config.settings = self.settings\n        else:\n            self.get_tree(conversation_id).settings.configure(replace=replace, **kwargs)\n\n    def change_style(self, style: str, conversation_id: str | None = None):\n        \"\"\"\n        Change the style for a tree in the TreeManager.\n        Or change the global style for all trees (if conversation_id is not supplied).\n        And applies these changes to current trees with default settings.\n\n        Args:\n            style (str): The new style for the tree.\n            conversation_id (str | None): Optional. The conversation ID which contains the tree.\n                If not supplied, the style will be changed on all trees.\n        \"\"\"\n        if conversation_id is None:\n            self.config.style = style\n            for conversation_id in self.trees:\n                if not self.trees[conversation_id][\"tree\"]._config_modified:\n                    self.trees[conversation_id][\"tree\"].change_style(style)\n                    self.trees[conversation_id][\"tree\"]._config_modified = False\n        else:\n            if conversation_id not in self.trees:\n                raise ValueError(f\"Tree {conversation_id} not found\")\n\n            self.trees[conversation_id][\"tree\"].change_style(style)\n\n    def change_agent_description(\n        self, agent_description: str, conversation_id: str | None = None\n    ):\n        \"\"\"\n        Change the agent description for a tree in the TreeManager.\n        Or change the global agent description for all trees (if conversation_id is not supplied).\n        And applies these changes to current trees with default settings.\n\n        Args:\n            agent_description (str): The new agent description for the tree.\n            conversation_id (str | None): Optional. The conversation ID which contains the tree.\n                If not supplied, the agent description will be changed on all trees.\n        \"\"\"\n        if conversation_id is None:\n            self.config.agent_description = agent_description\n            for conversation_id in self.trees:\n                if not self.trees[conversation_id][\"tree\"]._config_modified:\n                    self.trees[conversation_id][\"tree\"].change_agent_description(\n                        agent_description\n                    )\n                    self.trees[conversation_id][\"tree\"]._config_modified = False\n        else:\n            if conversation_id not in self.trees:\n                raise ValueError(f\"Tree {conversation_id} not found\")\n\n            self.trees[conversation_id][\"tree\"].change_agent_description(\n                agent_description\n            )\n\n    def change_end_goal(self, end_goal: str, conversation_id: str | None = None):\n        \"\"\"\n        Change the end goal for a tree in the TreeManager.\n        Or change the global end goal for all trees (if conversation_id is not supplied).\n        And applies these changes to current trees with default settings.\n\n        Args:\n            end_goal (str): The new end goal for the tree.\n            conversation_id (str | None): Optional. The conversation ID which contains the tree.\n                If not supplied, the end goal will be changed on all trees.\n        \"\"\"\n        if conversation_id is None:\n            self.config.end_goal = end_goal\n            for conversation_id in self.trees:\n                if not self.trees[conversation_id][\"tree\"]._config_modified:\n                    self.trees[conversation_id][\"tree\"].change_end_goal(end_goal)\n                    self.trees[conversation_id][\"tree\"]._config_modified = False\n        else:\n            if conversation_id not in self.trees:\n                raise ValueError(f\"Tree {conversation_id} not found\")\n\n            self.trees[conversation_id][\"tree\"].change_end_goal(end_goal)\n\n    def change_branch_initialisation(\n        self, branch_initialisation: BranchInitType, conversation_id: str | None = None\n    ):\n        \"\"\"\n        Change the branch initialisation for a tree in the TreeManager.\n        Or change the global branch initialisation for all trees (if conversation_id is not supplied).\n        And applies these changes to current trees with default settings.\n\n        Args:\n            branch_initialisation (str): The new branch initialisation for the tree.\n            conversation_id (str | None): Optional. The conversation ID which contains the tree.\n                If not supplied, the branch initialisation will be changed on all trees.\n        \"\"\"\n        if conversation_id is None:\n            self.config.branch_initialisation = branch_initialisation\n            for conversation_id in self.trees:\n                if not self.trees[conversation_id][\"tree\"]._config_modified:\n                    self.trees[conversation_id][\"tree\"].set_branch_initialisation(\n                        branch_initialisation\n                    )\n                    self.trees[conversation_id][\"tree\"]._config_modified = False\n        else:\n            if conversation_id not in self.trees:\n                raise ValueError(f\"Tree {conversation_id} not found\")\n\n            self.trees[conversation_id][\"tree\"].set_branch_initialisation(\n                branch_initialisation\n            )\n\n    async def process_tree(\n        self,\n        query: str,\n        conversation_id: str,\n        query_id: str | None = None,\n        training_route: str = \"\",\n        collection_names: list[str] = [],\n        client_manager: ClientManager | None = None,\n    ):\n        \"\"\"\n        Process a tree in the TreeManager.\n        This is an async generator which yields results from the tree.async_run() method.\n\n        Args:\n            query (str): Required. The user input/prompt to process in the decision tree.\n            conversation_id (str): Required. The conversation ID which contains the tree.\n            query_id (str): Optional. A unique identifier for the query.\n                If not supplied, a random UUID will be generated.\n            training_route (str): Optional. The training route, a string of the form \"tool1/tool2/tool1\" etc.\n                See the `tree.async_run()` method for more details.\n            collection_names (list[str]): Optional. A list of collection names to use in the query.\n                If not supplied, all collections will be used.\n            client_manager (ClientManager | None): Optional. A client manager to use for the query.\n                If not supplied, a new ClientManager will be created.\n        \"\"\"\n\n        if query_id is None:\n            query_id = str(uuid.uuid4())\n\n        tree: Tree = self.get_tree(conversation_id)\n        self.update_tree_last_request(conversation_id)\n\n        # wait for the tree to be idle\n        await self.trees[conversation_id][\"event\"].wait()\n\n        # clear the event, set it to working\n        self.trees[conversation_id][\"event\"].clear()\n\n        try:\n            async for yielded_result in tree.async_run(\n                query,\n                collection_names=collection_names,\n                client_manager=client_manager,\n                query_id=query_id,\n                training_route=training_route,\n                close_clients_after_completion=False,\n            ):\n                yield yielded_result\n                self.update_tree_last_request(conversation_id)\n\n        finally:\n            # set the event to idle\n            self.trees[conversation_id][\"event\"].set()\n\n    def check_tree_timeout(self, conversation_id: str):\n        \"\"\"\n        Check if a tree has been idle for the last tree_timeout.\n\n        Args:\n            conversation_id (str): The conversation ID which contains the tree.\n\n        Returns:\n            (bool): True if the tree has been idle for the last tree_timeout, False otherwise.\n        \"\"\"\n        # if tree not found, return True\n        if conversation_id not in self.trees:\n            return True\n\n        # Remove any trees that have not been active in the last TREE_TIMEOUT minutes\n        if (\n            \"last_request\" in self.trees[conversation_id]\n            and datetime.datetime.now() - self.trees[conversation_id][\"last_request\"]\n            &gt; self.tree_timeout\n        ):\n            return True\n\n        return False\n\n    def update_tree_last_request(self, conversation_id: str):\n        self.trees[conversation_id][\"last_request\"] = datetime.datetime.now()\n\n    def check_all_trees_timeout(self):\n        \"\"\"\n        Check all trees in the TreeManager and remove any that have not been active in the last tree_timeout.\n        \"\"\"\n        if self.tree_timeout == datetime.timedelta(minutes=0):\n            return\n\n        convs_to_remove = []\n        for i, conversation_id in enumerate(self.trees):\n            if self.check_tree_timeout(conversation_id):\n                convs_to_remove.append(conversation_id)\n\n        for conversation_id in convs_to_remove:\n            del self.trees[conversation_id]\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.__init__","title":"<code>__init__(user_id, config=None, tree_timeout=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>Required. A unique identifier for the user being managed within this TreeManager.</p> required <code>config</code> <code>Config | None</code> <p>Optional. A config for all trees managed by this TreeManager. Defaults to a new config with default settings. If <code>settings</code> is not provided, it will be set to the default settings (smart_setup).</p> <code>None</code> <code>tree_timeout</code> <code>timedelta | int | None</code> <p>Optional. A timeout for all trees managed by this TreeManager. Defaults to the value of the <code>TREE_TIMEOUT</code> environment variable. If an integer is passed, it will be interpreted as minutes. If set to 0, trees will not be automatically removed.</p> <code>None</code> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def __init__(\n    self,\n    user_id: str,\n    config: Config | None = None,\n    tree_timeout: datetime.timedelta | int | None = None,\n):\n    \"\"\"\n    Args:\n        user_id (str): Required. A unique identifier for the user being managed within this TreeManager.\n        config (Config | None): Optional. A config for all trees managed by this TreeManager.\n            Defaults to a new config with default settings.\n            If `settings` is not provided, it will be set to the default settings (smart_setup).\n        tree_timeout (datetime.timedelta | int | None): Optional. A timeout for all trees managed by this TreeManager.\n            Defaults to the value of the `TREE_TIMEOUT` environment variable.\n            If an integer is passed, it will be interpreted as minutes.\n            If set to 0, trees will not be automatically removed.\n    \"\"\"\n    self.trees = {}\n    self.user_id = user_id\n\n    if tree_timeout is None:\n        self.tree_timeout = datetime.timedelta(\n            minutes=int(os.environ.get(\"TREE_TIMEOUT\", 10))\n        )\n    elif isinstance(tree_timeout, int):\n        self.tree_timeout = datetime.timedelta(minutes=tree_timeout)\n    else:\n        self.tree_timeout = tree_timeout\n\n    if config is None:\n        self.config = Config()\n    else:\n        self.config = config\n\n    self.settings = self.config.settings\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.add_tree","title":"<code>add_tree(conversation_id, low_memory=False)</code>","text":"<p>Add a tree to the TreeManager. The decision tree can be initialised with specific config options, such as style, agent description and end goal. As well as a Settings object, which chooses options such as the LLM models, API keys and more.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>Required. A unique identifier for the conversation.</p> required <code>low_memory</code> <code>bool</code> <p>Optional. Whether to use low memory mode for the tree. Controls the LM history being saved in the tree, and some other variables. Defaults to False.</p> <code>False</code> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def add_tree(\n    self,\n    conversation_id: str,\n    low_memory: bool = False,\n):\n    \"\"\"\n    Add a tree to the TreeManager.\n    The decision tree can be initialised with specific config options, such as style, agent description and end goal.\n    As well as a Settings object, which chooses options such as the LLM models, API keys and more.\n\n    Args:\n        conversation_id (str): Required. A unique identifier for the conversation.\n        low_memory (bool): Optional. Whether to use low memory mode for the tree.\n            Controls the LM history being saved in the tree, and some other variables.\n            Defaults to False.\n    \"\"\"\n\n    if not self.tree_exists(conversation_id):\n        self.trees[conversation_id] = {\n            \"tree\": Tree(\n                conversation_id=conversation_id,\n                user_id=self.user_id,\n                settings=self.settings,\n                style=self.config.style,\n                agent_description=self.config.agent_description,\n                end_goal=self.config.end_goal,\n                branch_initialisation=self.config.branch_initialisation,\n                low_memory=low_memory,\n                use_elysia_collections=self.config.use_elysia_collections,\n            ),\n            \"last_request\": datetime.datetime.now(),\n            \"event\": asyncio.Event(),\n        }\n        self.trees[conversation_id][\"event\"].set()\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.change_agent_description","title":"<code>change_agent_description(agent_description, conversation_id=None)</code>","text":"<p>Change the agent description for a tree in the TreeManager. Or change the global agent description for all trees (if conversation_id is not supplied). And applies these changes to current trees with default settings.</p> <p>Parameters:</p> Name Type Description Default <code>agent_description</code> <code>str</code> <p>The new agent description for the tree.</p> required <code>conversation_id</code> <code>str | None</code> <p>Optional. The conversation ID which contains the tree. If not supplied, the agent description will be changed on all trees.</p> <code>None</code> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def change_agent_description(\n    self, agent_description: str, conversation_id: str | None = None\n):\n    \"\"\"\n    Change the agent description for a tree in the TreeManager.\n    Or change the global agent description for all trees (if conversation_id is not supplied).\n    And applies these changes to current trees with default settings.\n\n    Args:\n        agent_description (str): The new agent description for the tree.\n        conversation_id (str | None): Optional. The conversation ID which contains the tree.\n            If not supplied, the agent description will be changed on all trees.\n    \"\"\"\n    if conversation_id is None:\n        self.config.agent_description = agent_description\n        for conversation_id in self.trees:\n            if not self.trees[conversation_id][\"tree\"]._config_modified:\n                self.trees[conversation_id][\"tree\"].change_agent_description(\n                    agent_description\n                )\n                self.trees[conversation_id][\"tree\"]._config_modified = False\n    else:\n        if conversation_id not in self.trees:\n            raise ValueError(f\"Tree {conversation_id} not found\")\n\n        self.trees[conversation_id][\"tree\"].change_agent_description(\n            agent_description\n        )\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.change_branch_initialisation","title":"<code>change_branch_initialisation(branch_initialisation, conversation_id=None)</code>","text":"<p>Change the branch initialisation for a tree in the TreeManager. Or change the global branch initialisation for all trees (if conversation_id is not supplied). And applies these changes to current trees with default settings.</p> <p>Parameters:</p> Name Type Description Default <code>branch_initialisation</code> <code>str</code> <p>The new branch initialisation for the tree.</p> required <code>conversation_id</code> <code>str | None</code> <p>Optional. The conversation ID which contains the tree. If not supplied, the branch initialisation will be changed on all trees.</p> <code>None</code> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def change_branch_initialisation(\n    self, branch_initialisation: BranchInitType, conversation_id: str | None = None\n):\n    \"\"\"\n    Change the branch initialisation for a tree in the TreeManager.\n    Or change the global branch initialisation for all trees (if conversation_id is not supplied).\n    And applies these changes to current trees with default settings.\n\n    Args:\n        branch_initialisation (str): The new branch initialisation for the tree.\n        conversation_id (str | None): Optional. The conversation ID which contains the tree.\n            If not supplied, the branch initialisation will be changed on all trees.\n    \"\"\"\n    if conversation_id is None:\n        self.config.branch_initialisation = branch_initialisation\n        for conversation_id in self.trees:\n            if not self.trees[conversation_id][\"tree\"]._config_modified:\n                self.trees[conversation_id][\"tree\"].set_branch_initialisation(\n                    branch_initialisation\n                )\n                self.trees[conversation_id][\"tree\"]._config_modified = False\n    else:\n        if conversation_id not in self.trees:\n            raise ValueError(f\"Tree {conversation_id} not found\")\n\n        self.trees[conversation_id][\"tree\"].set_branch_initialisation(\n            branch_initialisation\n        )\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.change_end_goal","title":"<code>change_end_goal(end_goal, conversation_id=None)</code>","text":"<p>Change the end goal for a tree in the TreeManager. Or change the global end goal for all trees (if conversation_id is not supplied). And applies these changes to current trees with default settings.</p> <p>Parameters:</p> Name Type Description Default <code>end_goal</code> <code>str</code> <p>The new end goal for the tree.</p> required <code>conversation_id</code> <code>str | None</code> <p>Optional. The conversation ID which contains the tree. If not supplied, the end goal will be changed on all trees.</p> <code>None</code> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def change_end_goal(self, end_goal: str, conversation_id: str | None = None):\n    \"\"\"\n    Change the end goal for a tree in the TreeManager.\n    Or change the global end goal for all trees (if conversation_id is not supplied).\n    And applies these changes to current trees with default settings.\n\n    Args:\n        end_goal (str): The new end goal for the tree.\n        conversation_id (str | None): Optional. The conversation ID which contains the tree.\n            If not supplied, the end goal will be changed on all trees.\n    \"\"\"\n    if conversation_id is None:\n        self.config.end_goal = end_goal\n        for conversation_id in self.trees:\n            if not self.trees[conversation_id][\"tree\"]._config_modified:\n                self.trees[conversation_id][\"tree\"].change_end_goal(end_goal)\n                self.trees[conversation_id][\"tree\"]._config_modified = False\n    else:\n        if conversation_id not in self.trees:\n            raise ValueError(f\"Tree {conversation_id} not found\")\n\n        self.trees[conversation_id][\"tree\"].change_end_goal(end_goal)\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.change_style","title":"<code>change_style(style, conversation_id=None)</code>","text":"<p>Change the style for a tree in the TreeManager. Or change the global style for all trees (if conversation_id is not supplied). And applies these changes to current trees with default settings.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>str</code> <p>The new style for the tree.</p> required <code>conversation_id</code> <code>str | None</code> <p>Optional. The conversation ID which contains the tree. If not supplied, the style will be changed on all trees.</p> <code>None</code> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def change_style(self, style: str, conversation_id: str | None = None):\n    \"\"\"\n    Change the style for a tree in the TreeManager.\n    Or change the global style for all trees (if conversation_id is not supplied).\n    And applies these changes to current trees with default settings.\n\n    Args:\n        style (str): The new style for the tree.\n        conversation_id (str | None): Optional. The conversation ID which contains the tree.\n            If not supplied, the style will be changed on all trees.\n    \"\"\"\n    if conversation_id is None:\n        self.config.style = style\n        for conversation_id in self.trees:\n            if not self.trees[conversation_id][\"tree\"]._config_modified:\n                self.trees[conversation_id][\"tree\"].change_style(style)\n                self.trees[conversation_id][\"tree\"]._config_modified = False\n    else:\n        if conversation_id not in self.trees:\n            raise ValueError(f\"Tree {conversation_id} not found\")\n\n        self.trees[conversation_id][\"tree\"].change_style(style)\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.check_all_trees_timeout","title":"<code>check_all_trees_timeout()</code>","text":"<p>Check all trees in the TreeManager and remove any that have not been active in the last tree_timeout.</p> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def check_all_trees_timeout(self):\n    \"\"\"\n    Check all trees in the TreeManager and remove any that have not been active in the last tree_timeout.\n    \"\"\"\n    if self.tree_timeout == datetime.timedelta(minutes=0):\n        return\n\n    convs_to_remove = []\n    for i, conversation_id in enumerate(self.trees):\n        if self.check_tree_timeout(conversation_id):\n            convs_to_remove.append(conversation_id)\n\n    for conversation_id in convs_to_remove:\n        del self.trees[conversation_id]\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.check_tree_exists_weaviate","title":"<code>check_tree_exists_weaviate(conversation_id, client_manager)</code>  <code>async</code>","text":"<p>Check if a tree exists in a Weaviate instance. The collection ELYSIA_TREES__ must exist, returns False if it doesn't.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>The conversation ID which contains the tree.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use for the tree.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the tree exists in the Weaviate instance, False otherwise.</p> Source code in <code>elysia/api/services/tree.py</code> <pre><code>async def check_tree_exists_weaviate(\n    self, conversation_id: str, client_manager: ClientManager\n):\n    \"\"\"\n    Check if a tree exists in a Weaviate instance.\n    The collection ELYSIA_TREES__ must exist, returns False if it doesn't.\n\n    Args:\n        conversation_id (str): The conversation ID which contains the tree.\n        client_manager (ClientManager): The client manager to use for the tree.\n\n    Returns:\n        (bool): True if the tree exists in the Weaviate instance, False otherwise.\n    \"\"\"\n    async with client_manager.connect_to_async_client() as client:\n        if not await client.collections.exists(\"ELYSIA_TREES__\"):\n            return False\n\n        collection = client.collections.get(\"ELYSIA_TREES__\")\n        uuid = generate_uuid5(conversation_id)\n        return await collection.data.exists(uuid)\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.check_tree_timeout","title":"<code>check_tree_timeout(conversation_id)</code>","text":"<p>Check if a tree has been idle for the last tree_timeout.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>The conversation ID which contains the tree.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the tree has been idle for the last tree_timeout, False otherwise.</p> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def check_tree_timeout(self, conversation_id: str):\n    \"\"\"\n    Check if a tree has been idle for the last tree_timeout.\n\n    Args:\n        conversation_id (str): The conversation ID which contains the tree.\n\n    Returns:\n        (bool): True if the tree has been idle for the last tree_timeout, False otherwise.\n    \"\"\"\n    # if tree not found, return True\n    if conversation_id not in self.trees:\n        return True\n\n    # Remove any trees that have not been active in the last TREE_TIMEOUT minutes\n    if (\n        \"last_request\" in self.trees[conversation_id]\n        and datetime.datetime.now() - self.trees[conversation_id][\"last_request\"]\n        &gt; self.tree_timeout\n    ):\n        return True\n\n    return False\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.configure","title":"<code>configure(conversation_id=None, replace=False, **kwargs)</code>","text":"<p>Configure the settings for a tree in the TreeManager.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str | None</code> <p>The conversation ID which contains the tree.</p> <code>None</code> <code>replace</code> <code>bool</code> <p>Whether to override the current settings with the new settings. When this is True, all existing settings are removed, and only the new settings are used. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the Settings.configure() method.</p> <code>{}</code> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def configure(\n    self, conversation_id: str | None = None, replace: bool = False, **kwargs: Any\n):\n    \"\"\"\n    Configure the settings for a tree in the TreeManager.\n\n    Args:\n        conversation_id (str | None): The conversation ID which contains the tree.\n        replace (bool): Whether to override the current settings with the new settings.\n            When this is True, all existing settings are removed, and only the new settings are used.\n            Defaults to False.\n        **kwargs (Any): The keyword arguments to pass to the Settings.configure() method.\n    \"\"\"\n    if conversation_id is None:\n        self.settings.configure(replace=replace, **kwargs)\n        self.config.settings = self.settings\n    else:\n        self.get_tree(conversation_id).settings.configure(replace=replace, **kwargs)\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.delete_tree_local","title":"<code>delete_tree_local(conversation_id)</code>","text":"<p>Delete a tree from the TreeManager.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>The conversation ID of the tree to be deleted.</p> required Source code in <code>elysia/api/services/tree.py</code> <pre><code>def delete_tree_local(self, conversation_id: str):\n    \"\"\"\n    Delete a tree from the TreeManager.\n\n    Args:\n        conversation_id (str): The conversation ID of the tree to be deleted.\n    \"\"\"\n    if conversation_id in self.trees:\n        del self.trees[conversation_id]\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.delete_tree_weaviate","title":"<code>delete_tree_weaviate(conversation_id, client_manager)</code>  <code>async</code>","text":"<p>Delete a tree from the stored trees in Weaviate.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>The conversation ID of the tree to be deleted.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager pointing to the Weaviate instance containing the tree.</p> required Source code in <code>elysia/api/services/tree.py</code> <pre><code>async def delete_tree_weaviate(\n    self, conversation_id: str, client_manager: ClientManager\n):\n    \"\"\"\n    Delete a tree from the stored trees in Weaviate.\n\n    Args:\n        conversation_id (str): The conversation ID of the tree to be deleted.\n        client_manager (ClientManager): The client manager pointing to the Weaviate instance containing the tree.\n    \"\"\"\n    await delete_tree_from_weaviate(\n        conversation_id, \"ELYSIA_TREES__\", client_manager\n    )\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.get_event","title":"<code>get_event(conversation_id)</code>","text":"<p>Get the asyncio.Event for a tree in the TreeManager. This is cleared when the tree is processing, and set when the tree is idle. This is used to block the API from sending multiple requests to the tree at once.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>The conversation ID which contains the tree.</p> required <p>Returns:</p> Type Description <code>Event</code> <p>The event for the tree.</p> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def get_event(self, conversation_id: str):\n    \"\"\"\n    Get the asyncio.Event for a tree in the TreeManager.\n    This is cleared when the tree is processing, and set when the tree is idle.\n    This is used to block the API from sending multiple requests to the tree at once.\n\n    Args:\n        conversation_id (str): The conversation ID which contains the tree.\n\n    Returns:\n        (asyncio.Event): The event for the tree.\n    \"\"\"\n    return self.trees[conversation_id][\"event\"]\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.get_tree","title":"<code>get_tree(conversation_id)</code>","text":"<p>Get a tree from the TreeManager. Will raise a ValueError if the tree is not found.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>The conversation ID which contains the tree.</p> required <p>Returns:</p> Type Description <code>Tree</code> <p>The tree associated with the conversation ID.</p> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def get_tree(self, conversation_id: str):\n    \"\"\"\n    Get a tree from the TreeManager.\n    Will raise a ValueError if the tree is not found.\n\n    Args:\n        conversation_id (str): The conversation ID which contains the tree.\n\n    Returns:\n        (Tree): The tree associated with the conversation ID.\n    \"\"\"\n    if conversation_id not in self.trees:\n        raise ValueError(\n            f\"Tree {conversation_id} not found. Please initialise a tree first (by calling `add_tree`).\"\n        )\n\n    return self.trees[conversation_id][\"tree\"]\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.load_tree_weaviate","title":"<code>load_tree_weaviate(conversation_id, client_manager)</code>  <code>async</code>","text":"<p>Load a tree from Weaviate. The conversation ID from the loaded tree is placed into the tree manager (possibly overwriting an existing tree with the same conversation ID). Then the tree itself is not returned - instead the list of frontend payloads that were yielded to the frontend by the tree is returned.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>The conversation ID which contains the tree.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use for the tree.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of dictionaries, each containing a frontend payload that was used to generate the tree. The list is ordered by the time the payload was originally sent to the frontend (at the time it was saved).</p> Source code in <code>elysia/api/services/tree.py</code> <pre><code>async def load_tree_weaviate(\n    self, conversation_id: str, client_manager: ClientManager\n):\n    \"\"\"\n    Load a tree from Weaviate.\n    The conversation ID from the loaded tree is placed into the tree manager\n    (possibly overwriting an existing tree with the same conversation ID).\n    Then the tree itself is not returned - instead the list of frontend payloads\n    that were yielded to the frontend by the tree is returned.\n\n    Args:\n        conversation_id (str): The conversation ID which contains the tree.\n        client_manager (ClientManager): The client manager to use for the tree.\n\n    Returns:\n        (list): A list of dictionaries, each containing a frontend payload that was used to generate the tree.\n            The list is ordered by the time the payload was originally sent to the frontend (at the time it was saved).\n    \"\"\"\n    tree = await Tree.import_from_weaviate(\n        \"ELYSIA_TREES__\", conversation_id, client_manager\n    )\n    if conversation_id not in self.trees:\n        self.trees[conversation_id] = {\n            \"tree\": None,\n            \"event\": asyncio.Event(),\n            \"last_request\": datetime.datetime.now(),\n        }\n    self.trees[conversation_id][\"tree\"] = tree\n    self.trees[conversation_id][\"event\"].set()\n    self.update_tree_last_request(conversation_id)\n    return tree.returner.store\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.process_tree","title":"<code>process_tree(query, conversation_id, query_id=None, training_route='', collection_names=[], client_manager=None)</code>  <code>async</code>","text":"<p>Process a tree in the TreeManager. This is an async generator which yields results from the tree.async_run() method.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Required. The user input/prompt to process in the decision tree.</p> required <code>conversation_id</code> <code>str</code> <p>Required. The conversation ID which contains the tree.</p> required <code>query_id</code> <code>str</code> <p>Optional. A unique identifier for the query. If not supplied, a random UUID will be generated.</p> <code>None</code> <code>training_route</code> <code>str</code> <p>Optional. The training route, a string of the form \"tool1/tool2/tool1\" etc. See the <code>tree.async_run()</code> method for more details.</p> <code>''</code> <code>collection_names</code> <code>list[str]</code> <p>Optional. A list of collection names to use in the query. If not supplied, all collections will be used.</p> <code>[]</code> <code>client_manager</code> <code>ClientManager | None</code> <p>Optional. A client manager to use for the query. If not supplied, a new ClientManager will be created.</p> <code>None</code> Source code in <code>elysia/api/services/tree.py</code> <pre><code>async def process_tree(\n    self,\n    query: str,\n    conversation_id: str,\n    query_id: str | None = None,\n    training_route: str = \"\",\n    collection_names: list[str] = [],\n    client_manager: ClientManager | None = None,\n):\n    \"\"\"\n    Process a tree in the TreeManager.\n    This is an async generator which yields results from the tree.async_run() method.\n\n    Args:\n        query (str): Required. The user input/prompt to process in the decision tree.\n        conversation_id (str): Required. The conversation ID which contains the tree.\n        query_id (str): Optional. A unique identifier for the query.\n            If not supplied, a random UUID will be generated.\n        training_route (str): Optional. The training route, a string of the form \"tool1/tool2/tool1\" etc.\n            See the `tree.async_run()` method for more details.\n        collection_names (list[str]): Optional. A list of collection names to use in the query.\n            If not supplied, all collections will be used.\n        client_manager (ClientManager | None): Optional. A client manager to use for the query.\n            If not supplied, a new ClientManager will be created.\n    \"\"\"\n\n    if query_id is None:\n        query_id = str(uuid.uuid4())\n\n    tree: Tree = self.get_tree(conversation_id)\n    self.update_tree_last_request(conversation_id)\n\n    # wait for the tree to be idle\n    await self.trees[conversation_id][\"event\"].wait()\n\n    # clear the event, set it to working\n    self.trees[conversation_id][\"event\"].clear()\n\n    try:\n        async for yielded_result in tree.async_run(\n            query,\n            collection_names=collection_names,\n            client_manager=client_manager,\n            query_id=query_id,\n            training_route=training_route,\n            close_clients_after_completion=False,\n        ):\n            yield yielded_result\n            self.update_tree_last_request(conversation_id)\n\n    finally:\n        # set the event to idle\n        self.trees[conversation_id][\"event\"].set()\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.save_tree_weaviate","title":"<code>save_tree_weaviate(conversation_id, client_manager)</code>  <code>async</code>","text":"<p>Save a tree to Weaviate to collection ELYSIA_TREES__. Creates the collection if it doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>The conversation ID which contains the tree.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use for the tree.</p> required Source code in <code>elysia/api/services/tree.py</code> <pre><code>async def save_tree_weaviate(\n    self, conversation_id: str, client_manager: ClientManager\n):\n    \"\"\"\n    Save a tree to Weaviate to collection ELYSIA_TREES__.\n    Creates the collection if it doesn't exist.\n\n    Args:\n        conversation_id (str): The conversation ID which contains the tree.\n        client_manager (ClientManager): The client manager to use for the tree.\n    \"\"\"\n    tree: Tree = self.get_tree(conversation_id)\n    await tree.export_to_weaviate(\"ELYSIA_TREES__\", client_manager)\n</code></pre>"},{"location":"Reference/Managers/#elysia.api.services.tree.TreeManager.tree_exists","title":"<code>tree_exists(conversation_id)</code>","text":"<p>Check if a tree exists in the TreeManager.</p> <p>Parameters:</p> Name Type Description Default <code>conversation_id</code> <code>str</code> <p>The conversation ID which may contain the tree.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the tree exists, False otherwise.</p> Source code in <code>elysia/api/services/tree.py</code> <pre><code>def tree_exists(self, conversation_id: str):\n    \"\"\"\n    Check if a tree exists in the TreeManager.\n\n    Args:\n        conversation_id (str): The conversation ID which may contain the tree.\n\n    Returns:\n        (bool): True if the tree exists, False otherwise.\n    \"\"\"\n    return conversation_id in self.trees\n</code></pre>"},{"location":"Reference/Objects/","title":"Objects","text":""},{"location":"Reference/Objects/#elysia.objects.Completed","title":"<code>Completed</code>","text":"<p>               Bases: <code>Update</code></p> <p>Completed message to be sent to the frontend (tree is complete all recursions).</p> Source code in <code>elysia/objects.py</code> <pre><code>class Completed(Update):\n    \"\"\"\n    Completed message to be sent to the frontend (tree is complete all recursions).\n    \"\"\"\n\n    def __init__(self):\n        Update.__init__(self, \"completed\", {})\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Error","title":"<code>Error</code>","text":"<p>               Bases: <code>Update</code></p> <p>Error objects are used to communicate errors to the decision agent/tool calls. When yielded, Error objects are automatically saved inside the TreeData object. When calling the same tool again, the saved Error object is automatically loaded into any tool calls made with the same tool name. All errors are shown to the decision agent to help decide whether the tool should be called again (retried), or a different tool should be called.</p> Source code in <code>elysia/objects.py</code> <pre><code>class Error(Update):\n    \"\"\"\n    Error objects are used to communicate errors to the decision agent/tool calls.\n    When yielded, Error objects are automatically saved inside the TreeData object.\n    When calling the same tool again, the saved Error object is automatically loaded into any tool calls made with the same tool name.\n    All errors are shown to the decision agent to help decide whether the tool should be called again (retried), or a different tool should be called.\n    \"\"\"\n\n    def __init__(self, feedback: str = \"\", error_message: str = \"\"):\n        \"\"\"\n        Args:\n            feedback (str): The feedback to display to the decision agent.\n                Usually this will be the error message, but it could be formatted more specifically.\n        \"\"\"\n        self.feedback = feedback\n        self.error_message = error_message\n\n        if feedback == \"\":\n            self.feedback = \"An unknown issue occurred.\"\n\n        Update.__init__(\n            self,\n            \"self_healing_error\",\n            {\"feedback\": self.feedback, \"error_message\": self.error_message},\n        )\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Error.__init__","title":"<code>__init__(feedback='', error_message='')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>str</code> <p>The feedback to display to the decision agent. Usually this will be the error message, but it could be formatted more specifically.</p> <code>''</code> Source code in <code>elysia/objects.py</code> <pre><code>def __init__(self, feedback: str = \"\", error_message: str = \"\"):\n    \"\"\"\n    Args:\n        feedback (str): The feedback to display to the decision agent.\n            Usually this will be the error message, but it could be formatted more specifically.\n    \"\"\"\n    self.feedback = feedback\n    self.error_message = error_message\n\n    if feedback == \"\":\n        self.feedback = \"An unknown issue occurred.\"\n\n    Update.__init__(\n        self,\n        \"self_healing_error\",\n        {\"feedback\": self.feedback, \"error_message\": self.error_message},\n    )\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Result","title":"<code>Result</code>","text":"<p>               Bases: <code>Return</code></p> <p>Result objects are returned to the frontend. These are displayed on the frontend. E.g. a table, a chart, a text response, etc.</p> <p>You can yield a <code>Result</code> directly, and specify the <code>type</code> and <code>name</code> of the result.</p> Source code in <code>elysia/objects.py</code> <pre><code>class Result(Return):\n    \"\"\"\n    Result objects are returned to the frontend.\n    These are displayed on the frontend.\n    E.g. a table, a chart, a text response, etc.\n\n    You can yield a `Result` directly, and specify the `type` and `name` of the result.\n    \"\"\"\n\n    def __init__(\n        self,\n        objects: list[dict],\n        metadata: dict = {},\n        payload_type: str = \"default\",\n        name: str = \"default\",\n        mapping: dict | None = None,\n        llm_message: str | None = None,\n        unmapped_keys: list[str] = [\"_REF_ID\"],\n        display: bool = True,\n    ):\n        \"\"\"\n        Args:\n            objects (list[dict]): The objects attached to this result.\n            payload_type: (str): Identifier for the type of result.\n            metadata (dict): The metadata attached to this result,\n                for example, query used, time taken, any other information not directly within the objects\n            name (str): The name of the result, e.g. this could be the name of the collection queried.\n                Used to index the result in the environment.\n            mapping (dict | None): A mapping of the objects to the frontend.\n                Essentially, if the objects are going to be displayed on the frontend, the frontend has specific keys that it wants the objects to have.\n                This is a dictionary that maps from frontend keys to the object keys.\n            llm_message (str | None): A message to be displayed to the LLM to help it parse the result.\n                You can use the following placeholders that will automatically be replaced with the correct values:\n\n                - {type}: The type of the object\n                - {name}: The name of the object\n                - {num_objects}: The number of objects in the object\n                - {metadata_key}: Any key in the metadata dictionary\n            unmapped_keys (list[str]): A list of keys that are not mapped to the frontend.\n            display (bool): Whether to display the result on the frontend when yielding this object.\n                Defaults to `True`.\n        \"\"\"\n        Return.__init__(self, \"result\", payload_type)\n        self.objects = objects\n        self.metadata = metadata\n        self.name = name\n        self.mapping = mapping\n        self.llm_message = llm_message\n        self.unmapped_keys = unmapped_keys\n        self.display = display\n\n    def __len__(self):\n        return len(self.objects)\n\n    def format_llm_message(self):\n        \"\"\"\n        llm_message is a string that is used to help the LLM parse the output of the tool.\n        It is a placeholder for the actual message that will be displayed to the user.\n\n        You can use the following placeholders:\n\n        - {payload_type}: The type of the object\n        - {name}: The name of the object\n        - {num_objects}: The number of objects in the object\n        - {metadata_key}: Any key in the metadata dictionary\n        \"\"\"\n        if self.llm_message is None:\n            return \"\"\n\n        return self.llm_message.format_map(\n            {\n                \"payload_type\": self.payload_type,\n                \"name\": self.name,\n                \"num_objects\": len(self),\n                **self.metadata,\n            }\n        )\n\n    def do_mapping(self, objects: list[dict]):\n\n        if self.mapping is None:\n            return objects\n\n        output_objects = []\n        for obj in objects:\n            output_objects.append(\n                {\n                    key: obj[self.mapping[key]]\n                    for key in self.mapping\n                    if self.mapping[key] != \"\"\n                }\n            )\n            output_objects[-1].update(\n                {key: obj[key] for key in self.unmapped_keys if key in obj}\n            )\n\n        return output_objects\n\n    def to_json(self, mapping: bool = False):\n        \"\"\"\n        Convert the result to a list of dictionaries.\n\n        Args:\n            mapping (bool): Whether to map the objects to the frontend.\n                This will use the `mapping` dictionary created on initialisation of the `Result` object.\n                If `False`, the objects are returned as is.\n                Defaults to `False`.\n\n        Returns:\n            (list[dict]): A list of dictionaries, which can be serialised to JSON.\n        \"\"\"\n        from elysia.util.parsing import format_dict_to_serialisable\n\n        assert all(\n            isinstance(obj, dict) for obj in self.objects\n        ), \"All objects must be dictionaries\"\n\n        if mapping and self.mapping is not None:\n            output_objects = self.do_mapping(self.objects)\n        else:\n            output_objects = self.objects\n\n        for object in output_objects:\n            format_dict_to_serialisable(object)\n\n        return output_objects\n\n    async def to_frontend(\n        self,\n        user_id: str,\n        conversation_id: str,\n        query_id: str,\n    ):\n        \"\"\"\n        Convert the result to a frontend payload.\n        This is a wrapper around the `to_json` method.\n        But the objects and metadata are inside a `payload` key, which also includes a `type` key,\n        being the frontend identifier for the type of payload being sent.\n        (e.g. `ticket`, `product`, `message`, etc.)\n\n        The outside of the payload also contains the user_id, conversation_id, and query_id.\n\n        Args:\n            user_id (str): The user ID.\n            conversation_id (str): The conversation ID.\n            query_id (str): The query ID.\n\n        Returns:\n            (dict): The frontend payload, which is a dictionary with the following structure:\n                ```python\n                {\n                    \"type\": \"result\",\n                    \"user_id\": str,\n                    \"conversation_id\": str,\n                    \"query_id\": str,\n                    \"id\": str,\n                    \"payload\": dict = {\n                        \"type\": str,\n                        \"objects\": list[dict],\n                        \"metadata\": dict,\n                    },\n                }\n                ```\n        \"\"\"\n        if not self.display:\n            return\n\n        objects = self.to_json(mapping=True)\n        if len(objects) == 0:\n            return\n\n        payload = {\n            \"type\": self.payload_type,\n            \"objects\": objects,\n            \"metadata\": self.metadata,\n        }\n\n        return {\n            \"type\": self.frontend_type,\n            \"user_id\": user_id,\n            \"conversation_id\": conversation_id,\n            \"query_id\": query_id,\n            \"id\": self.frontend_type[:3] + \"-\" + str(uuid.uuid4()),\n            \"payload\": payload,\n        }\n\n    def llm_parse(self):\n        \"\"\"\n        This method is called when the result is displayed to the LLM.\n        It is used to display custom information to the LLM about the result.\n        If `llm_message` was defined, then the llm message is formatted using the placeholders.\n        Otherwise a default message is used.\n\n        Returns:\n            (str): The formatted llm message.\n        \"\"\"\n\n        if self.llm_message is not None:\n            return self.format_llm_message()\n        else:\n            # Default message\n            return f\"Displayed: A {self.payload_type} object with {len(self.objects)} objects.\"\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Result.__init__","title":"<code>__init__(objects, metadata={}, payload_type='default', name='default', mapping=None, llm_message=None, unmapped_keys=['_REF_ID'], display=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>objects</code> <code>list[dict]</code> <p>The objects attached to this result.</p> required <code>payload_type</code> <code>str</code> <p>(str): Identifier for the type of result.</p> <code>'default'</code> <code>metadata</code> <code>dict</code> <p>The metadata attached to this result, for example, query used, time taken, any other information not directly within the objects</p> <code>{}</code> <code>name</code> <code>str</code> <p>The name of the result, e.g. this could be the name of the collection queried. Used to index the result in the environment.</p> <code>'default'</code> <code>mapping</code> <code>dict | None</code> <p>A mapping of the objects to the frontend. Essentially, if the objects are going to be displayed on the frontend, the frontend has specific keys that it wants the objects to have. This is a dictionary that maps from frontend keys to the object keys.</p> <code>None</code> <code>llm_message</code> <code>str | None</code> <p>A message to be displayed to the LLM to help it parse the result. You can use the following placeholders that will automatically be replaced with the correct values:</p> <ul> <li>{type}: The type of the object</li> <li>{name}: The name of the object</li> <li>{num_objects}: The number of objects in the object</li> <li>{metadata_key}: Any key in the metadata dictionary</li> </ul> <code>None</code> <code>unmapped_keys</code> <code>list[str]</code> <p>A list of keys that are not mapped to the frontend.</p> <code>['_REF_ID']</code> <code>display</code> <code>bool</code> <p>Whether to display the result on the frontend when yielding this object. Defaults to <code>True</code>.</p> <code>True</code> Source code in <code>elysia/objects.py</code> <pre><code>def __init__(\n    self,\n    objects: list[dict],\n    metadata: dict = {},\n    payload_type: str = \"default\",\n    name: str = \"default\",\n    mapping: dict | None = None,\n    llm_message: str | None = None,\n    unmapped_keys: list[str] = [\"_REF_ID\"],\n    display: bool = True,\n):\n    \"\"\"\n    Args:\n        objects (list[dict]): The objects attached to this result.\n        payload_type: (str): Identifier for the type of result.\n        metadata (dict): The metadata attached to this result,\n            for example, query used, time taken, any other information not directly within the objects\n        name (str): The name of the result, e.g. this could be the name of the collection queried.\n            Used to index the result in the environment.\n        mapping (dict | None): A mapping of the objects to the frontend.\n            Essentially, if the objects are going to be displayed on the frontend, the frontend has specific keys that it wants the objects to have.\n            This is a dictionary that maps from frontend keys to the object keys.\n        llm_message (str | None): A message to be displayed to the LLM to help it parse the result.\n            You can use the following placeholders that will automatically be replaced with the correct values:\n\n            - {type}: The type of the object\n            - {name}: The name of the object\n            - {num_objects}: The number of objects in the object\n            - {metadata_key}: Any key in the metadata dictionary\n        unmapped_keys (list[str]): A list of keys that are not mapped to the frontend.\n        display (bool): Whether to display the result on the frontend when yielding this object.\n            Defaults to `True`.\n    \"\"\"\n    Return.__init__(self, \"result\", payload_type)\n    self.objects = objects\n    self.metadata = metadata\n    self.name = name\n    self.mapping = mapping\n    self.llm_message = llm_message\n    self.unmapped_keys = unmapped_keys\n    self.display = display\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Result.format_llm_message","title":"<code>format_llm_message()</code>","text":"<p>llm_message is a string that is used to help the LLM parse the output of the tool. It is a placeholder for the actual message that will be displayed to the user.</p> <p>You can use the following placeholders:</p> <ul> <li>{payload_type}: The type of the object</li> <li>{name}: The name of the object</li> <li>{num_objects}: The number of objects in the object</li> <li>{metadata_key}: Any key in the metadata dictionary</li> </ul> Source code in <code>elysia/objects.py</code> <pre><code>def format_llm_message(self):\n    \"\"\"\n    llm_message is a string that is used to help the LLM parse the output of the tool.\n    It is a placeholder for the actual message that will be displayed to the user.\n\n    You can use the following placeholders:\n\n    - {payload_type}: The type of the object\n    - {name}: The name of the object\n    - {num_objects}: The number of objects in the object\n    - {metadata_key}: Any key in the metadata dictionary\n    \"\"\"\n    if self.llm_message is None:\n        return \"\"\n\n    return self.llm_message.format_map(\n        {\n            \"payload_type\": self.payload_type,\n            \"name\": self.name,\n            \"num_objects\": len(self),\n            **self.metadata,\n        }\n    )\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Result.llm_parse","title":"<code>llm_parse()</code>","text":"<p>This method is called when the result is displayed to the LLM. It is used to display custom information to the LLM about the result. If <code>llm_message</code> was defined, then the llm message is formatted using the placeholders. Otherwise a default message is used.</p> <p>Returns:</p> Type Description <code>str</code> <p>The formatted llm message.</p> Source code in <code>elysia/objects.py</code> <pre><code>def llm_parse(self):\n    \"\"\"\n    This method is called when the result is displayed to the LLM.\n    It is used to display custom information to the LLM about the result.\n    If `llm_message` was defined, then the llm message is formatted using the placeholders.\n    Otherwise a default message is used.\n\n    Returns:\n        (str): The formatted llm message.\n    \"\"\"\n\n    if self.llm_message is not None:\n        return self.format_llm_message()\n    else:\n        # Default message\n        return f\"Displayed: A {self.payload_type} object with {len(self.objects)} objects.\"\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Result.to_frontend","title":"<code>to_frontend(user_id, conversation_id, query_id)</code>  <code>async</code>","text":"<p>Convert the result to a frontend payload. This is a wrapper around the <code>to_json</code> method. But the objects and metadata are inside a <code>payload</code> key, which also includes a <code>type</code> key, being the frontend identifier for the type of payload being sent. (e.g. <code>ticket</code>, <code>product</code>, <code>message</code>, etc.)</p> <p>The outside of the payload also contains the user_id, conversation_id, and query_id.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>str</code> <p>The user ID.</p> required <code>conversation_id</code> <code>str</code> <p>The conversation ID.</p> required <code>query_id</code> <code>str</code> <p>The query ID.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The frontend payload, which is a dictionary with the following structure: <pre><code>{\n    \"type\": \"result\",\n    \"user_id\": str,\n    \"conversation_id\": str,\n    \"query_id\": str,\n    \"id\": str,\n    \"payload\": dict = {\n        \"type\": str,\n        \"objects\": list[dict],\n        \"metadata\": dict,\n    },\n}\n</code></pre></p> Source code in <code>elysia/objects.py</code> <pre><code>async def to_frontend(\n    self,\n    user_id: str,\n    conversation_id: str,\n    query_id: str,\n):\n    \"\"\"\n    Convert the result to a frontend payload.\n    This is a wrapper around the `to_json` method.\n    But the objects and metadata are inside a `payload` key, which also includes a `type` key,\n    being the frontend identifier for the type of payload being sent.\n    (e.g. `ticket`, `product`, `message`, etc.)\n\n    The outside of the payload also contains the user_id, conversation_id, and query_id.\n\n    Args:\n        user_id (str): The user ID.\n        conversation_id (str): The conversation ID.\n        query_id (str): The query ID.\n\n    Returns:\n        (dict): The frontend payload, which is a dictionary with the following structure:\n            ```python\n            {\n                \"type\": \"result\",\n                \"user_id\": str,\n                \"conversation_id\": str,\n                \"query_id\": str,\n                \"id\": str,\n                \"payload\": dict = {\n                    \"type\": str,\n                    \"objects\": list[dict],\n                    \"metadata\": dict,\n                },\n            }\n            ```\n    \"\"\"\n    if not self.display:\n        return\n\n    objects = self.to_json(mapping=True)\n    if len(objects) == 0:\n        return\n\n    payload = {\n        \"type\": self.payload_type,\n        \"objects\": objects,\n        \"metadata\": self.metadata,\n    }\n\n    return {\n        \"type\": self.frontend_type,\n        \"user_id\": user_id,\n        \"conversation_id\": conversation_id,\n        \"query_id\": query_id,\n        \"id\": self.frontend_type[:3] + \"-\" + str(uuid.uuid4()),\n        \"payload\": payload,\n    }\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Result.to_json","title":"<code>to_json(mapping=False)</code>","text":"<p>Convert the result to a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>bool</code> <p>Whether to map the objects to the frontend. This will use the <code>mapping</code> dictionary created on initialisation of the <code>Result</code> object. If <code>False</code>, the objects are returned as is. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A list of dictionaries, which can be serialised to JSON.</p> Source code in <code>elysia/objects.py</code> <pre><code>def to_json(self, mapping: bool = False):\n    \"\"\"\n    Convert the result to a list of dictionaries.\n\n    Args:\n        mapping (bool): Whether to map the objects to the frontend.\n            This will use the `mapping` dictionary created on initialisation of the `Result` object.\n            If `False`, the objects are returned as is.\n            Defaults to `False`.\n\n    Returns:\n        (list[dict]): A list of dictionaries, which can be serialised to JSON.\n    \"\"\"\n    from elysia.util.parsing import format_dict_to_serialisable\n\n    assert all(\n        isinstance(obj, dict) for obj in self.objects\n    ), \"All objects must be dictionaries\"\n\n    if mapping and self.mapping is not None:\n        output_objects = self.do_mapping(self.objects)\n    else:\n        output_objects = self.objects\n\n    for object in output_objects:\n        format_dict_to_serialisable(object)\n\n    return output_objects\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Retrieval","title":"<code>Retrieval</code>","text":"<p>               Bases: <code>Result</code></p> <p>Store of returned objects from a query/aggregate/any displayed results.</p> Source code in <code>elysia/objects.py</code> <pre><code>class Retrieval(Result):\n    \"\"\"\n    Store of returned objects from a query/aggregate/any displayed results.\n    \"\"\"\n\n    def __init__(\n        self,\n        objects: list[dict],\n        metadata: dict = {},\n        payload_type: str = \"default\",\n        name: str | None = None,\n        mapping: dict | None = None,\n        unmapped_keys: list[str] = [\n            \"uuid\",\n            \"ELYSIA_SUMMARY\",\n            \"collection_name\",\n            \"_REF_ID\",\n        ],\n        display: bool = True,\n    ) -&gt; None:\n        if name is None and \"collection_name\" in metadata:\n            result_name = metadata[\"collection_name\"]\n        elif name is None:\n            result_name = \"default\"\n        else:\n            result_name = name\n\n        Result.__init__(\n            self,\n            objects=objects,\n            payload_type=payload_type,\n            metadata=metadata,\n            name=result_name,\n            mapping=mapping,\n            unmapped_keys=unmapped_keys,\n            display=display,\n        )\n\n    def add_summaries(self, summaries: list[str] = []) -&gt; None:\n        for i, obj in enumerate(self.objects):\n            if i &lt; len(summaries):\n                obj[\"ELYSIA_SUMMARY\"] = summaries[i]\n            else:\n                obj[\"ELYSIA_SUMMARY\"] = \"\"\n\n    def llm_parse(self) -&gt; str:\n        out = \"\"\n        count = len(self.objects)\n\n        if \"collection_name\" in self.metadata:\n            if count != 0:\n                out += f\"\\nQueried collection: '{self.metadata['collection_name']}' and returned {count} objects, \"\n            else:\n                out += f\"\\nQueried collection: '{self.metadata['collection_name']}' but no objects were returned.\"\n                out += f\" Since it had no objects, judge the query that was created, and evaluate whether it was appropriate for the collection, the user prompt, and the data available.\"\n                out += f\" If it seemed innappropriate, you can choose to try again if you think it can still be completed (or there is more to do).\"\n        if \"return_type\" in self.metadata:\n            out += f\"\\nReturned with type '{self.metadata['return_type']}', \"\n        if \"output_type\" in self.metadata:\n            out += f\"\\noutputted '{'itemised summaries' if self.metadata['output_type'] == 'summary' else 'original objects'}'\\n\"\n        if \"query_text\" in self.metadata:\n            out += f\"\\nSearch terms: '{self.metadata['query_text']}'\"\n        if \"query_type\" in self.metadata:\n            out += f\"\\nType of query: '{self.metadata['query_type']}'\"\n        if \"impossible\" in self.metadata:\n            out += f\"\\nImpossible prompt: '{self.metadata['impossible']}'\"\n            if \"collection_name\" in self.metadata:\n                out += f\"\\nThis attempt at querying the collection: {self.metadata['collection_name']} was deemed impossible.\"\n            if \"impossible_reason\" in self.metadata:\n                out += f\"\\nReasoning for impossibility: {self.metadata['impossible_reason']}\"\n        if \"query_output\" in self.metadata:\n            out += f\"\\nThe query used was:\\n{self.metadata['query_output']}\"\n        return out\n\n    async def to_frontend(\n        self,\n        user_id: str,\n        conversation_id: str,\n        query_id: str,\n    ) -&gt; dict | None:\n        objects = self.to_json(mapping=True)\n        if len(objects) == 0:\n            return\n\n        payload = {\n            \"type\": self.payload_type,\n            \"objects\": objects,\n            \"metadata\": self.metadata,\n        }\n\n        if \"code\" in self.metadata:\n            payload[\"code\"] = self.metadata[\"code\"]\n\n        return {\n            \"type\": self.frontend_type,\n            \"user_id\": user_id,\n            \"conversation_id\": conversation_id,\n            \"query_id\": query_id,\n            \"id\": self.frontend_type[:3] + \"-\" + str(uuid.uuid4()),\n            \"payload\": payload,\n        }\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Return","title":"<code>Return</code>","text":"<p>A Return object is something that is returned to the frontend. This kind of object is frontend-aware.</p> Source code in <code>elysia/objects.py</code> <pre><code>class Return:\n    \"\"\"\n    A Return object is something that is returned to the frontend.\n    This kind of object is frontend-aware.\n    \"\"\"\n\n    def __init__(self, frontend_type: str, payload_type: str):\n        self.frontend_type = frontend_type  # frontend identifier\n        self.payload_type = payload_type  # backend identifier\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Status","title":"<code>Status</code>","text":"<p>               Bases: <code>Update</code></p> <p>Status message to be sent to the frontend for real-time updates in words.</p> Source code in <code>elysia/objects.py</code> <pre><code>class Status(Update):\n    \"\"\"\n    Status message to be sent to the frontend for real-time updates in words.\n    \"\"\"\n\n    def __init__(self, text: str):\n        self.text = text\n        Update.__init__(self, \"status\", {\"text\": text})\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Text","title":"<code>Text</code>","text":"<p>               Bases: <code>Return</code></p> <p>Frontend Return Type 1: Text Objects is usually a one-element list containing a dict with a \"text\" key only. But is not limited to this.</p> Source code in <code>elysia/objects.py</code> <pre><code>class Text(Return):\n    \"\"\"\n    Frontend Return Type 1: Text\n    Objects is usually a one-element list containing a dict with a \"text\" key only.\n    But is not limited to this.\n    \"\"\"\n\n    def __init__(\n        self,\n        payload_type: str,\n        objects: list[dict],\n        metadata: dict = {},\n        display: bool = True,\n    ):\n        Return.__init__(self, \"text\", payload_type)\n        self.objects = objects\n        self.metadata = metadata\n        self.text = self._concat_text(self.objects)\n        self.display = display\n\n    def _concat_text(self, objects: list[dict]):\n        text = \"\"\n        for i, obj in enumerate(objects):\n            if \"text\" in obj:\n                spacer = \"\"\n                if (\n                    not obj[\"text\"].endswith(\" \")\n                    and not obj[\"text\"].endswith(\"\\n\")\n                    and i != len(objects) - 1\n                ):\n                    spacer += \" \"\n\n                if (\n                    i != len(objects) - 1\n                    and \"text\" in objects[i + 1]\n                    and objects[i + 1][\"text\"].startswith(\"* \")\n                    and not obj[\"text\"].endswith(\"\\n\")\n                ):\n                    spacer += \"\\n\"\n\n                text += obj[\"text\"] + spacer\n\n        text = text.replace(\"_REF_ID\", \"\")\n        text = text.replace(\"REF_ID\", \"\")\n        text = text.replace(\"  \", \" \")\n        return text\n\n    def to_json(self):\n        return {\n            \"type\": self.payload_type,\n            \"metadata\": self.metadata,\n            \"objects\": self.objects,\n        }\n\n    async def to_frontend(\n        self,\n        user_id: str,\n        conversation_id: str,\n        query_id: str,\n    ):\n        if not self.display:\n            return\n\n        return {\n            \"type\": self.frontend_type,\n            \"id\": self.frontend_type[:3] + \"-\" + str(uuid.uuid4()),\n            \"user_id\": user_id,\n            \"conversation_id\": conversation_id,\n            \"query_id\": query_id,\n            \"payload\": self.to_json(),\n        }\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Tool","title":"<code>Tool</code>","text":"<p>The generic Tool class, which will be a superclass of any tools used by Elysia.</p> Source code in <code>elysia/objects.py</code> <pre><code>class Tool(metaclass=ToolMeta):\n    \"\"\"\n    The generic Tool class, which will be a superclass of any tools used by Elysia.\n    \"\"\"\n\n    @classmethod\n    def get_metadata(cls) -&gt; dict[str, str | bool | dict | None]:\n        \"\"\"Get tool metadata without instantiation.\"\"\"\n        return {\n            \"name\": getattr(cls, \"_tool_name\", None),\n            \"description\": getattr(cls, \"_tool_description\", None),\n            \"inputs\": getattr(cls, \"_tool_inputs\", None),\n            \"end\": getattr(cls, \"_tool_end\", False),\n        }\n\n    def __init__(\n        self,\n        name: str,\n        description: str,\n        status: str = \"\",\n        inputs: dict = {},\n        end: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            name (str): The name of the tool. Required.\n            description (str): A detailed description of the tool to give to the LLM. Required.\n            status (str): A status update message to display while the tool is running. Optional, defaults to \"Running {name}...\".\n            inputs (dict): A dictionary of inputs for the tool, with the following structure:\n                ```python\n                {\n                    input_name: {\n                        \"description\": str,\n                        \"type\": str,\n                        \"default\": str,\n                        \"required\": bool\n                    }\n                }\n                ```\n            end (bool): Whether the tool is an end tool. Optional, defaults to False.\n        \"\"\"\n        self.name = name\n        self.description = description\n        self.inputs = inputs\n        self.end = end\n\n        if status == \"\":\n            self.status = f\"Running {self.name}...\"\n        else:\n            self.status = status\n\n    def get_default_inputs(self) -&gt; dict:\n        return {\n            key: value[\"default\"] if \"default\" in value else None\n            for key, value in self.inputs.items()\n        }\n\n    async def run_if_true(\n        self,\n        tree_data,\n        base_lm,\n        complex_lm,\n        client_manager,\n    ) -&gt; tuple[bool, dict]:\n        \"\"\"\n        This method is called to check if the tool should be run automatically.\n        If this returns `True`, then the tool is immediately called at the start of the tree.\n        Otherwise it does not appear in the decision tree.\n        You must also return a dictionary of inputs for the tool, which will be used to call the tool if `True` is returned.\n        If the inputs are None or an empty dictionary, then the default inputs will be used.\n\n        This must be an async function.\n\n        Args:\n            tree_data (TreeData): The tree data object.\n            base_lm (LM): The base language model, a dspy.LM object.\n            complex_lm (LM): The complex language model, a dspy.LM object.\n            client_manager (ClientManager): The client manager, a way of interfacing with a Weaviate client.\n\n        Returns:\n            bool: True if the tool should be run automatically, False otherwise.\n            dict: A dictionary of inputs for the tool, with the following structure:\n                ```python\n                {\n                    input_name: input_value\n                }\n                ```\n        \"\"\"\n        return False, {}\n\n    async def is_tool_available(\n        self,\n        tree_data,\n        base_lm,\n        complex_lm,\n        client_manager,\n    ) -&gt; bool:\n        \"\"\"\n        This method is called to check if the tool is available.\n        If this returns `True`, then the tool is available to be used by the LLM.\n        Otherwise it does not appear in the decision tree.\n\n        The difference between this and `run_if_true` is that `run_if_true` will _always_ run the __call__ method if it returns `True`,\n        even if the LLM does not choose to use the tool.\n\n        This must be an async function.\n\n        Args:\n            tree_data (TreeData): The tree data object.\n            base_lm (LM): The base language model, a dspy.LM object.\n            complex_lm (LM): The complex language model, a dspy.LM object.\n            client_manager (ClientManager): The client manager, a way of interfacing with a Weaviate client.\n\n        Returns:\n            bool: True if the tool is available, False otherwise.\n        \"\"\"\n        return True\n\n    async def __call__(\n        self, tree_data, inputs, base_lm, complex_lm, client_manager, **kwargs\n    ) -&gt; AsyncGenerator[Any, None]:\n        \"\"\"\n        This method is called to run the tool.\n\n        This must be an async generator, so objects must be yielded and not returned.\n\n        Args:\n            tree_data (TreeData): The data from the decision tree, includes the environment, user prompt, etc.\n                See the `TreeData` class for more details.\n            base_lm (LM): The base language model, a dspy.LM object.\n            complex_lm (LM): The complex language model, a dspy.LM object.\n            client_manager (ClientManager): The client manager, a way of interfacing with a Weaviate client.\n        \"\"\"\n        yield None\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Tool.__call__","title":"<code>__call__(tree_data, inputs, base_lm, complex_lm, client_manager, **kwargs)</code>  <code>async</code>","text":"<p>This method is called to run the tool.</p> <p>This must be an async generator, so objects must be yielded and not returned.</p> <p>Parameters:</p> Name Type Description Default <code>tree_data</code> <code>TreeData</code> <p>The data from the decision tree, includes the environment, user prompt, etc. See the <code>TreeData</code> class for more details.</p> required <code>base_lm</code> <code>LM</code> <p>The base language model, a dspy.LM object.</p> required <code>complex_lm</code> <code>LM</code> <p>The complex language model, a dspy.LM object.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager, a way of interfacing with a Weaviate client.</p> required Source code in <code>elysia/objects.py</code> <pre><code>async def __call__(\n    self, tree_data, inputs, base_lm, complex_lm, client_manager, **kwargs\n) -&gt; AsyncGenerator[Any, None]:\n    \"\"\"\n    This method is called to run the tool.\n\n    This must be an async generator, so objects must be yielded and not returned.\n\n    Args:\n        tree_data (TreeData): The data from the decision tree, includes the environment, user prompt, etc.\n            See the `TreeData` class for more details.\n        base_lm (LM): The base language model, a dspy.LM object.\n        complex_lm (LM): The complex language model, a dspy.LM object.\n        client_manager (ClientManager): The client manager, a way of interfacing with a Weaviate client.\n    \"\"\"\n    yield None\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Tool.__init__","title":"<code>__init__(name, description, status='', inputs={}, end=False, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the tool. Required.</p> required <code>description</code> <code>str</code> <p>A detailed description of the tool to give to the LLM. Required.</p> required <code>status</code> <code>str</code> <p>A status update message to display while the tool is running. Optional, defaults to \"Running {name}...\".</p> <code>''</code> <code>inputs</code> <code>dict</code> <p>A dictionary of inputs for the tool, with the following structure: <pre><code>{\n    input_name: {\n        \"description\": str,\n        \"type\": str,\n        \"default\": str,\n        \"required\": bool\n    }\n}\n</code></pre></p> <code>{}</code> <code>end</code> <code>bool</code> <p>Whether the tool is an end tool. Optional, defaults to False.</p> <code>False</code> Source code in <code>elysia/objects.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    description: str,\n    status: str = \"\",\n    inputs: dict = {},\n    end: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        name (str): The name of the tool. Required.\n        description (str): A detailed description of the tool to give to the LLM. Required.\n        status (str): A status update message to display while the tool is running. Optional, defaults to \"Running {name}...\".\n        inputs (dict): A dictionary of inputs for the tool, with the following structure:\n            ```python\n            {\n                input_name: {\n                    \"description\": str,\n                    \"type\": str,\n                    \"default\": str,\n                    \"required\": bool\n                }\n            }\n            ```\n        end (bool): Whether the tool is an end tool. Optional, defaults to False.\n    \"\"\"\n    self.name = name\n    self.description = description\n    self.inputs = inputs\n    self.end = end\n\n    if status == \"\":\n        self.status = f\"Running {self.name}...\"\n    else:\n        self.status = status\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Tool.get_metadata","title":"<code>get_metadata()</code>  <code>classmethod</code>","text":"<p>Get tool metadata without instantiation.</p> Source code in <code>elysia/objects.py</code> <pre><code>@classmethod\ndef get_metadata(cls) -&gt; dict[str, str | bool | dict | None]:\n    \"\"\"Get tool metadata without instantiation.\"\"\"\n    return {\n        \"name\": getattr(cls, \"_tool_name\", None),\n        \"description\": getattr(cls, \"_tool_description\", None),\n        \"inputs\": getattr(cls, \"_tool_inputs\", None),\n        \"end\": getattr(cls, \"_tool_end\", False),\n    }\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Tool.is_tool_available","title":"<code>is_tool_available(tree_data, base_lm, complex_lm, client_manager)</code>  <code>async</code>","text":"<p>This method is called to check if the tool is available. If this returns <code>True</code>, then the tool is available to be used by the LLM. Otherwise it does not appear in the decision tree.</p> <p>The difference between this and <code>run_if_true</code> is that <code>run_if_true</code> will always run the call method if it returns <code>True</code>, even if the LLM does not choose to use the tool.</p> <p>This must be an async function.</p> <p>Parameters:</p> Name Type Description Default <code>tree_data</code> <code>TreeData</code> <p>The tree data object.</p> required <code>base_lm</code> <code>LM</code> <p>The base language model, a dspy.LM object.</p> required <code>complex_lm</code> <code>LM</code> <p>The complex language model, a dspy.LM object.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager, a way of interfacing with a Weaviate client.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the tool is available, False otherwise.</p> Source code in <code>elysia/objects.py</code> <pre><code>async def is_tool_available(\n    self,\n    tree_data,\n    base_lm,\n    complex_lm,\n    client_manager,\n) -&gt; bool:\n    \"\"\"\n    This method is called to check if the tool is available.\n    If this returns `True`, then the tool is available to be used by the LLM.\n    Otherwise it does not appear in the decision tree.\n\n    The difference between this and `run_if_true` is that `run_if_true` will _always_ run the __call__ method if it returns `True`,\n    even if the LLM does not choose to use the tool.\n\n    This must be an async function.\n\n    Args:\n        tree_data (TreeData): The tree data object.\n        base_lm (LM): The base language model, a dspy.LM object.\n        complex_lm (LM): The complex language model, a dspy.LM object.\n        client_manager (ClientManager): The client manager, a way of interfacing with a Weaviate client.\n\n    Returns:\n        bool: True if the tool is available, False otherwise.\n    \"\"\"\n    return True\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Tool.run_if_true","title":"<code>run_if_true(tree_data, base_lm, complex_lm, client_manager)</code>  <code>async</code>","text":"<p>This method is called to check if the tool should be run automatically. If this returns <code>True</code>, then the tool is immediately called at the start of the tree. Otherwise it does not appear in the decision tree. You must also return a dictionary of inputs for the tool, which will be used to call the tool if <code>True</code> is returned. If the inputs are None or an empty dictionary, then the default inputs will be used.</p> <p>This must be an async function.</p> <p>Parameters:</p> Name Type Description Default <code>tree_data</code> <code>TreeData</code> <p>The tree data object.</p> required <code>base_lm</code> <code>LM</code> <p>The base language model, a dspy.LM object.</p> required <code>complex_lm</code> <code>LM</code> <p>The complex language model, a dspy.LM object.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager, a way of interfacing with a Weaviate client.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the tool should be run automatically, False otherwise.</p> <code>dict</code> <code>dict</code> <p>A dictionary of inputs for the tool, with the following structure: <pre><code>{\n    input_name: input_value\n}\n</code></pre></p> Source code in <code>elysia/objects.py</code> <pre><code>async def run_if_true(\n    self,\n    tree_data,\n    base_lm,\n    complex_lm,\n    client_manager,\n) -&gt; tuple[bool, dict]:\n    \"\"\"\n    This method is called to check if the tool should be run automatically.\n    If this returns `True`, then the tool is immediately called at the start of the tree.\n    Otherwise it does not appear in the decision tree.\n    You must also return a dictionary of inputs for the tool, which will be used to call the tool if `True` is returned.\n    If the inputs are None or an empty dictionary, then the default inputs will be used.\n\n    This must be an async function.\n\n    Args:\n        tree_data (TreeData): The tree data object.\n        base_lm (LM): The base language model, a dspy.LM object.\n        complex_lm (LM): The complex language model, a dspy.LM object.\n        client_manager (ClientManager): The client manager, a way of interfacing with a Weaviate client.\n\n    Returns:\n        bool: True if the tool should be run automatically, False otherwise.\n        dict: A dictionary of inputs for the tool, with the following structure:\n            ```python\n            {\n                input_name: input_value\n            }\n            ```\n    \"\"\"\n    return False, {}\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.ToolMeta","title":"<code>ToolMeta</code>","text":"<p>               Bases: <code>type</code></p> <p>Metaclass that extracts tool metadata from init method.</p> Source code in <code>elysia/objects.py</code> <pre><code>class ToolMeta(type):\n    \"\"\"Metaclass that extracts tool metadata from __init__ method.\"\"\"\n\n    _tool_name: str | None = None\n    _tool_description: str | None = None\n    _tool_inputs: dict | None = None\n    _tool_end: bool | None = None\n\n    @staticmethod\n    def _convert_ast_dict(ast_dict: ast.Dict) -&gt; dict:\n        out = {}\n        for i in range(len(ast_dict.keys)):\n            k: ast.Constant = ast_dict.keys[i]  # type: ignore\n            v: ast.Dict | ast.List | ast.Constant = ast_dict.values[i]  # type: ignore\n\n            if isinstance(v, ast.Dict):\n                out[k.value] = ToolMeta._convert_ast_dict(v)\n            elif isinstance(v, ast.List):\n                out[k.value] = ToolMeta._convert_ast_list(v)\n            elif isinstance(v, ast.Constant):\n                out[k.value] = v.value\n        return out\n\n    @staticmethod\n    def _convert_ast_list(ast_list: ast.List) -&gt; list:\n        out = []\n        for v in ast_list.values:  # type: ignore\n            if isinstance(v, ast.Dict):\n                out.append(ToolMeta._convert_ast_dict(v))\n            elif isinstance(v, ast.List):\n                out.append(ToolMeta._convert_ast_list(v))\n            elif isinstance(v, ast.Constant):\n                out.append(v.value)\n        return out\n\n    def __new__(cls, name, bases, namespace):\n        new_class = super().__new__(cls, name, bases, namespace)\n\n        # Skip the base Tool class itself\n        if name == \"Tool\":\n            return new_class\n\n        # Extract metadata from __init__ method if it exists\n        if \"__init__\" in namespace:\n            try:\n                init_method = namespace[\"__init__\"]\n                source = inspect.getsource(init_method)\n                tree = ast.parse(source.strip())\n\n                # Find the super().__init__() call and extract arguments\n                for node in ast.walk(tree):\n                    if (\n                        isinstance(node, ast.Call)\n                        and isinstance(node.func, ast.Attribute)\n                        and node.func.attr == \"__init__\"\n                        and isinstance(node.func.value, ast.Call)\n                        and isinstance(node.func.value.func, ast.Name)\n                        and node.func.value.func.id == \"super\"\n                    ):\n\n                        # Extract keyword arguments\n                        for keyword in node.keywords:\n                            if keyword.arg == \"name\" and isinstance(\n                                keyword.value, ast.Constant\n                            ):\n                                new_class._tool_name = keyword.value.value\n                            elif keyword.arg == \"description\" and isinstance(\n                                keyword.value, ast.Constant\n                            ):\n                                new_class._tool_description = keyword.value.value\n                            elif keyword.arg == \"inputs\" and isinstance(\n                                keyword.value, ast.Dict\n                            ):\n                                new_class._tool_inputs = ToolMeta._convert_ast_dict(\n                                    keyword.value\n                                )\n                            elif keyword.arg == \"end\" and isinstance(\n                                keyword.value, ast.Constant\n                            ):\n                                new_class._tool_end = keyword.value.value\n                        break\n            except Exception:\n                # If parsing fails, just continue without setting class attributes\n                pass\n\n        return new_class\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Update","title":"<code>Update</code>","text":"<p>               Bases: <code>Return</code></p> <p>Frontend Return Type 2: Update An update is something that is not displayed on the frontend, but gives information to the frontend. E.g. a warning, error, status message, etc.</p> Source code in <code>elysia/objects.py</code> <pre><code>class Update(Return):\n    \"\"\"\n    Frontend Return Type 2: Update\n    An update is something that is not _displayed_ on the frontend, but gives information to the frontend.\n    E.g. a warning, error, status message, etc.\n    \"\"\"\n\n    def __init__(self, frontend_type: str, object: dict):\n        Return.__init__(self, frontend_type, \"update\")\n        self.object = object\n\n    def to_json(self):\n        return self.object\n\n    async def to_frontend(self, user_id: str, conversation_id: str, query_id: str):\n        return {\n            \"type\": self.frontend_type,\n            \"user_id\": user_id,\n            \"conversation_id\": conversation_id,\n            \"query_id\": query_id,\n            \"id\": self.frontend_type[:3] + \"-\" + str(uuid.uuid4()),\n            \"payload\": self.to_json(),\n        }\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.Warning","title":"<code>Warning</code>","text":"<p>               Bases: <code>Update</code></p> <p>Warning message to be sent to the frontend.</p> Source code in <code>elysia/objects.py</code> <pre><code>class Warning(Update):\n    \"\"\"\n    Warning message to be sent to the frontend.\n    \"\"\"\n\n    def __init__(self, text: str):\n        self.text = text\n        Update.__init__(self, \"warning\", {\"text\": text})\n</code></pre>"},{"location":"Reference/Objects/#elysia.objects.tool","title":"<code>tool(function=None, *, status=None, end=False, tree=None, branch_id=None)</code>","text":"<pre><code>tool(function: Callable, *, status: str | None = None, end: bool = False, tree: Tree | None = None, branch_id: str | None = None) -&gt; Tool\n</code></pre><pre><code>tool(function: None = None, *, status: str | None = None, end: bool = False, tree: Tree | None = None, branch_id: str | None = None) -&gt; Callable[[Callable], Tool]\n</code></pre> <p>Create a tool from a python function. Use this decorator to create a tool from a function. The function must be an async function or async generator function.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>The function to create a tool from.</p> <code>None</code> <code>status</code> <code>str | None</code> <p>The status message to display while the tool is running. Optional, defaults to None, which will use the default status message \"Running {tool_name}...\".</p> <code>None</code> <code>end</code> <code>bool</code> <p>Whether the tool can be at the end of the decision tree. Set to True when this tool is allowed to end the conversation. Optional, defaults to False.</p> <code>False</code> <code>tree</code> <code>Tree | None</code> <p>The tree to add the tool to. Optional, defaults to None, which will not add the tool to the tree.</p> <code>None</code> <code>branch_id</code> <code>str | None</code> <p>The branch of the tree to add the tool to. Optional, defaults to None, which will add the tool to the root branch.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tool</code> <p>The tool object which can be added to the tree (via <code>tree.add_tool(...)</code>).</p> Source code in <code>elysia/objects.py</code> <pre><code>def tool(\n    function: Callable | None = None,\n    *,\n    status: str | None = None,\n    end: bool = False,\n    tree: \"Tree | None\" = None,\n    branch_id: str | None = None,\n) -&gt; Callable[[Callable], Tool] | Tool:\n    \"\"\"\n    Create a tool from a python function.\n    Use this decorator to create a tool from a function.\n    The function must be an async function or async generator function.\n\n    Args:\n        function (Callable): The function to create a tool from.\n        status (str | None): The status message to display while the tool is running.\n            Optional, defaults to None, which will use the default status message \"Running {tool_name}...\".\n        end (bool): Whether the tool can be at the end of the decision tree.\n            Set to True when this tool is allowed to end the conversation.\n            Optional, defaults to False.\n        tree (Tree | None): The tree to add the tool to.\n            Optional, defaults to None, which will not add the tool to the tree.\n        branch_id (str | None): The branch of the tree to add the tool to.\n            Optional, defaults to None, which will add the tool to the root branch.\n\n    Returns:\n        (Tool): The tool object which can be added to the tree (via `tree.add_tool(...)`).\n    \"\"\"\n\n    def decorator(function: Callable) -&gt; Tool:\n        async_function = inspect.iscoroutinefunction(function)\n        async_generator_function = inspect.isasyncgenfunction(function)\n\n        if not async_function and not async_generator_function:\n            raise TypeError(\n                \"The provided function must be an async function or async generator function.\"\n            )\n\n        if \"inputs\" in list(function.__annotations__.keys()):\n            raise TypeError(\n                \"The `inputs` argument is reserved in tool functions, please choose another name.\"\n            )\n\n        sig = inspect.signature(function)\n        defaults_mapping = {\n            k: v.default\n            for k, v in sig.parameters.items()\n            if v.default is not inspect.Parameter.empty\n        }\n\n        def list_to_list_of_dicts(result: list) -&gt; list[dict]:\n            objects = []\n            for obj in result:\n                if isinstance(obj, dict):\n                    objects.append(obj)\n                elif isinstance(obj, int | float | bool):\n                    objects.append(\n                        {\n                            \"tool_result\": obj,\n                        }\n                    )\n                elif isinstance(obj, list):\n                    objects.append(list_to_list_of_dicts(obj))\n                else:\n                    objects.append(\n                        {\n                            \"tool_result\": obj,\n                        }\n                    )\n            return objects\n\n        def return_mapping(result, inputs: dict):\n            if isinstance(result, Result | Text | Update | Status | Error):\n                return result\n            elif isinstance(result, str):\n                return Response(result)\n            elif isinstance(result, int | float | bool):\n                return Result(\n                    objects=[\n                        {\n                            \"tool_result\": result,\n                        }\n                    ],\n                    metadata={\n                        \"tool_name\": function.__name__,\n                        \"inputs_used\": inputs,\n                    },\n                )\n            elif isinstance(result, dict):\n                return Result(\n                    objects=[result],\n                    metadata={\n                        \"tool_name\": function.__name__,\n                        \"inputs_used\": inputs,\n                    },\n                )\n            elif isinstance(result, list):\n                return Result(\n                    objects=list_to_list_of_dicts(result),\n                    metadata={\n                        \"tool_name\": function.__name__,\n                        \"inputs_used\": inputs,\n                    },\n                )\n            else:\n                return Result(\n                    objects=[\n                        {\n                            \"tool_result\": result,\n                        }\n                    ],\n                    metadata={\n                        \"tool_name\": function.__name__,\n                        \"inputs_used\": inputs,\n                    },\n                )\n\n        class ToolClass(Tool):\n            def __init__(self, **kwargs):\n                self._original_function = function\n                self._original_function_args = {\n                    arg: None\n                    for arg in function.__code__.co_varnames[\n                        : function.__code__.co_argcount\n                    ]\n                }\n                for arg in function.__annotations__:\n                    if arg in self._original_function_args:\n                        self._original_function_args[arg] = function.__annotations__[\n                            arg\n                        ]\n                super().__init__(\n                    name=function.__name__,\n                    description=function.__doc__ or \"\",\n                    status=(\n                        status\n                        if status is not None\n                        else f\"Running {function.__name__}...\"\n                    ),\n                    inputs={\n                        input_key: {\n                            \"description\": \"\",\n                            \"type\": (\n                                \"Not specified\" if input_value is None else input_value\n                            ),\n                            \"default\": defaults_mapping.get(input_key, None),\n                            \"required\": defaults_mapping.get(input_key, None)\n                            is not None,\n                        }\n                        for input_key, input_value in self._original_function_args.items()\n                        if input_key\n                        not in [\n                            \"tree_data\",\n                            \"base_lm\",\n                            \"complex_lm\",\n                            \"client_manager\",\n                            \"return\",\n                        ]\n                    },\n                    end=end,\n                )\n\n            async def __call__(\n                self, tree_data, inputs, base_lm, complex_lm, client_manager, **kwargs\n            ):\n\n                if async_function:\n                    tool_output = [\n                        await function(\n                            **inputs,\n                            **{\n                                k: v\n                                for k, v in {\n                                    \"tree_data\": tree_data,\n                                    \"base_lm\": base_lm,\n                                    \"complex_lm\": complex_lm,\n                                    \"client_manager\": client_manager,\n                                    **kwargs,\n                                }.items()\n                                if k in self._original_function_args\n                            },\n                        )\n                    ]\n                elif async_generator_function:\n                    results = []\n                    async for result in function(\n                        **inputs,\n                        **{\n                            k: v\n                            for k, v in {\n                                \"tree_data\": tree_data,\n                                \"base_lm\": base_lm,\n                                \"complex_lm\": complex_lm,\n                                \"client_manager\": client_manager,\n                                **kwargs,\n                            }.items()\n                            if k in self._original_function_args\n                        },\n                    ):\n                        results.append(result)\n                    tool_output = results\n\n                for result in tool_output:\n                    mapped_result = return_mapping(result, inputs)\n                    yield mapped_result\n\n        tool_class = ToolClass()\n\n        if tree is not None:\n            tree.add_tool(tool_class, branch_id=branch_id)\n\n        return tool_class\n\n    if function is not None:\n        return decorator(function)\n    else:\n        return decorator\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.CollectionData","title":"<code>CollectionData</code>","text":"<p>Store of data about the Weaviate collections that are used in the tree. These are the output of the <code>preprocess</code> function.</p> <p>You do not normally need to interact with this class directly. Instead, do so via the <code>TreeData</code> class, which has corresponding methods to get the data in a variety of formats. (Such as via the <code>output_full_metadata</code> method.)</p> Source code in <code>elysia/tree/objects.py</code> <pre><code>class CollectionData:\n    \"\"\"\n    Store of data about the Weaviate collections that are used in the tree.\n    These are the output of the `preprocess` function.\n\n    You do not normally need to interact with this class directly.\n    Instead, do so via the `TreeData` class, which has corresponding methods to get the data in a variety of formats.\n    (Such as via the `output_full_metadata` method.)\n    \"\"\"\n\n    def __init__(\n        self,\n        collection_names: list[str],\n        metadata: dict[str, Any] = {},\n        logger: Logger | None = None,\n    ):\n        self.collection_names = collection_names\n        self.metadata = metadata\n        self.logger = logger\n\n    async def set_collection_names(\n        self, collection_names: list[str], client_manager: ClientManager\n    ):\n        temp_metadata = {}\n\n        # check if any of these collections exist in the full metadata (cached)\n        collections_to_get = []\n        for collection_name in collection_names:\n            if collection_name not in self.metadata:\n                collections_to_get.append(collection_name)\n\n        self.removed_collections = []\n        self.incorrect_collections = []\n\n        metadata_name = \"ELYSIA_METADATA__\"\n\n        async with client_manager.connect_to_async_client() as client:\n            # check if the metadata collection exists\n            if not await client.collections.exists(metadata_name):\n                self.removed_collections.extend(collections_to_get)\n            else:\n                metadata_collection = client.collections.get(metadata_name)\n                filters = (\n                    Filter.any_of(\n                        [\n                            Filter.by_property(\"name\").equal(collection_name)\n                            for collection_name in collections_to_get\n                        ]\n                    )\n                    if len(collections_to_get) &gt;= 1\n                    else None\n                )\n                metadata = await metadata_collection.query.fetch_objects(\n                    filters=filters,\n                    limit=9999,\n                )\n                metadata_map = {\n                    metadata_obj.properties[\"name\"]: metadata_obj.properties\n                    for metadata_obj in metadata.objects\n                }\n\n                for collection_name in collections_to_get:\n\n                    if not await client.collections.exists(collection_name):\n                        self.incorrect_collections.append(collection_name)\n                        continue\n\n                    if collection_name not in metadata_map:\n                        self.removed_collections.append(collection_name)\n                    else:\n                        properties = metadata_map[collection_name]\n                        format_dict_to_serialisable(properties)  # type: ignore\n                        temp_metadata[collection_name] = properties\n\n        if len(self.removed_collections) &gt; 0 and self.logger:\n            self.logger.warning(\n                \"The following collections have not been pre-processed for Elysia. \"\n                f\"{self.removed_collections}. \"\n                \"Ignoring these collections for now.\"\n            )\n\n        if len(self.incorrect_collections) &gt; 0 and self.logger:\n            self.logger.warning(\n                \"The following collections cannot be found in this Weaviate cluster. \"\n                f\"{self.incorrect_collections}. \"\n                \"These are being ignored for now. Please check that the collection names are correct.\"\n            )\n\n        self.collection_names = [\n            collection_name\n            for collection_name in collection_names\n            if collection_name not in self.removed_collections\n            and collection_name not in self.incorrect_collections\n        ]\n\n        # add to cached full metadata\n        self.metadata = {\n            **self.metadata,\n            **{\n                collection_name: temp_metadata[collection_name]\n                for collection_name in temp_metadata\n                if collection_name not in self.metadata\n            },\n        }\n\n        return self.collection_names\n\n    def output_full_metadata(\n        self, collection_names: list[str] | None = None, with_mappings: bool = False\n    ):\n        if collection_names is None:\n            collection_names = self.collection_names\n\n        if with_mappings:\n            return {\n                collection_name: self.metadata[collection_name]\n                for collection_name in collection_names\n            }\n        else:\n            return {\n                collection_name: {\n                    k: v\n                    for k, v in self.metadata[collection_name].items()\n                    if k != \"mappings\"\n                }\n                for collection_name in collection_names\n            }\n\n    def output_collection_summaries(self, collection_names: list[str] | None = None):\n        if collection_names is None:\n            return {\n                collection_name: self.metadata[collection_name][\"summary\"]\n                for collection_name in self.collection_names\n            }\n        else:\n            return {\n                collection_name: self.metadata[collection_name][\"summary\"]\n                for collection_name in collection_names\n            }\n\n    def output_mapping_lists(self):\n        return {\n            collection_name: list(self.metadata[collection_name][\"mappings\"].keys())\n            for collection_name in self.collection_names\n        }\n\n    def output_mappings(self):\n        return {\n            collection_name: self.metadata[collection_name][\"mappings\"]\n            for collection_name in self.collection_names\n        }\n\n    def to_json(self):\n        return {\n            \"collection_names\": self.collection_names,\n            \"metadata\": self.metadata,\n        }\n\n    @classmethod\n    def from_json(cls, json_data: dict, logger: Logger | None = None):\n        return cls(\n            collection_names=json_data[\"collection_names\"],\n            metadata=json_data[\"metadata\"],\n            logger=logger,\n        )\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.Environment","title":"<code>Environment</code>","text":"<p>Store of all objects across different types of queries and responses.</p> <p>The environment is how the tree stores all objects across different types of queries and responses. This includes things like retrieved objects, retrieved metadata, retrieved summaries, etc.</p> <p>This is persistent across the tree, so that all agents are aware of the same objects.</p> <p>The environment is formatted and keyed as follows: <pre><code>{\n    \"tool_name\": {\n        \"result_name\": [\n            {\n                \"metadata\": dict,\n                \"objects\": list[dict],\n            },\n            ...\n        ]\n    }\n}\n</code></pre></p> <p>Where <code>\"tool_name\"</code> is the name of the function that the result belongs to, e.g. if the result came from a tool called <code>\"query\"</code>, then <code>\"tool_name\"</code> is <code>\"query\"</code>.</p> <p><code>\"result_name\"</code> is the name of the Result object, which can be customised, e.g. if the result comes from a specific collection, then <code>\"result_name\"</code> is the name of the collection.</p> <p><code>\"metadata\"</code> is the metadata of the result, e.g. the time taken to retrieve the result, the query used, etc.</p> <p><code>\"objects\"</code> is the list of objects retrieved from the result. This is a list of dictionaries, where each dictionary is an object. It is important that <code>objects</code> should have be list of dictionaries. e.g. each object that was returned from a retrieval, where the fields of each dictionary are the fields of the object returned.</p> <p>Each list under <code>result_name</code> is a dictionary with both <code>metadata</code> and <code>objects</code> keys. This is if, for example, you retrieve multiple objects from the same collection, each one is stored with different metadata. Because, for example, the query used to retrieve each object may be different (and stored differently in the metadata).</p> <p>The environment is initialised with a default \"SelfInfo.generic\" key, which is a list of one object, containing information about Elysia itself.</p> <p>You can use various methods to add, remove, replace, and find objects in the environment. See the methods below for more information.</p> <p>Within the environment, there is a variable called <code>hidden_environment</code>, which is a dictionary of key-value pairs. This is used to store information that is not shown to the LLM, but is instead a 'store' of data that can be used across tools.</p> Source code in <code>elysia/tree/objects.py</code> <pre><code>class Environment:\n    \"\"\"\n    Store of all objects across different types of queries and responses.\n\n    The environment is how the tree stores all objects across different types of queries and responses.\n    This includes things like retrieved objects, retrieved metadata, retrieved summaries, etc.\n\n    This is persistent across the tree, so that all agents are aware of the same objects.\n\n    The environment is formatted and keyed as follows:\n    ```python\n    {\n        \"tool_name\": {\n            \"result_name\": [\n                {\n                    \"metadata\": dict,\n                    \"objects\": list[dict],\n                },\n                ...\n            ]\n        }\n    }\n    ```\n\n    Where `\"tool_name\"` is the name of the function that the result belongs to,\n    e.g. if the result came from a tool called `\"query\"`, then `\"tool_name\"` is `\"query\"`.\n\n    `\"result_name\"` is the name of the Result object, which can be customised,\n    e.g. if the result comes from a specific collection, then `\"result_name\"` is the name of the collection.\n\n    `\"metadata\"` is the metadata of the result,\n    e.g. the time taken to retrieve the result, the query used, etc.\n\n    `\"objects\"` is the list of objects retrieved from the result. This is a list of dictionaries, where each dictionary is an object.\n    It is important that `objects` should have be list of dictionaries.\n    e.g. each object that was returned from a retrieval, where the fields of each dictionary are the fields of the object returned.\n\n    Each list under `result_name` is a dictionary with both `metadata` and `objects` keys.\n    This is if, for example, you retrieve multiple objects from the same collection, each one is stored with different metadata.\n    Because, for example, the query used to retrieve each object may be different (and stored differently in the metadata).\n\n    The environment is initialised with a default \"SelfInfo.generic\" key, which is a list of one object, containing information about Elysia itself.\n\n    You can use various methods to add, remove, replace, and find objects in the environment. See the methods below for more information.\n\n    Within the environment, there is a variable called `hidden_environment`, which is a dictionary of key-value pairs.\n    This is used to store information that is not shown to the LLM, but is instead a 'store' of data that can be used across tools.\n    \"\"\"\n\n    def __init__(\n        self,\n        environment: dict[str, dict[str, Any]] | None = None,\n        self_info: bool = True,\n        hidden_environment: dict[str, Any] = {},\n    ):\n        if environment is None:\n            environment = {}\n        self.environment = environment\n        self.hidden_environment = hidden_environment\n        self.self_info = self_info\n        if self_info:\n            self.environment[\"SelfInfo\"] = {}\n            self.environment[\"SelfInfo\"][\"info\"] = [\n                {\n                    \"name\": \"Elysia\",\n                    \"description\": \"An agentic RAG service in Weaviate.\",\n                    \"purpose\": remove_whitespace(\n                        \"\"\"Elysia is an agentic retrieval augmented generation (RAG) service, where users can query from Weaviate collections,\n                    and the assistant will retrieve the most relevant information and answer the user's question. This includes a variety\n                    of different ways to query, such as by filtering, sorting, querying multiple collections, and providing summaries\n                    and textual responses.\n\n                    Elysia will dynamically display retrieved objects from the collections in the frontend.\n                    Elysia works via a tree-based approach, where the user's question is used to generate a tree of potential\n                    queries to retrieve the most relevant information.\n                    Each end of the tree connects to a separate agent that will perform a specific task, such as retrieval, aggregation, or generation, or more.\n\n                    The tree itself has decision nodes that determine the next step in the query.\n                    The decision nodes are decided via a decision-agent, which decides the task.\n\n                    The agents communicate via a series of different prompts, which are stored in the prompt-library.\n                    The decision-agent prompts are designed to be as general as possible, so that they can be used for a variety of different tasks.\n                    Some of these variables include conversation history, retrieved objects, the user's original question, train of thought via model reasoning, and more.\n\n                    The backend of Elysia is built with a range of libraries. Elysia is built with a lot of base python, and was constructed from the ground up to keep it as modular as possible.\n                    The LLM components are built with DSPy, a library for training and running LLMs.\n                    \"\"\"\n                    ),\n                }\n            ]\n\n    def is_empty(self):\n        \"\"\"\n        Check if the environment is empty.\n\n        The \"SelfInfo\" key is not counted towards the empty environment.\n\n        If the `.remove` method has been used, this is accounted for (e.g. empty lists count towards an empty environment).\n        \"\"\"\n        empty = True\n        for tool_key in self.environment.keys():\n            if tool_key == \"SelfInfo\":\n                continue\n\n            for result_key in self.environment[tool_key].keys():\n                if len(self.environment[tool_key][result_key]) &gt; 0:\n                    empty = False\n                    break\n        return empty\n\n    def add(self, tool_name: str, result: Result, include_duplicates: bool = False):\n        \"\"\"\n        Adds a result to the environment.\n        Is called automatically by the tree when a result is returned from an agent.\n\n        You can also add a result to the environment manually by using this method.\n        In this case, you must be adding a 'Result' object (which has an implicit 'name' attribute used to key the environment).\n        If you want to add something manually, you can use the `add_objects` method.\n\n        Each item is added with a `_REF_ID` attribute, which is a unique identifier used to identify the object in the environment.\n        If duplicate objects are detected, they are added with a duplicate `_REF_ID` entry detailing that they are a duplicate,\n        as well as the `_REF_ID` of the original object.\n\n        Args:\n            tool_name (str): The name of the tool called that the result belongs to.\n            result (Result): The result to add to the environment.\n            include_duplicates (bool): Optional. Whether to include duplicate objects in the environment.\n                Defaults to `False`, which still adds the duplicate object, but with a duplicate `_REF_ID` entry (and no repeating properties).\n                If `True`, the duplicate object is added with a new `_REF_ID` entry, and the repeated properties are added to the object.\n        \"\"\"\n        objects = result.to_json()\n        name = result.name\n        metadata = result.metadata\n        if tool_name not in self.environment:\n            self.environment[tool_name] = {}\n\n        self.add_objects(tool_name, name, objects, metadata, include_duplicates)\n\n    def add_objects(\n        self,\n        tool_name: str,\n        name: str,\n        objects: list[dict],\n        metadata: dict = {},\n        include_duplicates: bool = False,\n    ):\n        \"\"\"\n        Adds an object to the environment.\n        Is not called automatically by the tree, so you must manually call this method.\n        This is useful if you want to add an object to the environment manually that doesn't come from a Result object.\n\n        Each item is added with a `_REF_ID` attribute, which is a unique identifier used to identify the object in the environment.\n        If duplicate objects are detected, they are added with a duplicate `_REF_ID` entry detailing that they are a duplicate,\n        as well as the `_REF_ID` of the original object.\n\n        Args:\n            tool_name (str): The name of the tool called that the result belongs to.\n            name (str): The name of the result.\n            objects (list[dict]): The objects to add to the environment.\n            metadata (dict): Optional. The metadata of the objects to add to the environment.\n                Defaults to an empty dictionary.\n            include_duplicates (bool): Optional. Whether to include duplicate objects in the environment.\n                Defaults to `False`, which still adds the duplicate object, but with a duplicate `_REF_ID` entry (and no repeating properties).\n                If `True`, the duplicate object is added with a new `_REF_ID` entry, and the repeated properties are added to the object.\n\n        \"\"\"\n        if tool_name not in self.environment:\n            self.environment[tool_name] = {}\n\n        if name not in self.environment[tool_name]:\n            self.environment[tool_name][name] = []\n\n        if len(objects) &gt; 0:\n            self.environment[tool_name][name].append(\n                {\n                    \"metadata\": metadata,\n                    \"objects\": [],\n                }\n            )\n\n            for i, obj in enumerate(objects):\n                # check if the object is already in the environment\n                obj_found = False\n                where_obj = None\n                for env_item in self.environment[tool_name][name]:\n                    if obj in env_item[\"objects\"]:\n                        obj_found = True\n                        where_obj = env_item[\"objects\"].index(obj)\n                        _REF_ID = env_item[\"objects\"][where_obj][\"_REF_ID\"]\n                        break\n\n                if obj_found and not include_duplicates:\n                    self.environment[tool_name][name][-1][\"objects\"].append(\n                        {\n                            \"object_info\": f\"[repeat]\",\n                            \"_REF_ID\": _REF_ID,\n                        }\n                    )\n                elif \"_REF_ID\" not in obj:\n                    _REF_ID = f\"{tool_name}_{name}_{len(self.environment[tool_name][name])}_{i}\"\n                    self.environment[tool_name][name][-1][\"objects\"].append(\n                        {\n                            \"_REF_ID\": _REF_ID,\n                            **obj,\n                        }\n                    )\n                else:\n                    self.environment[tool_name][name][-1][\"objects\"].append(obj)\n\n    def remove(self, tool_name: str, name: str, index: int | None = None):\n        \"\"\"\n        Replaces the list of objects for the given `tool_name` and `name` with an empty list.\n\n        Args:\n            tool_name (str): The name of the tool called that the result belongs to.\n            name (str): The name of the result.\n            index (int | None): The index of the object to remove.\n                If `None`, the entire list corresponding to `tool_name`/`name` is deleted.\n                If an integer, the object at the given index is removed.\n                Defaults to `None`.\n                If `index=-1`, the last object is removed.\n        \"\"\"\n        if tool_name in self.environment:\n            if name in self.environment[tool_name]:\n                if index is None:\n                    self.environment[tool_name][name] = []\n                else:\n                    self.environment[tool_name][name].pop(index)\n\n    def replace(\n        self,\n        tool_name: str,\n        name: str,\n        objects: list[dict],\n        metadata: dict = {},\n        index: int | None = None,\n    ):\n        \"\"\"\n        Replaces the list of objects for the given `tool_name` and `name` with the given list of objects.\n\n        Args:\n            tool_name (str): The name of the tool called that the result belongs to.\n            name (str): The name of the result.\n            objects (list[dict]): The objects to replace the existing objects with.\n            metadata (dict): The metadata of the objects to replace the existing objects with.\n            index (int | None): The index of the object to replace.\n                If `None`, the entire list corresponding to `tool_name`/`name` is deleted and replaced with the new objects.\n                If an integer, the object at the given index is replaced with the new objects.\n                Defaults to `None`.\n        \"\"\"\n        if tool_name in self.environment:\n            if name in self.environment[tool_name]:\n                if index is None:\n                    self.environment[tool_name][name] = [\n                        {\n                            \"metadata\": metadata,\n                            \"objects\": objects,\n                        }\n                    ]\n                else:\n                    self.environment[tool_name][name][index] = {\n                        \"metadata\": metadata,\n                        \"objects\": objects,\n                    }\n\n    def find(self, tool_name: str, name: str, index: int | None = None):\n        \"\"\"\n        Finds a corresponding list of objects in the environment.\n        Keyed via `tool_name` and `name`. See the base class description for more information on how the environment is keyed.\n\n        Args:\n            tool_name (str): The name of the tool called that the result belongs to.\n            name (str): The name of the result.\n            index (int | None): The index of the object to find.\n                If `None`, the entire list corresponding to `tool_name`/`name` is returned.\n                If an integer, the object at the given index is returned.\n\n        Returns:\n            (list[dict]): if `index` is `None` - The list of objects for the given `tool_name` and `name`.\n            (dict): if `index` is an integer - The object at the given `index` for the given `tool_name` and `name`.\n            (None): If the `tool_name` or `name` is not found in the environment.\n        \"\"\"\n\n        if tool_name not in self.environment:\n            return None\n        if name not in self.environment[tool_name]:\n            return None\n\n        if index is None:\n            return self.environment[tool_name][name]\n        else:\n            return self.environment[tool_name][name][index]\n\n    def to_json(self, remove_unserialisable: bool = False):\n        \"\"\"\n        Converts the environment to a JSON serialisable format.\n        Used to access specific objects from the environment.\n        \"\"\"\n\n        env_copy = deepcopy(self.environment)\n        hidden_env_copy = deepcopy(self.hidden_environment)\n\n        # Check if environment and hidden_environment are JSON serialisable\n        for tool_name in env_copy:\n            if tool_name != \"SelfInfo\":\n                for name in self.environment[tool_name]:\n                    for obj_metadata in self.environment[tool_name][name]:\n                        format_dict_to_serialisable(\n                            obj_metadata[\"metadata\"], remove_unserialisable\n                        )\n                        for obj in obj_metadata[\"objects\"]:\n                            format_dict_to_serialisable(obj, remove_unserialisable)\n\n        format_dict_to_serialisable(hidden_env_copy, remove_unserialisable)\n\n        return {\n            \"environment\": env_copy,\n            \"hidden_environment\": hidden_env_copy,\n            \"self_info\": self.self_info,\n        }\n\n    @classmethod\n    def from_json(cls, json_data: dict):\n        return cls(\n            environment=json_data[\"environment\"],\n            hidden_environment=json_data[\"hidden_environment\"],\n            self_info=json_data[\"self_info\"],\n        )\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.Environment.add","title":"<code>add(tool_name, result, include_duplicates=False)</code>","text":"<p>Adds a result to the environment. Is called automatically by the tree when a result is returned from an agent.</p> <p>You can also add a result to the environment manually by using this method. In this case, you must be adding a 'Result' object (which has an implicit 'name' attribute used to key the environment). If you want to add something manually, you can use the <code>add_objects</code> method.</p> <p>Each item is added with a <code>_REF_ID</code> attribute, which is a unique identifier used to identify the object in the environment. If duplicate objects are detected, they are added with a duplicate <code>_REF_ID</code> entry detailing that they are a duplicate, as well as the <code>_REF_ID</code> of the original object.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool called that the result belongs to.</p> required <code>result</code> <code>Result</code> <p>The result to add to the environment.</p> required <code>include_duplicates</code> <code>bool</code> <p>Optional. Whether to include duplicate objects in the environment. Defaults to <code>False</code>, which still adds the duplicate object, but with a duplicate <code>_REF_ID</code> entry (and no repeating properties). If <code>True</code>, the duplicate object is added with a new <code>_REF_ID</code> entry, and the repeated properties are added to the object.</p> <code>False</code> Source code in <code>elysia/tree/objects.py</code> <pre><code>def add(self, tool_name: str, result: Result, include_duplicates: bool = False):\n    \"\"\"\n    Adds a result to the environment.\n    Is called automatically by the tree when a result is returned from an agent.\n\n    You can also add a result to the environment manually by using this method.\n    In this case, you must be adding a 'Result' object (which has an implicit 'name' attribute used to key the environment).\n    If you want to add something manually, you can use the `add_objects` method.\n\n    Each item is added with a `_REF_ID` attribute, which is a unique identifier used to identify the object in the environment.\n    If duplicate objects are detected, they are added with a duplicate `_REF_ID` entry detailing that they are a duplicate,\n    as well as the `_REF_ID` of the original object.\n\n    Args:\n        tool_name (str): The name of the tool called that the result belongs to.\n        result (Result): The result to add to the environment.\n        include_duplicates (bool): Optional. Whether to include duplicate objects in the environment.\n            Defaults to `False`, which still adds the duplicate object, but with a duplicate `_REF_ID` entry (and no repeating properties).\n            If `True`, the duplicate object is added with a new `_REF_ID` entry, and the repeated properties are added to the object.\n    \"\"\"\n    objects = result.to_json()\n    name = result.name\n    metadata = result.metadata\n    if tool_name not in self.environment:\n        self.environment[tool_name] = {}\n\n    self.add_objects(tool_name, name, objects, metadata, include_duplicates)\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.Environment.add_objects","title":"<code>add_objects(tool_name, name, objects, metadata={}, include_duplicates=False)</code>","text":"<p>Adds an object to the environment. Is not called automatically by the tree, so you must manually call this method. This is useful if you want to add an object to the environment manually that doesn't come from a Result object.</p> <p>Each item is added with a <code>_REF_ID</code> attribute, which is a unique identifier used to identify the object in the environment. If duplicate objects are detected, they are added with a duplicate <code>_REF_ID</code> entry detailing that they are a duplicate, as well as the <code>_REF_ID</code> of the original object.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool called that the result belongs to.</p> required <code>name</code> <code>str</code> <p>The name of the result.</p> required <code>objects</code> <code>list[dict]</code> <p>The objects to add to the environment.</p> required <code>metadata</code> <code>dict</code> <p>Optional. The metadata of the objects to add to the environment. Defaults to an empty dictionary.</p> <code>{}</code> <code>include_duplicates</code> <code>bool</code> <p>Optional. Whether to include duplicate objects in the environment. Defaults to <code>False</code>, which still adds the duplicate object, but with a duplicate <code>_REF_ID</code> entry (and no repeating properties). If <code>True</code>, the duplicate object is added with a new <code>_REF_ID</code> entry, and the repeated properties are added to the object.</p> <code>False</code> Source code in <code>elysia/tree/objects.py</code> <pre><code>def add_objects(\n    self,\n    tool_name: str,\n    name: str,\n    objects: list[dict],\n    metadata: dict = {},\n    include_duplicates: bool = False,\n):\n    \"\"\"\n    Adds an object to the environment.\n    Is not called automatically by the tree, so you must manually call this method.\n    This is useful if you want to add an object to the environment manually that doesn't come from a Result object.\n\n    Each item is added with a `_REF_ID` attribute, which is a unique identifier used to identify the object in the environment.\n    If duplicate objects are detected, they are added with a duplicate `_REF_ID` entry detailing that they are a duplicate,\n    as well as the `_REF_ID` of the original object.\n\n    Args:\n        tool_name (str): The name of the tool called that the result belongs to.\n        name (str): The name of the result.\n        objects (list[dict]): The objects to add to the environment.\n        metadata (dict): Optional. The metadata of the objects to add to the environment.\n            Defaults to an empty dictionary.\n        include_duplicates (bool): Optional. Whether to include duplicate objects in the environment.\n            Defaults to `False`, which still adds the duplicate object, but with a duplicate `_REF_ID` entry (and no repeating properties).\n            If `True`, the duplicate object is added with a new `_REF_ID` entry, and the repeated properties are added to the object.\n\n    \"\"\"\n    if tool_name not in self.environment:\n        self.environment[tool_name] = {}\n\n    if name not in self.environment[tool_name]:\n        self.environment[tool_name][name] = []\n\n    if len(objects) &gt; 0:\n        self.environment[tool_name][name].append(\n            {\n                \"metadata\": metadata,\n                \"objects\": [],\n            }\n        )\n\n        for i, obj in enumerate(objects):\n            # check if the object is already in the environment\n            obj_found = False\n            where_obj = None\n            for env_item in self.environment[tool_name][name]:\n                if obj in env_item[\"objects\"]:\n                    obj_found = True\n                    where_obj = env_item[\"objects\"].index(obj)\n                    _REF_ID = env_item[\"objects\"][where_obj][\"_REF_ID\"]\n                    break\n\n            if obj_found and not include_duplicates:\n                self.environment[tool_name][name][-1][\"objects\"].append(\n                    {\n                        \"object_info\": f\"[repeat]\",\n                        \"_REF_ID\": _REF_ID,\n                    }\n                )\n            elif \"_REF_ID\" not in obj:\n                _REF_ID = f\"{tool_name}_{name}_{len(self.environment[tool_name][name])}_{i}\"\n                self.environment[tool_name][name][-1][\"objects\"].append(\n                    {\n                        \"_REF_ID\": _REF_ID,\n                        **obj,\n                    }\n                )\n            else:\n                self.environment[tool_name][name][-1][\"objects\"].append(obj)\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.Environment.find","title":"<code>find(tool_name, name, index=None)</code>","text":"<p>Finds a corresponding list of objects in the environment. Keyed via <code>tool_name</code> and <code>name</code>. See the base class description for more information on how the environment is keyed.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool called that the result belongs to.</p> required <code>name</code> <code>str</code> <p>The name of the result.</p> required <code>index</code> <code>int | None</code> <p>The index of the object to find. If <code>None</code>, the entire list corresponding to <code>tool_name</code>/<code>name</code> is returned. If an integer, the object at the given index is returned.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[dict]</code> <p>if <code>index</code> is <code>None</code> - The list of objects for the given <code>tool_name</code> and <code>name</code>.</p> <code>dict</code> <p>if <code>index</code> is an integer - The object at the given <code>index</code> for the given <code>tool_name</code> and <code>name</code>.</p> <code>None</code> <p>If the <code>tool_name</code> or <code>name</code> is not found in the environment.</p> Source code in <code>elysia/tree/objects.py</code> <pre><code>def find(self, tool_name: str, name: str, index: int | None = None):\n    \"\"\"\n    Finds a corresponding list of objects in the environment.\n    Keyed via `tool_name` and `name`. See the base class description for more information on how the environment is keyed.\n\n    Args:\n        tool_name (str): The name of the tool called that the result belongs to.\n        name (str): The name of the result.\n        index (int | None): The index of the object to find.\n            If `None`, the entire list corresponding to `tool_name`/`name` is returned.\n            If an integer, the object at the given index is returned.\n\n    Returns:\n        (list[dict]): if `index` is `None` - The list of objects for the given `tool_name` and `name`.\n        (dict): if `index` is an integer - The object at the given `index` for the given `tool_name` and `name`.\n        (None): If the `tool_name` or `name` is not found in the environment.\n    \"\"\"\n\n    if tool_name not in self.environment:\n        return None\n    if name not in self.environment[tool_name]:\n        return None\n\n    if index is None:\n        return self.environment[tool_name][name]\n    else:\n        return self.environment[tool_name][name][index]\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.Environment.is_empty","title":"<code>is_empty()</code>","text":"<p>Check if the environment is empty.</p> <p>The \"SelfInfo\" key is not counted towards the empty environment.</p> <p>If the <code>.remove</code> method has been used, this is accounted for (e.g. empty lists count towards an empty environment).</p> Source code in <code>elysia/tree/objects.py</code> <pre><code>def is_empty(self):\n    \"\"\"\n    Check if the environment is empty.\n\n    The \"SelfInfo\" key is not counted towards the empty environment.\n\n    If the `.remove` method has been used, this is accounted for (e.g. empty lists count towards an empty environment).\n    \"\"\"\n    empty = True\n    for tool_key in self.environment.keys():\n        if tool_key == \"SelfInfo\":\n            continue\n\n        for result_key in self.environment[tool_key].keys():\n            if len(self.environment[tool_key][result_key]) &gt; 0:\n                empty = False\n                break\n    return empty\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.Environment.remove","title":"<code>remove(tool_name, name, index=None)</code>","text":"<p>Replaces the list of objects for the given <code>tool_name</code> and <code>name</code> with an empty list.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool called that the result belongs to.</p> required <code>name</code> <code>str</code> <p>The name of the result.</p> required <code>index</code> <code>int | None</code> <p>The index of the object to remove. If <code>None</code>, the entire list corresponding to <code>tool_name</code>/<code>name</code> is deleted. If an integer, the object at the given index is removed. Defaults to <code>None</code>. If <code>index=-1</code>, the last object is removed.</p> <code>None</code> Source code in <code>elysia/tree/objects.py</code> <pre><code>def remove(self, tool_name: str, name: str, index: int | None = None):\n    \"\"\"\n    Replaces the list of objects for the given `tool_name` and `name` with an empty list.\n\n    Args:\n        tool_name (str): The name of the tool called that the result belongs to.\n        name (str): The name of the result.\n        index (int | None): The index of the object to remove.\n            If `None`, the entire list corresponding to `tool_name`/`name` is deleted.\n            If an integer, the object at the given index is removed.\n            Defaults to `None`.\n            If `index=-1`, the last object is removed.\n    \"\"\"\n    if tool_name in self.environment:\n        if name in self.environment[tool_name]:\n            if index is None:\n                self.environment[tool_name][name] = []\n            else:\n                self.environment[tool_name][name].pop(index)\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.Environment.replace","title":"<code>replace(tool_name, name, objects, metadata={}, index=None)</code>","text":"<p>Replaces the list of objects for the given <code>tool_name</code> and <code>name</code> with the given list of objects.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool called that the result belongs to.</p> required <code>name</code> <code>str</code> <p>The name of the result.</p> required <code>objects</code> <code>list[dict]</code> <p>The objects to replace the existing objects with.</p> required <code>metadata</code> <code>dict</code> <p>The metadata of the objects to replace the existing objects with.</p> <code>{}</code> <code>index</code> <code>int | None</code> <p>The index of the object to replace. If <code>None</code>, the entire list corresponding to <code>tool_name</code>/<code>name</code> is deleted and replaced with the new objects. If an integer, the object at the given index is replaced with the new objects. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>elysia/tree/objects.py</code> <pre><code>def replace(\n    self,\n    tool_name: str,\n    name: str,\n    objects: list[dict],\n    metadata: dict = {},\n    index: int | None = None,\n):\n    \"\"\"\n    Replaces the list of objects for the given `tool_name` and `name` with the given list of objects.\n\n    Args:\n        tool_name (str): The name of the tool called that the result belongs to.\n        name (str): The name of the result.\n        objects (list[dict]): The objects to replace the existing objects with.\n        metadata (dict): The metadata of the objects to replace the existing objects with.\n        index (int | None): The index of the object to replace.\n            If `None`, the entire list corresponding to `tool_name`/`name` is deleted and replaced with the new objects.\n            If an integer, the object at the given index is replaced with the new objects.\n            Defaults to `None`.\n    \"\"\"\n    if tool_name in self.environment:\n        if name in self.environment[tool_name]:\n            if index is None:\n                self.environment[tool_name][name] = [\n                    {\n                        \"metadata\": metadata,\n                        \"objects\": objects,\n                    }\n                ]\n            else:\n                self.environment[tool_name][name][index] = {\n                    \"metadata\": metadata,\n                    \"objects\": objects,\n                }\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.Environment.to_json","title":"<code>to_json(remove_unserialisable=False)</code>","text":"<p>Converts the environment to a JSON serialisable format. Used to access specific objects from the environment.</p> Source code in <code>elysia/tree/objects.py</code> <pre><code>def to_json(self, remove_unserialisable: bool = False):\n    \"\"\"\n    Converts the environment to a JSON serialisable format.\n    Used to access specific objects from the environment.\n    \"\"\"\n\n    env_copy = deepcopy(self.environment)\n    hidden_env_copy = deepcopy(self.hidden_environment)\n\n    # Check if environment and hidden_environment are JSON serialisable\n    for tool_name in env_copy:\n        if tool_name != \"SelfInfo\":\n            for name in self.environment[tool_name]:\n                for obj_metadata in self.environment[tool_name][name]:\n                    format_dict_to_serialisable(\n                        obj_metadata[\"metadata\"], remove_unserialisable\n                    )\n                    for obj in obj_metadata[\"objects\"]:\n                        format_dict_to_serialisable(obj, remove_unserialisable)\n\n    format_dict_to_serialisable(hidden_env_copy, remove_unserialisable)\n\n    return {\n        \"environment\": env_copy,\n        \"hidden_environment\": hidden_env_copy,\n        \"self_info\": self.self_info,\n    }\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.TreeData","title":"<code>TreeData</code>","text":"<p>Store of data across the tree. This includes things like conversation history, actions, decisions, etc. These data are given to ALL agents, so every agent is aware of the stage of the decision processes.</p> <p>This also contains functions that process the data into an LLM friendly format, such as a string with extra description. E.g. the number of trees completed is converted into a string (i/N)      and additional warnings if i is close to N.</p> <p>The TreeData has the following objects:</p> <ul> <li>collection_data (CollectionData): The collection metadata/schema, which contains information about the collections used in the tree.     This is the store of data that is saved by the <code>preprocess</code> function, and retrieved on initialisation of this object.</li> <li>atlas (Atlas): The atlas, described in the Atlas class.</li> <li>user_prompt (str): The user's prompt.</li> <li>conversation_history (list[dict]): The conversation history stored in the current tree, of the form:     <pre><code>[\n    {\n        \"role\": \"user\" | \"assistant\",\n        \"content\": str,\n    },\n    ...\n]\n</code></pre></li> <li>environment (Environment): The environment, described in the Environment class.</li> <li>tasks_completed (list[dict]): The tasks completed as a list of dictionaries.     This is separate from the environment, as it separates what tasks were completed in each prompt in which order.</li> <li>num_trees_completed (int): The current level of the decision tree, how many iterations have been completed so far.</li> <li>recursion_limit (int): The maximum number of iterations allowed in the decision tree.</li> <li>errors (dict): A dictionary of self-healing errors that have occurred in the tree. Keyed by the function name that caused the error.</li> </ul> <p>In general, you should not initialise this class directly. But you can access the data in this class to access the relevant data from the tree (in e.g. tool construction/usage).</p> Source code in <code>elysia/tree/objects.py</code> <pre><code>class TreeData:\n    \"\"\"\n    Store of data across the tree.\n    This includes things like conversation history, actions, decisions, etc.\n    These data are given to ALL agents, so every agent is aware of the stage of the decision processes.\n\n    This also contains functions that process the data into an LLM friendly format,\n    such as a string with extra description.\n    E.g. the number of trees completed is converted into a string (i/N)\n         and additional warnings if i is close to N.\n\n    The TreeData has the following objects:\n\n    - collection_data (CollectionData): The collection metadata/schema, which contains information about the collections used in the tree.\n        This is the store of data that is saved by the `preprocess` function, and retrieved on initialisation of this object.\n    - atlas (Atlas): The atlas, described in the Atlas class.\n    - user_prompt (str): The user's prompt.\n    - conversation_history (list[dict]): The conversation history stored in the current tree, of the form:\n        ```python\n        [\n            {\n                \"role\": \"user\" | \"assistant\",\n                \"content\": str,\n            },\n            ...\n        ]\n        ```\n    - environment (Environment): The environment, described in the Environment class.\n    - tasks_completed (list[dict]): The tasks completed as a list of dictionaries.\n        This is separate from the environment, as it separates what tasks were completed in each prompt in which order.\n    - num_trees_completed (int): The current level of the decision tree, how many iterations have been completed so far.\n    - recursion_limit (int): The maximum number of iterations allowed in the decision tree.\n    - errors (dict): A dictionary of self-healing errors that have occurred in the tree. Keyed by the function name that caused the error.\n\n    In general, you should not initialise this class directly.\n    But you can access the data in this class to access the relevant data from the tree (in e.g. tool construction/usage).\n    \"\"\"\n\n    def __init__(\n        self,\n        collection_data: CollectionData,\n        atlas: Atlas,\n        user_prompt: str | None = None,\n        conversation_history: list[dict] | None = None,\n        environment: Environment | None = None,\n        tasks_completed: list[dict] | None = None,\n        num_trees_completed: int | None = None,\n        recursion_limit: int | None = None,\n        settings: Settings | None = None,\n    ):\n        if settings is None:\n            self.settings = environment_settings\n        else:\n            self.settings = settings\n\n        # -- Base Data --\n        if user_prompt is None:\n            self.user_prompt = \"\"\n        else:\n            self.user_prompt = user_prompt\n\n        if conversation_history is None:\n            self.conversation_history = []\n        else:\n            self.conversation_history = conversation_history\n\n        if environment is None:\n            self.environment = Environment()\n        else:\n            self.environment = environment\n\n        if tasks_completed is None:\n            self.tasks_completed = []\n        else:\n            self.tasks_completed = tasks_completed\n\n        if num_trees_completed is None:\n            self.num_trees_completed = 0\n        else:\n            self.num_trees_completed = num_trees_completed\n\n        if recursion_limit is None:\n            self.recursion_limit = 3\n        else:\n            self.recursion_limit = recursion_limit\n\n        # -- Atlas --\n        self.atlas = atlas\n\n        # -- Collection Data --\n        self.collection_data = collection_data\n        self.collection_names = []\n\n        # -- Errors --\n        self.errors: dict[str, list[str]] = {}\n        self.current_task = None\n\n    def set_property(self, property: str, value: Any):\n        self.__dict__[property] = value\n\n    def update_string(self, property: str, value: str):\n        if property not in self.__dict__:\n            self.__dict__[property] = \"\"\n        self.__dict__[property] += value\n\n    def update_list(self, property: str, value: Any):\n        if property not in self.__dict__:\n            self.__dict__[property] = []\n        self.__dict__[property].append(value)\n\n    def update_dict(self, property: str, key: str, value: Any):\n        if property not in self.__dict__:\n            self.__dict__[property] = {}\n        self.__dict__[property][key] = value\n\n    def delete_from_dict(self, property: str, key: str):\n        if property in self.__dict__ and key in self.__dict__[property]:\n            del self.__dict__[property][key]\n\n    def soft_reset(self):\n        self.previous_reasoning = {}\n\n    def _update_task(self, task_dict, key, value):\n        if value is not None:\n            if key in task_dict:\n                # If key already exists, append to it\n                if isinstance(value, str):\n                    task_dict[key] += \"\\n\" + value\n                elif isinstance(value, int) or isinstance(value, float):\n                    task_dict[key] += value\n                elif isinstance(value, list):\n                    task_dict[key].extend(value)\n                elif isinstance(value, dict):\n                    task_dict[key].update(value)\n                elif isinstance(value, bool):\n                    task_dict[key] = value\n            elif key not in task_dict:\n                # If key does not exist, create it\n                task_dict[key] = value\n\n    def update_tasks_completed(\n        self, prompt: str, task: str, num_trees_completed: int, **kwargs\n    ):\n        # search to see if the current prompt already has an entry for this task\n        prompt_found = False\n        task_found = False\n        iteration_found = False\n\n        for i, task_prompt in enumerate(self.tasks_completed):\n            if task_prompt[\"prompt\"] == prompt:\n                prompt_found = True\n                for j, task_j in enumerate(task_prompt[\"task\"]):\n                    if task_j[\"task\"] == task:\n                        task_found = True\n                        task_i = j  # position of the task in the prompt\n\n                    if task_j[\"iteration\"] == num_trees_completed:\n                        iteration_found = True\n\n        # If the prompt is not found, add it to the list\n        if not prompt_found:\n            self.tasks_completed.append({\"prompt\": prompt, \"task\": [{}]})\n            self.tasks_completed[-1][\"task\"][0][\"task\"] = task\n            self.tasks_completed[-1][\"task\"][0][\"iteration\"] = num_trees_completed\n            for kwarg in kwargs:\n                self._update_task(\n                    self.tasks_completed[-1][\"task\"][0], kwarg, kwargs[kwarg]\n                )\n            return\n\n        # If the prompt is found but the task is not, add it to the list\n        if prompt_found and not task_found:\n            self.tasks_completed[-1][\"task\"].append({})\n            self.tasks_completed[-1][\"task\"][-1][\"task\"] = task\n            self.tasks_completed[-1][\"task\"][-1][\"iteration\"] = num_trees_completed\n            for kwarg in kwargs:\n                self._update_task(\n                    self.tasks_completed[-1][\"task\"][-1], kwarg, kwargs[kwarg]\n                )\n            return\n\n        # task already exists in this query, but the iteration is new\n        if prompt_found and task_found and not iteration_found:\n            self.tasks_completed[-1][\"task\"].append({})\n            self.tasks_completed[-1][\"task\"][-1][\"task\"] = task\n            self.tasks_completed[-1][\"task\"][-1][\"iteration\"] = num_trees_completed\n            for kwarg in kwargs:\n                self._update_task(\n                    self.tasks_completed[-1][\"task\"][-1], kwarg, kwargs[kwarg]\n                )\n            return\n\n        # If the prompt is found and the task is found, update the task\n        if prompt_found and task_found:\n            for kwarg in kwargs:\n                self._update_task(\n                    self.tasks_completed[-1][\"task\"][task_i], kwarg, kwargs[kwarg]\n                )\n\n    def set_current_task(self, task: str):\n        self.current_task = task\n\n    def get_errors(self):\n        if self.current_task == \"elysia_decision_node\":\n            return self.errors\n        elif self.current_task is None or self.current_task not in self.errors:\n            return []\n        else:\n            return self.errors[self.current_task]\n\n    def clear_error(self, task: str):\n        if task in self.errors:\n            self.errors[task] = []\n\n    def tasks_completed_string(self):\n        \"\"\"\n        Output a nicely formatted string of the tasks completed so far, designed to be used in the LLM prompt.\n        This is where the outputs of the `llm_message` fields are displayed.\n        You can use this if you are interfacing with LLMs in tools, to help it understand the context of the tasks completed so far.\n\n        Returns:\n            (str): A separated and formatted string of the tasks completed so far in an LLM-parseable format.\n        \"\"\"\n        out = \"\"\n        for j, task_prompt in enumerate(self.tasks_completed):\n            out += f\"&lt;prompt_{j+1}&gt;\\n\"\n            out += f\"Prompt: {task_prompt['prompt']}\\n\"\n\n            for i, task in enumerate(task_prompt[\"task\"]):\n                out += f\"&lt;task_{i+1}&gt;\\n\"\n\n                if \"action\" in task and task[\"action\"]:\n                    out += (\n                        f\"Chosen action: {task['task']} (this does not mean it has been completed, \"\n                        \"only that it was chosen, use the environment to judge if a task is completed)\\n\"\n                    )\n                else:\n                    out += f\"Chosen subcategory: {task['task']} (this action has not been completed, this is only a subcategory)\"\n\n                if \"error\" in task and task[\"error\"]:\n                    out += (\n                        f\" (UNSUCCESSFUL) There was an error during this tool call. \"\n                        \"See the error messages for details. This action did not complete.\\n\"\n                    )\n                else:\n                    out += f\" (SUCCESSFUL)\\n\"\n                    for key in task:\n                        if key != \"task\" and key != \"action\":\n                            out += f\"{key.capitalize()}: {task[key]}\\n\"\n\n                out += f\"&lt;/task_{i+1}&gt;\\n\"\n            out += f\"&lt;/prompt_{j+1}&gt;\\n\"\n\n        return out\n\n    async def set_collection_names(\n        self, collection_names: list[str], client_manager: ClientManager\n    ):\n        self.collection_names = await self.collection_data.set_collection_names(\n            collection_names, client_manager\n        )\n        return self.collection_names\n\n    def tree_count_string(self):\n        out = f\"{self.num_trees_completed+1}/{self.recursion_limit}\"\n        if self.num_trees_completed == self.recursion_limit - 1:\n            out += \" (this is the last decision you can make before being cut off)\"\n        if self.num_trees_completed &gt;= self.recursion_limit:\n            out += \" (recursion limit reached, write your full chat response accordingly - the decision process has been cut short, and it is likely the user's question has not been fully answered and you either haven't been able to do it or it was impossible)\"\n        return out\n\n    def output_collection_metadata(\n        self, collection_names: list[str] | None = None, with_mappings: bool = False\n    ):\n        \"\"\"\n        Outputs the full metadata for the given collection names.\n\n        Args:\n            with_mappings (bool): Whether to output the mappings for the collections as well as the other metadata.\n\n        Returns:\n            dict (dict[str, dict]): A dictionary of collection names to their metadata.\n                The metadata are of the form:\n                ```python\n                {\n                    # summary statistics of each field in the collection\n                    \"fields\": list = [\n                        field_name_1: dict = {\n                            \"description\": str,\n                            \"range\": list[float],\n                            \"type\": str,\n                            \"groups\": dict[str, str],\n                            \"mean\": float\n                        },\n                        field_name_2: dict = {\n                            ... # same fields as above\n                        },\n                        ...\n                    ],\n\n                    # mapping_1, mapping_2 etc refer to frontend-specific types that the AI has deemed appropriate for this data\n                    # then the dict is to map the frontend fields to the data fields\n                    \"mappings\": dict = {\n                        mapping_1: dict = {\n                            \"frontend_field_1\": \"data_field_1\",\n                            \"frontend_field_2\": \"data_field_2\",\n                            ...\n                        },\n                        mapping_2: dict = {\n                            ... # same fields as above\n                        },\n                        ...,\n                    },\n\n                    # number of items in collection (float but just for consistency)\n                    \"length\": float,\n\n                    # AI generated summary of the dataset\n                    \"summary\": str,\n\n                    # name of collection\n                    \"name\": str,\n\n                    # what named vectors are available and their properties (if any)\n                    \"named_vectors\": list = [\n                        {\n                            \"name\": str,\n                            \"enabled\": bool,\n                            \"source_properties\": list,\n                            \"description\": str # defaults to empty\n                        },\n                        ...\n                    ],\n\n                    # some config settings relevant for queries\n                    \"index_properties\": {\n                        \"isNullIndexed\": bool,\n                        \"isLengthIndexed\": bool,\n                        \"isTimestampIndexed\": bool,\n                    },\n                }\n                ```\n                If `with_mappings` is `False`, then the mappings are not included.\n                Each key in the outer level dictionary is a collection name.\n\n        \"\"\"\n\n        if collection_names is None:\n            collection_names = self.collection_names\n\n        return self.collection_data.output_full_metadata(\n            collection_names, with_mappings\n        )\n\n    def output_collection_return_types(self) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Outputs the return types for the collections in the tree data.\n        Essentially, this is a list of the keys that can be used to map the objects to the frontend.\n\n        Returns:\n            (dict): A dictionary of collection names to their return types.\n                ```python\n                {\n                    collection_name_1: list[str],\n                    collection_name_2: list[str],\n                    ...,\n                }\n                ```\n                Each of these lists is a list of the keys that can be used to map the objects to the frontend.\n        \"\"\"\n        collection_return_types = self.collection_data.output_mapping_lists()\n        out = {\n            collection_name: collection_return_types[collection_name]\n            for collection_name in self.collection_names\n        }\n        return out\n\n    def to_json(self, remove_unserialisable: bool = False):\n        out = {\n            k: v\n            for k, v in self.__dict__.items()\n            if k not in [\"collection_data\", \"atlas\", \"environment\", \"settings\"]\n        }\n        out[\"collection_data\"] = self.collection_data.to_json()\n        out[\"atlas\"] = self.atlas.model_dump()\n        out[\"environment\"] = self.environment.to_json(remove_unserialisable)\n        out[\"settings\"] = self.settings.to_json()\n        return out\n\n    @classmethod\n    def from_json(cls, json_data: dict):\n        settings = Settings.from_json(json_data[\"settings\"])\n        logger = settings.logger\n        collection_data = CollectionData.from_json(json_data[\"collection_data\"], logger)\n        atlas = Atlas.model_validate(json_data[\"atlas\"])\n        environment = Environment.from_json(json_data[\"environment\"])\n\n        tree_data = cls(\n            collection_data=collection_data,\n            atlas=atlas,\n            environment=environment,\n            settings=settings,\n        )\n        for item in json_data:\n            if item not in [\"collection_data\", \"atlas\", \"environment\", \"settings\"]:\n                tree_data.set_property(item, json_data[item])\n        return tree_data\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.TreeData.output_collection_metadata","title":"<code>output_collection_metadata(collection_names=None, with_mappings=False)</code>","text":"<p>Outputs the full metadata for the given collection names.</p> <p>Parameters:</p> Name Type Description Default <code>with_mappings</code> <code>bool</code> <p>Whether to output the mappings for the collections as well as the other metadata.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, dict]</code> <p>A dictionary of collection names to their metadata. The metadata are of the form: <pre><code>{\n    # summary statistics of each field in the collection\n    \"fields\": list = [\n        field_name_1: dict = {\n            \"description\": str,\n            \"range\": list[float],\n            \"type\": str,\n            \"groups\": dict[str, str],\n            \"mean\": float\n        },\n        field_name_2: dict = {\n            ... # same fields as above\n        },\n        ...\n    ],\n\n    # mapping_1, mapping_2 etc refer to frontend-specific types that the AI has deemed appropriate for this data\n    # then the dict is to map the frontend fields to the data fields\n    \"mappings\": dict = {\n        mapping_1: dict = {\n            \"frontend_field_1\": \"data_field_1\",\n            \"frontend_field_2\": \"data_field_2\",\n            ...\n        },\n        mapping_2: dict = {\n            ... # same fields as above\n        },\n        ...,\n    },\n\n    # number of items in collection (float but just for consistency)\n    \"length\": float,\n\n    # AI generated summary of the dataset\n    \"summary\": str,\n\n    # name of collection\n    \"name\": str,\n\n    # what named vectors are available and their properties (if any)\n    \"named_vectors\": list = [\n        {\n            \"name\": str,\n            \"enabled\": bool,\n            \"source_properties\": list,\n            \"description\": str # defaults to empty\n        },\n        ...\n    ],\n\n    # some config settings relevant for queries\n    \"index_properties\": {\n        \"isNullIndexed\": bool,\n        \"isLengthIndexed\": bool,\n        \"isTimestampIndexed\": bool,\n    },\n}\n</code></pre> If <code>with_mappings</code> is <code>False</code>, then the mappings are not included. Each key in the outer level dictionary is a collection name.</p> Source code in <code>elysia/tree/objects.py</code> <pre><code>def output_collection_metadata(\n    self, collection_names: list[str] | None = None, with_mappings: bool = False\n):\n    \"\"\"\n    Outputs the full metadata for the given collection names.\n\n    Args:\n        with_mappings (bool): Whether to output the mappings for the collections as well as the other metadata.\n\n    Returns:\n        dict (dict[str, dict]): A dictionary of collection names to their metadata.\n            The metadata are of the form:\n            ```python\n            {\n                # summary statistics of each field in the collection\n                \"fields\": list = [\n                    field_name_1: dict = {\n                        \"description\": str,\n                        \"range\": list[float],\n                        \"type\": str,\n                        \"groups\": dict[str, str],\n                        \"mean\": float\n                    },\n                    field_name_2: dict = {\n                        ... # same fields as above\n                    },\n                    ...\n                ],\n\n                # mapping_1, mapping_2 etc refer to frontend-specific types that the AI has deemed appropriate for this data\n                # then the dict is to map the frontend fields to the data fields\n                \"mappings\": dict = {\n                    mapping_1: dict = {\n                        \"frontend_field_1\": \"data_field_1\",\n                        \"frontend_field_2\": \"data_field_2\",\n                        ...\n                    },\n                    mapping_2: dict = {\n                        ... # same fields as above\n                    },\n                    ...,\n                },\n\n                # number of items in collection (float but just for consistency)\n                \"length\": float,\n\n                # AI generated summary of the dataset\n                \"summary\": str,\n\n                # name of collection\n                \"name\": str,\n\n                # what named vectors are available and their properties (if any)\n                \"named_vectors\": list = [\n                    {\n                        \"name\": str,\n                        \"enabled\": bool,\n                        \"source_properties\": list,\n                        \"description\": str # defaults to empty\n                    },\n                    ...\n                ],\n\n                # some config settings relevant for queries\n                \"index_properties\": {\n                    \"isNullIndexed\": bool,\n                    \"isLengthIndexed\": bool,\n                    \"isTimestampIndexed\": bool,\n                },\n            }\n            ```\n            If `with_mappings` is `False`, then the mappings are not included.\n            Each key in the outer level dictionary is a collection name.\n\n    \"\"\"\n\n    if collection_names is None:\n        collection_names = self.collection_names\n\n    return self.collection_data.output_full_metadata(\n        collection_names, with_mappings\n    )\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.TreeData.output_collection_return_types","title":"<code>output_collection_return_types()</code>","text":"<p>Outputs the return types for the collections in the tree data. Essentially, this is a list of the keys that can be used to map the objects to the frontend.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of collection names to their return types. <pre><code>{\n    collection_name_1: list[str],\n    collection_name_2: list[str],\n    ...,\n}\n</code></pre> Each of these lists is a list of the keys that can be used to map the objects to the frontend.</p> Source code in <code>elysia/tree/objects.py</code> <pre><code>def output_collection_return_types(self) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Outputs the return types for the collections in the tree data.\n    Essentially, this is a list of the keys that can be used to map the objects to the frontend.\n\n    Returns:\n        (dict): A dictionary of collection names to their return types.\n            ```python\n            {\n                collection_name_1: list[str],\n                collection_name_2: list[str],\n                ...,\n            }\n            ```\n            Each of these lists is a list of the keys that can be used to map the objects to the frontend.\n    \"\"\"\n    collection_return_types = self.collection_data.output_mapping_lists()\n    out = {\n        collection_name: collection_return_types[collection_name]\n        for collection_name in self.collection_names\n    }\n    return out\n</code></pre>"},{"location":"Reference/Objects/#elysia.tree.objects.TreeData.tasks_completed_string","title":"<code>tasks_completed_string()</code>","text":"<p>Output a nicely formatted string of the tasks completed so far, designed to be used in the LLM prompt. This is where the outputs of the <code>llm_message</code> fields are displayed. You can use this if you are interfacing with LLMs in tools, to help it understand the context of the tasks completed so far.</p> <p>Returns:</p> Type Description <code>str</code> <p>A separated and formatted string of the tasks completed so far in an LLM-parseable format.</p> Source code in <code>elysia/tree/objects.py</code> <pre><code>def tasks_completed_string(self):\n    \"\"\"\n    Output a nicely formatted string of the tasks completed so far, designed to be used in the LLM prompt.\n    This is where the outputs of the `llm_message` fields are displayed.\n    You can use this if you are interfacing with LLMs in tools, to help it understand the context of the tasks completed so far.\n\n    Returns:\n        (str): A separated and formatted string of the tasks completed so far in an LLM-parseable format.\n    \"\"\"\n    out = \"\"\n    for j, task_prompt in enumerate(self.tasks_completed):\n        out += f\"&lt;prompt_{j+1}&gt;\\n\"\n        out += f\"Prompt: {task_prompt['prompt']}\\n\"\n\n        for i, task in enumerate(task_prompt[\"task\"]):\n            out += f\"&lt;task_{i+1}&gt;\\n\"\n\n            if \"action\" in task and task[\"action\"]:\n                out += (\n                    f\"Chosen action: {task['task']} (this does not mean it has been completed, \"\n                    \"only that it was chosen, use the environment to judge if a task is completed)\\n\"\n                )\n            else:\n                out += f\"Chosen subcategory: {task['task']} (this action has not been completed, this is only a subcategory)\"\n\n            if \"error\" in task and task[\"error\"]:\n                out += (\n                    f\" (UNSUCCESSFUL) There was an error during this tool call. \"\n                    \"See the error messages for details. This action did not complete.\\n\"\n                )\n            else:\n                out += f\" (SUCCESSFUL)\\n\"\n                for key in task:\n                    if key != \"task\" and key != \"action\":\n                        out += f\"{key.capitalize()}: {task[key]}\\n\"\n\n            out += f\"&lt;/task_{i+1}&gt;\\n\"\n        out += f\"&lt;/prompt_{j+1}&gt;\\n\"\n\n    return out\n</code></pre>"},{"location":"Reference/PayloadTypes/","title":"Payload Types","text":"<p>Below is the full contents of the <code>return_types.py</code> file, which specifies the different ways that Elysia sends results whenever <code>tree.async_run()</code> yields an object. This is primarily intended for interacting with the frontend - giving the frontend a specific 'payload type' and ensuring that all objects sent conform to the same 'mapping'.</p> <p>Each individual payload type has its own mapping. In this file, the keys of each dictionary are the allowed fields, and the values are descriptions for each field. The <code>specific_return_types</code> has broad descriptions of each class.</p> <pre><code>specific_return_types = {\n    \"conversation\": (\n        \"Full conversations, including all messages and message authors, with timestamps and context of other messages in the conversation. \"\n        \"This type can only be selected if there is a field that uniquely identifies what conversation each message belongs to, e.g. a 'Conversation ID', \"\n        \"as well as a field that uniquely identifies each message within the conversation, e.g. a 'Message ID'.\"\n    ),\n    \"message\": (\n        \"Individual messages, only including the author of each individual message and timestamp, \"\n        \"without surrounding context of other messages by different people. \"\n        \"If the 'conversation' field is suitable, then this is also suitable by definition.\"\n    ),\n    \"ticket\": (\"Support tickets, similar to Github issues or similar.\"),\n    \"product\": (\n        \"Products items, so usually involving descriptions, prices, ratings, reviews, etc, but not always. \"\n        \"Contains an image field, and space for plenty of metadata.\"\n    ),\n    \"document\": (\n        \"Text-based information, optionally with a title, author, date, and content, but not always. \"\n        \"Ideal for any text-based information.\"\n    ),\n}\n\nall_return_types = {\n    **specific_return_types,\n    \"generic\": (\n        \"Any other type of information that does not fit into the more specific categories. \"\n        \"Contains fields for a range of different types of information, and is a good option for a wide range of data if no other display type is available. \"\n    ),\n    \"table\": (\n        \"A table of information, with rows and columns. Used for displaying all of the data in a structured way. \"\n        \"This is a fall-back option if not other display type is available. \"\n        \"Alternatively, if the data or query requires a more analytical insight, this could be a good option.\"\n    ),\n}\n\nconversation = {\n    \"content\": \"the content or text of the message, what was written. string\",\n    \"author\": \"the author of the message. string\",\n    \"timestamp\": \"the timestamp of the message in any format. datetime/string/other\",\n    \"conversation_id\": \"the id of the conversation that the message belongs to. integer/string/other\",\n    \"message_id\": \"the id of the message itself, within the conversation. integer/string/other\",\n}\n\nmessage = {\n    \"content\": \"the content or text of the message, what was written. string\",\n    \"author\": \"the author of the message. string\",\n    \"timestamp\": \"the timestamp of the message in any format. datetime/string/other\",\n    \"conversation_id\": \"the id of the conversation that the message belongs to. integer/string/other\",\n    \"message_id\": \"the id or index of the message, used to either identify a message within a conversation or the message itself. integer/string/other\",\n}\n\nticket = {\n    \"title\": \"the title of the ticket. string\",\n    \"subtitle\": \"the subtitle of the ticket. string\",\n    \"author\": \"the author of the ticket. string\",\n    \"content\": \"the text of the ticket. string\",\n    \"created_at\": \"the timestamp of the original creation time/date of the ticket in any format. datetime/string/other\",\n    \"updated_at\": \"the timestamp of the last update time/date of the ticket in any format. datetime/string/other\",\n    \"url\": \"the url of the ticket. string\",\n    \"status\": \"the status of the ticket. string\",\n    \"id\": \"the id of the ticket. integer/string/other\",\n    \"tags\": \"the tags of the ticket. list[string/other]\",\n    \"comments\": \"either the comments of the ticket, or the number of comments. list[string/dict/other] / integer\",\n}\n\nproduct = {\n    \"name\": \"the name of the product. string\",\n    \"description\": \"the description of the product. string\",\n    \"price\": \"the price of the product. float/integer/other\",\n    \"category\": \"the category of the product. string\",\n    \"subcategory\": \"the subcategory of the product. string\",\n    \"collection\": \"the collection that the product belongs to. string\",\n    \"rating\": \"the rating of the product. float/integer/other\",\n    \"reviews\": \"the reviews of the product, or number of reviews. list[string/dict/other] / integer\",\n    \"tags\": \"the tags of the product. list[string/other]\",\n    \"url\": \"the url of the product. string\",\n    \"image\": \"the image of the product. string/other\",\n    \"brand\": \"the brand of the product. string\",\n    \"id\": \"the id of the product. integer/string/other\",\n    \"colors\": \"the color(s) of the product. list[string/other] / string\",\n    \"sizes\": \"the size(s) of the product. list[string/other] / string\",\n}\n\ndocument = {\n    \"title\": \"the title of the document. string\",\n    \"author\": \"the author or username or creator of the document. string\",\n    \"date\": \"any date or time format. datetime/string/other\",\n    \"content\": \"the textual content of the document. string/other\",\n    \"category\": \"some string describing the category of the document, e.g. type of something. string\",\n}\n\ngeneric = {\n    \"title\": \"the title of the information. string\",\n    \"subtitle\": \"the subtitle of the information. string\",\n    \"content\": \"the content of the information. string/other\",\n    \"url\": \"the url of the information. string\",\n    \"id\": \"the id of the information. integer/string/other\",\n    \"author\": \"the author of the information. string/other\",\n    \"timestamp\": \"the timestamp of the information in any format. datetime/string/other\",\n    \"tags\": \"the tags of the information. list[string/other]\",\n    \"category\": \"some string describing the category of the information, e.g. type of something. string\",\n    \"subcategory\": \"some string describing a nested level of category of the data. string\",\n}\n\ntypes_dict: dict[str, dict[str, str]] = {\n    \"conversation\": conversation,\n    \"message\": message,\n    \"ticket\": ticket,\n    \"product\": product,\n    \"generic\": generic,\n    \"document\": document,\n    \"table\": {},\n}\n</code></pre>"},{"location":"Reference/Preprocessor/","title":"Preprocessor","text":""},{"location":"Reference/Preprocessor/#elysia.preprocessing.collection.delete_preprocessed_collection","title":"<code>delete_preprocessed_collection(collection_name, client_manager=None)</code>","text":"<p>Delete a preprocessed collection. This function allows you to delete the preprocessing done for a particular collection. It does so by deleting the object in the ELYSIA_METADATA__ collection with the name of the collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to delete.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use.</p> <code>None</code> Source code in <code>elysia/preprocessing/collection.py</code> <pre><code>def delete_preprocessed_collection(\n    collection_name: str, client_manager: ClientManager | None = None\n) -&gt; None:\n    \"\"\"\n    Delete a preprocessed collection.\n    This function allows you to delete the preprocessing done for a particular collection.\n    It does so by deleting the object in the ELYSIA_METADATA__ collection with the name of the collection.\n\n    Args:\n        collection_name (str): The name of the collection to delete.\n        client_manager (ClientManager): The client manager to use.\n    \"\"\"\n    return asyncio_run(\n        delete_preprocessed_collection_async(collection_name, client_manager)\n    )\n</code></pre>"},{"location":"Reference/Preprocessor/#elysia.preprocessing.collection.delete_preprocessed_collection_async","title":"<code>delete_preprocessed_collection_async(collection_name, client_manager=None)</code>  <code>async</code>","text":"<p>Delete the preprocessed collection from the Weaviate cluster. This function simply deletes the cached preprocessed metadata from the Weaviate cluster. It does so by deleting the object in the collection ELYSIA_METADATA__ with the name of the collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to delete the preprocessed metadata for.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use. If not provided, a new ClientManager will be created using the environment variables/configured settings.</p> <code>None</code> Source code in <code>elysia/preprocessing/collection.py</code> <pre><code>async def delete_preprocessed_collection_async(\n    collection_name: str, client_manager: ClientManager | None = None\n) -&gt; None:\n    \"\"\"\n    Delete the preprocessed collection from the Weaviate cluster.\n    This function simply deletes the cached preprocessed metadata from the Weaviate cluster.\n    It does so by deleting the object in the collection ELYSIA_METADATA__ with the name of the collection.\n\n    Args:\n        collection_name (str): The name of the collection to delete the preprocessed metadata for.\n        client_manager (ClientManager): The client manager to use.\n            If not provided, a new ClientManager will be created using the environment variables/configured settings.\n    \"\"\"\n    if client_manager is None:\n        client_manager = ClientManager()\n        close_clients_after_completion = True\n    else:\n        close_clients_after_completion = False\n\n    async with client_manager.connect_to_async_client() as client:\n        if await client.collections.exists(f\"ELYSIA_METADATA__\"):\n            metadata_collection = client.collections.get(\"ELYSIA_METADATA__\")\n            metadata = await metadata_collection.query.fetch_objects(\n                filters=Filter.by_property(\"name\").equal(collection_name),\n                limit=1,\n            )\n            if metadata is not None and len(metadata.objects) &gt; 0:\n                await metadata_collection.data.delete_by_id(metadata.objects[0].uuid)\n            else:\n                raise Exception(f\"Metadata for {collection_name} does not exist\")\n\n    if close_clients_after_completion:\n        await client_manager.close_clients()\n</code></pre>"},{"location":"Reference/Preprocessor/#elysia.preprocessing.collection.edit_preprocessed_collection","title":"<code>edit_preprocessed_collection(collection_name, client_manager=None, named_vectors=None, summary=None, mappings=None, fields=None)</code>","text":"<p>Edit a preprocessed collection. This function allows you to edit the named vectors, summary, mappings, and fields of a preprocessed collection. It does so by updating the ELYSIA_METADATA__ collection. Find available mappings in the <code>elysia.util.return_types</code> module.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to edit.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use. If not provided, a new ClientManager will be created using the environment variables/configured settings.</p> <code>None</code> <code>named_vectors</code> <code>list[dict]</code> <p>The named vectors to update. This has fields \"name\", \"enabled\", and \"description\". The \"name\" is used to identify the named vector to change (the name will not change). Set \"enabled\" to True/False to enable/disable the named vector. Set \"description\" to describe the named vector. The description of named vectors is not automatically generated by the LLM. Any named vectors that are not provided will not be updated. If None or not provided, the named vectors will not be updated.</p> <code>None</code> <code>summary</code> <code>str</code> <p>The summary to update. The summary is a short description of the collection, generated by the LLM. This will replace the existing summary of the collection. If None or not provided, the summary will not be updated.</p> <code>None</code> <code>mappings</code> <code>dict</code> <p>The mappings to update. The mappings are what the frontend will use to display the collection, and the associated fields. I.e., which fields correspond to which output fields on the frontend. The keys of the outer level of the dictionary are the mapping names, the values are dictionaries with the mappings. The inner dictionary has the keys as the collection fields, and the values as the frontend fields. If None or not provided, the mappings will not be updated.</p> <code>None</code> <code>fields</code> <code>list[dict]</code> <p>The fields to update. Each element in the list is a dictionary with the following fields: - \"name\": The name of the field. (This is used to identify the field to change, the name will not change). - \"description\": The description of the field to update. Any fields that are not provided will not be updated. If None or not provided, the fields will not be updated.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>None</code> <p>The updated preprocessed collection.</p> Source code in <code>elysia/preprocessing/collection.py</code> <pre><code>def edit_preprocessed_collection(\n    collection_name: str,\n    client_manager: ClientManager | None = None,\n    named_vectors: list[dict] | None = None,\n    summary: str | None = None,\n    mappings: dict[str, dict[str, str]] | None = None,\n    fields: list[dict[str, str] | None] | None = None,\n) -&gt; None:\n    \"\"\"\n    Edit a preprocessed collection.\n    This function allows you to edit the named vectors, summary, mappings, and fields of a preprocessed collection.\n    It does so by updating the ELYSIA_METADATA__ collection.\n    Find available mappings in the `elysia.util.return_types` module.\n\n    Args:\n        collection_name (str): The name of the collection to edit.\n        client_manager (ClientManager): The client manager to use.\n            If not provided, a new ClientManager will be created using the environment variables/configured settings.\n        named_vectors (list[dict]): The named vectors to update. This has fields \"name\", \"enabled\", and \"description\".\n            The \"name\" is used to identify the named vector to change (the name will not change).\n            Set \"enabled\" to True/False to enable/disable the named vector.\n            Set \"description\" to describe the named vector.\n            The description of named vectors is not automatically generated by the LLM.\n            Any named vectors that are not provided will not be updated.\n            If None or not provided, the named vectors will not be updated.\n        summary (str): The summary to update.\n            The summary is a short description of the collection, generated by the LLM.\n            This will replace the existing summary of the collection.\n            If None or not provided, the summary will not be updated.\n        mappings (dict): The mappings to update.\n            The mappings are what the frontend will use to display the collection, and the associated fields.\n            I.e., which fields correspond to which output fields on the frontend.\n            The keys of the outer level of the dictionary are the mapping names, the values are dictionaries with the mappings.\n            The inner dictionary has the keys as the collection fields, and the values as the frontend fields.\n            If None or not provided, the mappings will not be updated.\n        fields (list[dict]): The fields to update.\n            Each element in the list is a dictionary with the following fields:\n            - \"name\": The name of the field. (This is used to identify the field to change, the name will not change).\n            - \"description\": The description of the field to update.\n            Any fields that are not provided will not be updated.\n            If None or not provided, the fields will not be updated.\n\n    Returns:\n        dict: The updated preprocessed collection.\n    \"\"\"\n\n    return asyncio_run(\n        edit_preprocessed_collection_async(\n            collection_name, client_manager, named_vectors, summary, mappings, fields\n        )\n    )\n</code></pre>"},{"location":"Reference/Preprocessor/#elysia.preprocessing.collection.edit_preprocessed_collection_async","title":"<code>edit_preprocessed_collection_async(collection_name, client_manager=None, named_vectors=None, summary=None, mappings=None, fields=None)</code>  <code>async</code>","text":"<p>Async version of <code>edit_preprocessed_collection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to edit.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use. If not provided, a new ClientManager will be created using the environment variables/configured settings.</p> <code>None</code> <code>named_vectors</code> <code>list[dict]</code> <p>The named vectors to update. This has fields \"name\", \"enabled\", and \"description\". The \"name\" is used to identify the named vector to change (the name will not change). Set \"enabled\" to True/False to enable/disable the named vector. Set \"description\" to describe the named vector. The description of named vectors is not automatically generated by the LLM. Any named vectors that are not provided will not be updated. If None or not provided, the named vectors will not be updated.</p> <code>None</code> <code>summary</code> <code>str</code> <p>The summary to update. The summary is a short description of the collection, generated by the LLM. This will replace the existing summary of the collection. If None or not provided, the summary will not be updated.</p> <code>None</code> <code>mappings</code> <code>dict</code> <p>The mappings to update. The mappings are what the frontend will use to display the collection, and the associated fields. I.e., which fields correspond to which output fields on the frontend. The keys of the outer level of the dictionary are the mapping names, the values are dictionaries with the mappings. The inner dictionary has the keys as the collection fields, and the values as the frontend fields. If None or not provided, the mappings will not be updated.</p> <code>None</code> <code>fields</code> <code>list[dict]</code> <p>The fields to update. Each element in the list is a dictionary with the following fields: - \"name\": The name of the field. (This is used to identify the field to change, the name will not change). - \"description\": The description of the field to update. Any fields that are not provided will not be updated. If None or not provided, the fields will not be updated.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated preprocessed collection.</p> Source code in <code>elysia/preprocessing/collection.py</code> <pre><code>async def edit_preprocessed_collection_async(\n    collection_name: str,\n    client_manager: ClientManager | None = None,\n    named_vectors: list[dict] | None = None,\n    summary: str | None = None,\n    mappings: dict[str, dict[str, str]] | None = None,\n    fields: list[dict[str, str] | None] | None = None,\n) -&gt; dict:\n    \"\"\"\n    Async version of `edit_preprocessed_collection`.\n\n    Args:\n        collection_name (str): The name of the collection to edit.\n        client_manager (ClientManager): The client manager to use.\n            If not provided, a new ClientManager will be created using the environment variables/configured settings.\n        named_vectors (list[dict]): The named vectors to update. This has fields \"name\", \"enabled\", and \"description\".\n            The \"name\" is used to identify the named vector to change (the name will not change).\n            Set \"enabled\" to True/False to enable/disable the named vector.\n            Set \"description\" to describe the named vector.\n            The description of named vectors is not automatically generated by the LLM.\n            Any named vectors that are not provided will not be updated.\n            If None or not provided, the named vectors will not be updated.\n        summary (str): The summary to update.\n            The summary is a short description of the collection, generated by the LLM.\n            This will replace the existing summary of the collection.\n            If None or not provided, the summary will not be updated.\n        mappings (dict): The mappings to update.\n            The mappings are what the frontend will use to display the collection, and the associated fields.\n            I.e., which fields correspond to which output fields on the frontend.\n            The keys of the outer level of the dictionary are the mapping names, the values are dictionaries with the mappings.\n            The inner dictionary has the keys as the collection fields, and the values as the frontend fields.\n            If None or not provided, the mappings will not be updated.\n        fields (list[dict]): The fields to update.\n            Each element in the list is a dictionary with the following fields:\n            - \"name\": The name of the field. (This is used to identify the field to change, the name will not change).\n            - \"description\": The description of the field to update.\n            Any fields that are not provided will not be updated.\n            If None or not provided, the fields will not be updated.\n\n    Returns:\n        dict: The updated preprocessed collection.\n    \"\"\"\n\n    if client_manager is None:\n        client_manager = ClientManager()\n        close_clients_after_completion = True\n    else:\n        close_clients_after_completion = False\n\n    async with client_manager.connect_to_async_client() as client:\n        metadata_name = f\"ELYSIA_METADATA__\"\n\n        # check if the collection itself exists\n        if not await client.collections.exists(collection_name):\n            raise Exception(f\"Collection {collection_name} does not exist\")\n\n        # check if the metadata collection exists\n        if not await client.collections.exists(metadata_name):\n            raise Exception(f\"Metadata collection does not exist\")\n\n        else:\n            metadata_collection = client.collections.get(metadata_name)\n            metadata = await metadata_collection.query.fetch_objects(\n                filters=Filter.by_property(\"name\").equal(collection_name),\n                limit=1,\n            )\n            uuid = metadata.objects[0].uuid\n            properties: dict = metadata.objects[0].properties  # type: ignore\n\n        # update the named vectors\n        if named_vectors is not None:\n            for named_vector in named_vectors:\n                for property_named_vector in properties[\"named_vectors\"]:\n                    if property_named_vector[\"name\"] == named_vector[\"name\"]:\n\n                        if named_vector[\"enabled\"] is not None:\n                            property_named_vector[\"enabled\"] = named_vector[\"enabled\"]\n\n                        if named_vector[\"description\"] is not None:\n                            property_named_vector[\"description\"] = named_vector[\n                                \"description\"\n                            ]\n\n        # update the summary\n        if summary is not None:\n            properties[\"summary\"] = summary\n\n        # update the mappings\n        if mappings is not None:\n            if \"table\" in mappings:\n                mappings[\"table\"] = {\n                    field[\"name\"]: field[\"name\"] for field in properties[\"fields\"]\n                }\n\n            # format all mappings\n            for mapping_type, mapping in mappings.items():\n                if mapping_type == \"table\":\n                    continue\n\n                # check if the mapping_type is valid\n                if mapping_type not in rt.types_dict:\n                    raise ValueError(\n                        f\"Invalid mapping type: {mapping_type}. Valid mapping types are: {list(rt.types_dict.keys())}\"\n                    )\n\n                # check if the mapping is valid\n                for field_name, field_value in mapping.items():\n                    if field_name not in rt.types_dict[mapping_type]:\n                        raise ValueError(\n                            f\"Invalid field name: {field_name} for mapping type: {mapping_type}. \"\n                            f\"Valid fields are: {list(rt.types_dict[mapping_type].keys())}\"\n                        )\n\n                # add empty fields\n                for true_field in rt.types_dict[mapping_type]:\n                    if true_field not in mapping:\n                        mapping[true_field] = \"\"\n\n            properties[\"mappings\"] = mappings\n\n            # check if the `conversation_id` field is in the mapping (required for conversation type)\n            if \"conversation\" in mappings and (\n                mappings[\"conversation\"][\"conversation_id\"] is None\n                or mappings[\"conversation\"][\"conversation_id\"] == \"\"\n            ):\n                raise ValueError(\n                    \"Conversation type requires a conversation_id field, but none was found in the mappings for conversation. \"\n                )\n\n            if \"conversation\" in mappings and (\n                mappings[\"conversation\"][\"message_id\"] is None\n                or mappings[\"conversation\"][\"message_id\"] == \"\"\n            ):\n                raise ValueError(\n                    \"Conversation type requires a message_id field, but none was found in the mappings for conversation. \"\n                )\n\n            # check if there is a message type as well as conversation\n            if \"message\" not in mappings and \"conversation\" in mappings:\n                raise ValueError(\n                    \"Conversation type requires message type to also be set as a fallback.\"\n                )\n\n        # update the fields\n        if fields is not None:\n            for field in fields:\n                for property_field in properties[\"fields\"]:\n                    if field is not None and property_field[\"name\"] == field[\"name\"]:\n                        property_field[\"description\"] = field[\"description\"]\n\n        format_dict_to_serialisable(properties)\n\n        # update the collection\n        await metadata_collection.data.update(uuid=uuid, properties=properties)\n\n    if close_clients_after_completion:\n        await client_manager.close_clients()\n\n    return properties\n</code></pre>"},{"location":"Reference/Preprocessor/#elysia.preprocessing.collection.preprocess","title":"<code>preprocess(collection_names, client_manager=None, min_sample_size=10, max_sample_size=None, num_sample_tokens=30000, settings=environment_settings, force=False)</code>","text":"<p>Preprocess a collection, obtain a LLM-generated summary of the collection, a set of statistics for each field (such as unique categories), and a set of mappings from the fields to the frontend-specific fields in Elysia.</p> <p>In order: 1. Evaluate all the data fields and groups/statistics of the data fields as a whole 2. Write a summary of the collection via an LLM 3. Evaluate what return types are available for this collection 4. For each data field in the collection, evaluate what corresponding entry goes to what field in the return type (mapping) 5. Save as a ELYSIA_METADATA__ collection</p> <p>Depending on the size of objects in the collection, you can choose the minimum and maximum sample size, which will be used to create a sample of objects for the LLM to create a collection summary. If your objects are particularly large, you can set the sample size to be smaller, to use less tokens and speed up the LLM processing. If your objects are small, you can set the sample size to be larger, to get a more accurate summary. This is a trade-off between speed/compute and accuracy.</p> <p>But note that the pre-processing step only needs to be done once for each collection. The output of this function is cached, so that if you run it again, it will not re-process the collection (unless the force flag is set to True).</p> <p>This function saves the output into a collection called ELYSIA_METADATA__, which is automatically called by Elysia. This is saved to whatever Weaviate cluster URL/API key you have configured, or in your environment variables. You can change this by setting the <code>wcd_url</code> and <code>wcd_api_key</code> in the settings, and pass this Settings object to this function.</p> <p>Parameters:</p> Name Type Description Default <code>collection_names</code> <code>str | list[str]</code> <p>The name(s) of the collections to preprocess. Can supply either a single string for one collection, or a list of strings for multiple collections.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use. If not provided, a new ClientManager will be created using the environment variables/configured settings.</p> <code>None</code> <code>min_sample_size</code> <code>int</code> <p>The minimum number of objects to sample from the collection to evaluate the statistics/summary. Optional, defaults to 10.</p> <code>10</code> <code>max_sample_size</code> <code>int</code> <p>The maximum number of objects to sample from the collection to evaluate the statistics/summary. Optional, defaults to 20.</p> <code>None</code> <code>num_sample_tokens</code> <code>int</code> <p>The maximum number of tokens in the sample objects used to evaluate the summary. Optional, defaults to 30000.</p> <code>30000</code> <code>settings</code> <code>Settings</code> <p>The settings to use. Optional, defaults to the environment variables/configured settings.</p> <code>settings</code> <code>force</code> <code>bool</code> <p>Whether to force the preprocessor to run even if the collection already exists. Optional, defaults to False.</p> <code>False</code> Source code in <code>elysia/preprocessing/collection.py</code> <pre><code>def preprocess(\n    collection_names: str | list[str],\n    client_manager: ClientManager | None = None,\n    min_sample_size: int = 10,\n    max_sample_size: int | None = None,\n    num_sample_tokens: int = 30000,\n    settings: Settings = environment_settings,\n    force: bool = False,\n) -&gt; None:\n    \"\"\"\n    Preprocess a collection, obtain a LLM-generated summary of the collection,\n    a set of statistics for each field (such as unique categories), and a set of mappings\n    from the fields to the frontend-specific fields in Elysia.\n\n    In order:\n    1. Evaluate all the data fields and groups/statistics of the data fields as a whole\n    2. Write a summary of the collection via an LLM\n    3. Evaluate what return types are available for this collection\n    4. For each data field in the collection, evaluate what corresponding entry goes to what field in the return type (mapping)\n    5. Save as a ELYSIA_METADATA__ collection\n\n    Depending on the size of objects in the collection, you can choose the minimum and maximum sample size,\n    which will be used to create a sample of objects for the LLM to create a collection summary.\n    If your objects are particularly large, you can set the sample size to be smaller, to use less tokens and speed up the LLM processing.\n    If your objects are small, you can set the sample size to be larger, to get a more accurate summary.\n    This is a trade-off between speed/compute and accuracy.\n\n    But note that the pre-processing step only needs to be done once for each collection.\n    The output of this function is cached, so that if you run it again, it will not re-process the collection (unless the force flag is set to True).\n\n    This function saves the output into a collection called ELYSIA_METADATA__, which is automatically called by Elysia.\n    This is saved to whatever Weaviate cluster URL/API key you have configured, or in your environment variables.\n    You can change this by setting the `wcd_url` and `wcd_api_key` in the settings, and pass this Settings object to this function.\n\n    Args:\n        collection_names (str | list[str]): The name(s) of the collections to preprocess.\n            Can supply either a single string for one collection, or a list of strings for multiple collections.\n        client_manager (ClientManager): The client manager to use.\n            If not provided, a new ClientManager will be created using the environment variables/configured settings.\n        min_sample_size (int): The minimum number of objects to sample from the collection to evaluate the statistics/summary. Optional, defaults to 10.\n        max_sample_size (int): The maximum number of objects to sample from the collection to evaluate the statistics/summary. Optional, defaults to 20.\n        num_sample_tokens (int): The maximum number of tokens in the sample objects used to evaluate the summary. Optional, defaults to 30000.\n        settings (Settings): The settings to use. Optional, defaults to the environment variables/configured settings.\n        force (bool): Whether to force the preprocessor to run even if the collection already exists. Optional, defaults to False.\n    \"\"\"\n\n    asyncio_run(\n        _preprocess_async(\n            collection_names,\n            client_manager,\n            min_sample_size,\n            max_sample_size,\n            num_sample_tokens,\n            settings,\n            force,\n        )\n    )\n</code></pre>"},{"location":"Reference/Preprocessor/#elysia.preprocessing.collection.preprocess_async","title":"<code>preprocess_async(collection_name, client_manager=None, min_sample_size=10, max_sample_size=None, num_sample_tokens=30000, force=False, percentage_correct_threshold=0.3, settings=environment_settings)</code>  <code>async</code>","text":"<p>Preprocess a collection, obtain a LLM-generated summary of the collection, a set of statistics for each field (such as unique categories), and a set of mappings from the fields to the frontend-specific fields in Elysia.</p> <p>In order:</p> <ol> <li>Evaluate all the data fields and groups/statistics of the data fields as a whole</li> <li>Write a summary of the collection via an LLM</li> <li>Evaluate what return types are available for this collection</li> <li>For each data field in the collection, evaluate what corresponding entry goes to what field in the return type (mapping)</li> <li>Save as a ELYSIA_METADATA__ collection</li> </ol> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to preprocess.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use.</p> <code>None</code> <code>min_sample_size</code> <code>int</code> <p>The minimum number of objects to sample from the collection to evaluate the statistics/summary. Optional, defaults to 10.</p> <code>10</code> <code>max_sample_size</code> <code>int</code> <p>The maximum number of objects to sample from the collection to evaluate the statistics/summary. Optional, defaults to 20.</p> <code>None</code> <code>num_sample_tokens</code> <code>int</code> <p>The number of tokens to approximately sample from the collection to evaluate the summary. The preprocessor will aim to use this many tokens in the sample objects to evaluate the summary. But will not exceed the maximum number of objects specified by <code>max_sample_size</code>, and always use at least <code>min_sample_size</code> objects.</p> <code>30000</code> <code>force</code> <code>bool</code> <p>Whether to force the preprocessor to run even if the collection already exists. Optional, defaults to False.</p> <code>False</code> <code>threshold_for_missing_fields</code> <code>float</code> <p>The threshold for the number of missing fields in the data mapping. Optional, defaults to 0.1.</p> required <code>settings</code> <code>Settings</code> <p>The settings to use. Optional, defaults to the environment variables/configured settings.</p> <code>settings</code> <p>Returns:</p> Type Description <code>AsyncGenerator[dict, None]</code> <p>AsyncGenerator[dict, None]: A generator that yields dictionaries with the status updates and progress of the preprocessor.</p> Source code in <code>elysia/preprocessing/collection.py</code> <pre><code>async def preprocess_async(\n    collection_name: str,\n    client_manager: ClientManager | None = None,\n    min_sample_size: int = 10,\n    max_sample_size: int | None = None,\n    num_sample_tokens: int = 30000,\n    force: bool = False,\n    percentage_correct_threshold: float = 0.3,\n    settings: Settings = environment_settings,\n) -&gt; AsyncGenerator[dict, None]:\n    \"\"\"\n    Preprocess a collection, obtain a LLM-generated summary of the collection,\n    a set of statistics for each field (such as unique categories), and a set of mappings\n    from the fields to the frontend-specific fields in Elysia.\n\n    In order:\n\n    1. Evaluate all the data fields and groups/statistics of the data fields as a whole\n    2. Write a summary of the collection via an LLM\n    3. Evaluate what return types are available for this collection\n    4. For each data field in the collection, evaluate what corresponding entry goes to what field in the return type (mapping)\n    5. Save as a ELYSIA_METADATA__ collection\n\n    Args:\n        collection_name (str): The name of the collection to preprocess.\n        client_manager (ClientManager): The client manager to use.\n        min_sample_size (int): The minimum number of objects to sample from the collection to evaluate the statistics/summary. Optional, defaults to 10.\n        max_sample_size (int): The maximum number of objects to sample from the collection to evaluate the statistics/summary. Optional, defaults to 20.\n        num_sample_tokens (int): The number of tokens to approximately sample from the collection to evaluate the summary.\n            The preprocessor will aim to use this many tokens in the sample objects to evaluate the summary.\n            But will not exceed the maximum number of objects specified by `max_sample_size`, and always use at least `min_sample_size` objects.\n        force (bool): Whether to force the preprocessor to run even if the collection already exists. Optional, defaults to False.\n        threshold_for_missing_fields (float): The threshold for the number of missing fields in the data mapping. Optional, defaults to 0.1.\n        settings (Settings): The settings to use. Optional, defaults to the environment variables/configured settings.\n\n    Returns:\n        AsyncGenerator[dict, None]: A generator that yields dictionaries with the status updates and progress of the preprocessor.\n    \"\"\"\n\n    collection_summariser_prompt = dspy.ChainOfThought(CollectionSummariserPrompt)\n    return_type_prompt = dspy.ChainOfThought(ReturnTypePrompt)\n    data_mapping_prompt = dspy.ChainOfThought(DataMappingPrompt)\n    prompt_suggestor_prompt = dspy.ChainOfThought(PromptSuggestorPrompt)\n\n    lm = load_base_lm(settings)\n    logger = settings.logger\n    process_update = ProcessUpdate(collection_name, len(rt.specific_return_types) + 5)\n\n    if client_manager is None:\n        client_manager = ClientManager(\n            wcd_url=settings.WCD_URL, wcd_api_key=settings.WCD_API_KEY\n        )\n        close_clients_after_completion = True\n    else:\n        close_clients_after_completion = False\n\n    try:\n        # Check if the collection exists\n        async with client_manager.connect_to_async_client() as client:\n            if not await client.collections.exists(collection_name):\n                raise Exception(f\"Collection {collection_name} does not exist!\")\n\n        # Check if the preprocessed collection exists\n        if (\n            await preprocessed_collection_exists_async(collection_name, client_manager)\n            and not force\n        ):\n            logger.info(f\"Preprocessed metadata for {collection_name} already exists!\")\n            return\n\n        # Get the collection and its properties\n        async with client_manager.connect_to_async_client() as client:\n            collection = client.collections.get(collection_name)\n            properties = await async_get_collection_data_types(client, collection_name)\n\n        # get number of items in collection\n        agg = await collection.aggregate.over_all(total_count=True)\n        len_collection: int = agg.total_count  # type: ignore\n\n        if max_sample_size is None and len_collection &gt; 50_000:\n            max_sample_size = 20\n            logger.warning(\n                f\"Collection is large (greater than 50,000 objects), causing slowdown in pre-processing. \"\n                f\"Reducing maximum sample size to {max_sample_size} objects. \"\n                \"To override this, set `max_sample_size` as an argument to preprocess.\"\n            )\n        elif max_sample_size is None:\n            max_sample_size = 50\n\n        # Randomly sample sample_size objects for the summary\n        indices = random.sample(\n            range(min(99_999, len_collection)),\n            max(min(max_sample_size, len_collection), 1),\n        )\n\n        # Get first object to estimate token count\n        obj = await collection.query.fetch_objects(limit=1, offset=indices[0])\n        token_count_0 = len(nlp(str(obj.objects[0].properties)))\n        subset_objects: list[dict] = [obj.objects[0].properties]  # type: ignore\n\n        # Get number of objects to sample to get close to num_sample_tokens\n        num_sample_objects = min(\n            max(min_sample_size, num_sample_tokens // token_count_0),\n            max_sample_size,\n        )\n\n        # Estimate number of tokens\n        logger.debug(\n            f\"Estimated token count of sample: {token_count_0*num_sample_objects}\"\n        )\n        logger.debug(f\"Number of objects in sample: {num_sample_objects}\")\n\n        for index in indices[1:num_sample_objects]:\n            obj = await collection.query.fetch_objects(limit=1, offset=index)\n            subset_objects.append(obj.objects[0].properties)  # type: ignore\n\n        # Summarise the collection using LLM and the subset of the data\n        summary, field_descriptions = await _summarise_collection(\n            collection_summariser_prompt,\n            properties,\n            subset_objects,\n            len_collection,\n            settings,\n            lm,\n        )\n\n        yield await process_update(\n            message=\"Generated summary of collection\",\n        )\n\n        if len_collection &gt; 10_000:  # arbitrary cutoff for estimating field statistics\n            full_response = subset_objects\n        else:\n            weaviate_resp = await collection.query.fetch_objects(limit=len_collection)\n            full_response = [obj.properties for obj in weaviate_resp.objects]\n\n        # Initialise the output\n        named_vectors, vectoriser = await _find_vectorisers(collection)\n        out = {\n            \"name\": collection_name,\n            \"length\": len_collection,\n            \"summary\": summary,\n            \"index_properties\": await _evaluate_index_properties(collection),\n            \"named_vectors\": named_vectors,\n            \"vectorizer\": vectoriser,\n            \"fields\": [],\n            \"mappings\": {},\n        }\n\n        # Evaluate the summary statistics of each field\n        for property in properties:\n            out[\"fields\"].append(\n                await _evaluate_field_statistics(\n                    collection, properties, property, len_collection, full_response\n                )\n            )\n            if property in field_descriptions:\n                out[\"fields\"][-1][\"description\"] = field_descriptions[property]\n            else:\n                out[\"fields\"][-1][\"description\"] = \"\"\n\n        yield await process_update(\n            message=\"Evaluated field statistics\",\n        )\n\n        return_types = await _evaluate_return_types(\n            return_type_prompt,\n            summary,\n            properties,\n            subset_objects,\n            settings,\n            lm,\n        )\n\n        yield await process_update(\n            message=\"Evaluated return types\",\n        )\n        process_update.update_total(len(return_types) + 5)\n\n        # suggest prompts\n        out[\"prompts\"] = await _suggest_prompts(\n            prompt_suggestor_prompt,\n            out,\n            subset_objects,\n            settings,\n            lm,\n        )\n\n        yield await process_update(\n            message=\"Created suggestions for prompts\",\n        )\n\n        # For each return type created above, define the mappings from the properties to the frontend types\n        mappings = {}\n        for return_type in return_types:\n            fields = rt.types_dict[return_type]\n\n            mapping = await _define_mappings(\n                data_mapping_prompt,\n                mapping_type=return_type,\n                input_fields=list(fields.keys()),\n                output_fields=list(properties.keys()),\n                properties=properties,\n                collection_information=out,\n                example_objects=subset_objects,\n                settings=settings,\n                lm=lm,\n            )\n\n            # remove any extra fields the model may have added\n            mapping = {k: v for k, v in mapping.items() if k in list(fields.keys())}\n\n            yield await process_update(\n                message=f\"Defined mappings for {return_type}\",\n            )\n\n            mappings[return_type] = mapping\n\n        new_return_types = []\n        for return_type in return_types:\n\n            # check if the `conversation_id` field is in the mapping (required for conversation type)\n            if return_type == \"conversation\" and (\n                (\n                    mappings[return_type][\"conversation_id\"] is None\n                    or mappings[return_type][\"conversation_id\"] == \"\"\n                )\n                or (\n                    mappings[return_type][\"message_id\"] is None\n                    or mappings[return_type][\"message_id\"] == \"\"\n                )\n            ):\n                continue\n\n            # If less than threshold_for_missing_fields%, keep the return type\n            num_missing = sum([m == \"\" for m in list(mappings[return_type].values())])\n            perc_correct = 1 - (num_missing / len(mappings[return_type].keys()))\n            if perc_correct &gt;= percentage_correct_threshold:\n                new_return_types.append(return_type)\n\n        # If no return types are left, fall-back to generic\n        if len(new_return_types) == 0:\n\n            # Map for generic\n            mapping = await _define_mappings(\n                data_mapping_prompt,\n                mapping_type=\"generic\",\n                input_fields=list(rt.generic.keys()),\n                output_fields=list(properties.keys()),\n                properties=properties,\n                collection_information=out,\n                example_objects=subset_objects,\n                settings=settings,\n                lm=lm,\n            )\n            yield await process_update(\n                message=\"No display types found, defined mappings for generic\",\n            )\n\n            # remove any extra fields the model may have added\n            mapping = {k: v for k, v in mapping.items() if k in list(rt.generic.keys())}\n            mappings[\"generic\"] = mapping\n\n            # re-check the threshold for missing fields on generic\n            num_missing = sum([m == \"\" for m in list(mappings[return_type].values())])\n            perc_correct = 1 - (num_missing / len(mappings[return_type].keys()))\n            if perc_correct &gt;= percentage_correct_threshold:\n                new_return_types = [\"generic\"]\n\n        # Add the mappings to the output\n        out[\"mappings\"] = {\n            return_type: mappings[return_type] for return_type in new_return_types\n        }\n\n        # always include the table return type\n        out[\"mappings\"][\"table\"] = {field: field for field in properties.keys()}\n\n        # Delete existing metadata if it exists\n        if await preprocessed_collection_exists_async(collection_name, client_manager):\n            await delete_preprocessed_collection_async(collection_name, client_manager)\n\n        # Save final metadata to a collection\n        async with client_manager.connect_to_async_client() as client:\n            if await client.collections.exists(f\"ELYSIA_METADATA__\"):\n                metadata_collection = client.collections.get(\"ELYSIA_METADATA__\")\n            else:\n                metadata_collection = await client.collections.create(\n                    f\"ELYSIA_METADATA__\",\n                    vectorizer_config=Configure.Vectorizer.none(),\n                    properties=[\n                        Property(\n                            name=\"name\",\n                            data_type=DataType.TEXT,\n                            tokenization=Tokenization.FIELD,\n                        ),\n                        Property(\n                            name=\"length\",\n                            data_type=DataType.NUMBER,\n                        ),\n                        Property(\n                            name=\"summary\",\n                            data_type=DataType.TEXT,\n                        ),\n                        Property(\n                            name=\"index_properties\",\n                            data_type=DataType.OBJECT,\n                            nested_properties=[\n                                Property(\n                                    name=\"isNullIndexed\",\n                                    data_type=DataType.BOOL,\n                                ),\n                                Property(\n                                    name=\"isLengthIndexed\",\n                                    data_type=DataType.BOOL,\n                                ),\n                                Property(\n                                    name=\"isTimestampIndexed\",\n                                    data_type=DataType.BOOL,\n                                ),\n                            ],\n                        ),\n                        Property(\n                            name=\"named_vectors\",\n                            data_type=DataType.OBJECT_ARRAY,\n                            nested_properties=[\n                                Property(\n                                    name=\"name\",\n                                    data_type=DataType.TEXT,\n                                ),\n                                Property(\n                                    name=\"vectorizer\",\n                                    data_type=DataType.TEXT,\n                                ),\n                                Property(\n                                    name=\"model\",\n                                    data_type=DataType.TEXT,\n                                ),\n                                Property(\n                                    name=\"source_properties\",\n                                    data_type=DataType.TEXT_ARRAY,\n                                ),\n                                Property(\n                                    name=\"enabled\",\n                                    data_type=DataType.BOOL,\n                                ),\n                                Property(\n                                    name=\"description\",\n                                    data_type=DataType.TEXT,\n                                ),\n                            ],\n                        ),\n                        Property(\n                            name=\"vectorizer\",\n                            data_type=DataType.OBJECT,\n                            nested_properties=[\n                                Property(name=\"vectorizer\", data_type=DataType.TEXT),\n                                Property(name=\"model\", data_type=DataType.TEXT),\n                            ],\n                        ),\n                        Property(\n                            name=\"fields\",\n                            data_type=DataType.OBJECT_ARRAY,\n                            nested_properties=[\n                                Property(\n                                    name=\"name\",\n                                    data_type=DataType.TEXT,\n                                ),\n                                Property(\n                                    name=\"type\",\n                                    data_type=DataType.TEXT,\n                                ),\n                                Property(\n                                    name=\"description\",\n                                    data_type=DataType.TEXT,\n                                ),\n                                Property(\n                                    name=\"range\",\n                                    data_type=DataType.NUMBER_ARRAY,\n                                ),\n                                Property(\n                                    name=\"date_range\",\n                                    data_type=DataType.DATE_ARRAY,\n                                ),\n                                Property(\n                                    name=\"groups\",\n                                    data_type=DataType.OBJECT_ARRAY,\n                                    nested_properties=[\n                                        Property(\n                                            name=\"value\",\n                                            data_type=DataType.TEXT,\n                                        ),\n                                        Property(name=\"count\", data_type=DataType.INT),\n                                    ],\n                                ),\n                                Property(\n                                    name=\"date_median\",\n                                    data_type=DataType.DATE,\n                                ),\n                                Property(\n                                    name=\"mean\",\n                                    data_type=DataType.NUMBER,\n                                ),\n                            ],\n                        ),\n                        # leave mappings for auto-schema generation\n                    ],\n                    inverted_index_config=Configure.inverted_index(\n                        index_null_state=True,\n                    ),\n                )\n            await metadata_collection.data.insert(out)\n\n        yield await process_update(\n            completed=True,\n            message=\"Saved metadata to Weaviate\",\n        )\n\n    except Exception as e:\n        yield await process_update(\n            error=f\"Error preprocessing collection: {str(e)}\",\n        )\n</code></pre>"},{"location":"Reference/Preprocessor/#elysia.preprocessing.collection.preprocessed_collection_exists","title":"<code>preprocessed_collection_exists(collection_name, client_manager=None)</code>","text":"<p>Check if the preprocessed collection exists in the Weaviate cluster. This function simply checks if the cached preprocessed metadata exists in the Weaviate cluster. It does so by checking if the collection name exists in the ELYSIA_METADATA__ collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to check.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use.</p> <code>None</code> Source code in <code>elysia/preprocessing/collection.py</code> <pre><code>def preprocessed_collection_exists(\n    collection_name: str, client_manager: ClientManager | None = None\n) -&gt; bool:\n    \"\"\"\n    Check if the preprocessed collection exists in the Weaviate cluster.\n    This function simply checks if the cached preprocessed metadata exists in the Weaviate cluster.\n    It does so by checking if the collection name exists in the ELYSIA_METADATA__ collection.\n\n    Args:\n        collection_name (str): The name of the collection to check.\n        client_manager (ClientManager): The client manager to use.\n    \"\"\"\n    return asyncio_run(\n        preprocessed_collection_exists_async(collection_name, client_manager)\n    )\n</code></pre>"},{"location":"Reference/Preprocessor/#elysia.preprocessing.collection.preprocessed_collection_exists_async","title":"<code>preprocessed_collection_exists_async(collection_name, client_manager=None)</code>  <code>async</code>","text":"<p>Async version of <code>preprocessed_collection_exists</code>.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to check.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use. If not provided, a new ClientManager will be created using the environment variables/configured settings.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the collection exists, False otherwise.</p> Source code in <code>elysia/preprocessing/collection.py</code> <pre><code>async def preprocessed_collection_exists_async(\n    collection_name: str, client_manager: ClientManager | None = None\n) -&gt; bool:\n    \"\"\"\n    Async version of `preprocessed_collection_exists`.\n\n    Args:\n        collection_name (str): The name of the collection to check.\n        client_manager (ClientManager): The client manager to use.\n            If not provided, a new ClientManager will be created using the environment variables/configured settings.\n\n    Returns:\n        bool: True if the collection exists, False otherwise.\n    \"\"\"\n    if client_manager is None:\n        client_manager = ClientManager()\n        close_clients_after_completion = True\n    else:\n        close_clients_after_completion = False\n\n    async with client_manager.connect_to_async_client() as client:\n        metadata_exists = await client.collections.exists(f\"ELYSIA_METADATA__\")\n        if not metadata_exists:\n            return False\n\n        metadata_collection = client.collections.get(\"ELYSIA_METADATA__\")\n        metadata = await metadata_collection.query.fetch_objects(\n            filters=Filter.by_property(\"name\").equal(collection_name)\n        )\n\n    if close_clients_after_completion:\n        await client_manager.close_clients()\n\n    return metadata is not None and len(metadata.objects) &gt; 0\n</code></pre>"},{"location":"Reference/Preprocessor/#elysia.preprocessing.collection.view_preprocessed_collection","title":"<code>view_preprocessed_collection(collection_name, client_manager=None)</code>","text":"<p>View a preprocessed collection. This function allows you to view the preprocessed collection generated by the preprocess function. It does so by querying the ELYSIA_METADATA__ collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to view.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use. If not provided, a new ClientManager will be created using the environment variables/configured settings.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The preprocessed collection.</p> Source code in <code>elysia/preprocessing/collection.py</code> <pre><code>def view_preprocessed_collection(\n    collection_name: str, client_manager: ClientManager | None = None\n) -&gt; dict:\n    \"\"\"\n    View a preprocessed collection.\n    This function allows you to view the preprocessed collection generated by the preprocess function.\n    It does so by querying the ELYSIA_METADATA__ collection.\n\n    Args:\n        collection_name (str): The name of the collection to view.\n        client_manager (ClientManager): The client manager to use.\n            If not provided, a new ClientManager will be created using the environment variables/configured settings.\n\n    Returns:\n        dict: The preprocessed collection.\n    \"\"\"\n    return asyncio_run(\n        view_preprocessed_collection_async(collection_name, client_manager)\n    )\n</code></pre>"},{"location":"Reference/Preprocessor/#elysia.preprocessing.collection.view_preprocessed_collection_async","title":"<code>view_preprocessed_collection_async(collection_name, client_manager=None)</code>  <code>async</code>","text":"<p>Async version of <code>view_preprocessed_collection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to view.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use. If not provided, a new ClientManager will be created using the environment variables/configured settings.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The preprocessed collection.</p> Source code in <code>elysia/preprocessing/collection.py</code> <pre><code>async def view_preprocessed_collection_async(\n    collection_name: str, client_manager: ClientManager | None = None\n) -&gt; dict:\n    \"\"\"\n    Async version of `view_preprocessed_collection`.\n\n    Args:\n        collection_name (str): The name of the collection to view.\n        client_manager (ClientManager): The client manager to use.\n            If not provided, a new ClientManager will be created using the environment variables/configured settings.\n\n    Returns:\n        dict: The preprocessed collection.\n    \"\"\"\n\n    if client_manager is None:\n        client_manager = ClientManager()\n        close_clients_after_completion = True\n    else:\n        close_clients_after_completion = False\n\n    async with client_manager.connect_to_async_client() as client:\n        metadata_name = f\"ELYSIA_METADATA__\"\n\n        # check if the collection itself exists\n        if not await client.collections.exists(collection_name):\n            raise Exception(f\"Collection {collection_name} does not exist\")\n\n        # check if the metadata collection exists\n        if not await client.collections.exists(metadata_name):\n            raise Exception(f\"Metadata collection does not exist\")\n\n        else:\n            metadata_collection = client.collections.get(metadata_name)\n            metadata = await metadata_collection.query.fetch_objects(\n                filters=Filter.by_property(\"name\").equal(collection_name),\n                limit=1,\n            )\n            if len(metadata.objects) == 0:\n                raise Exception(f\"Metadata for {collection_name} does not exist\")\n\n            properties: dict = metadata.objects[0].properties  # type: ignore\n\n    if close_clients_after_completion:\n        await client_manager.close_clients()\n\n    return properties\n</code></pre>"},{"location":"Reference/Settings/","title":"Settings","text":""},{"location":"Reference/Settings/#elysia.config.Settings","title":"<code>Settings</code>","text":"<p>Settings for Elysia. This class handles the configuration of various settings within Elysia. This includes: - The base and complex models to use. - The providers for the base and complex models. - The Weaviate cloud URL and API key. - Whether the Weaviate cluster is local. - The API keys for the providers. - The logger and logging level.</p> <p>The Settings object is set as at a default <code>settings</code> object if running Elysia as a package. You can import this via <code>elysia.config.settings</code>. This is initialised to default values from the environment variables.</p> <p>Or, you can create your own Settings object, and configure it as you wish. The Settings object can be passed to different Elysia classes and functions, such as <code>Tree</code> and <code>preprocess</code>. These will not use the global <code>settings</code> object, but instead use the Settings object you passed to them.</p> Source code in <code>elysia/config.py</code> <pre><code>class Settings:\n    \"\"\"\n    Settings for Elysia.\n    This class handles the configuration of various settings within Elysia.\n    This includes:\n    - The base and complex models to use.\n    - The providers for the base and complex models.\n    - The Weaviate cloud URL and API key.\n    - Whether the Weaviate cluster is local.\n    - The API keys for the providers.\n    - The logger and logging level.\n\n    The Settings object is set as at a default `settings` object if running Elysia as a package.\n    You can import this via `elysia.config.settings`.\n    This is initialised to default values from the environment variables.\n\n    Or, you can create your own Settings object, and configure it as you wish.\n    The Settings object can be passed to different Elysia classes and functions, such as `Tree` and `preprocess`.\n    These will not use the global `settings` object, but instead use the Settings object you passed to them.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the settings for Elysia.\n        These are all settings initialised to None, and should be set using the `configure` method.\n        \"\"\"\n        # Default settings\n        self.SETTINGS_ID = str(random.randint(100000000000000, 999999999999999))\n\n        self.base_init()\n\n    def base_init(self):\n        self.BASE_MODEL: str | None = None\n        self.BASE_PROVIDER: str | None = None\n        self.COMPLEX_MODEL: str | None = None\n        self.COMPLEX_PROVIDER: str | None = None\n\n        self.WCD_URL: str = \"\"\n        self.WCD_API_KEY: str = \"\"\n        self.WEAVIATE_IS_LOCAL: bool = False\n        self.LOCAL_WEAVIATE_PORT: int = 8080\n        self.LOCAL_WEAVIATE_GRPC_PORT: int = 50051\n        self.MODEL_API_BASE: str | None = None\n\n        self.API_KEYS: dict[str, str] = {}\n\n        self.logger = logging.getLogger(\"rich\")\n        self.logger.setLevel(logging.INFO)\n\n        # Remove any existing handlers before adding a new one\n        for handler in self.logger.handlers[:]:\n            self.logger.removeHandler(handler)\n        self.logger.addHandler(RichHandler(rich_tracebacks=True, markup=True))\n\n        self.logger.propagate = False\n        self.LOGGING_LEVEL = \"INFO\"\n        self.LOGGING_LEVEL_INT = 20\n\n        # Experimental features\n        self.USE_FEEDBACK = False\n        self.BASE_USE_REASONING = True\n        self.COMPLEX_USE_REASONING = True\n\n    def setup_app_logger(self, logger: logging.Logger):\n        \"\"\"\n        Override existing logger with the app-level logger.\n\n        Args:\n            logger (Logger): The logger to use.\n        \"\"\"\n        self.logger = logger\n        self.LOGGING_LEVEL_INT = logger.level\n        inverted_logging_mapping = {\n            v: k for k, v in logging.getLevelNamesMapping().items()\n        }\n        self.LOGGING_LEVEL = inverted_logging_mapping[self.LOGGING_LEVEL_INT]\n\n    def configure_logger(\n        self,\n        level: Literal[\n            \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\", \"NOTSET\"\n        ] = \"NOTSET\",\n    ):\n        \"\"\"\n        Configure the logger with a RichHandler.\n\n        Args:\n            level (Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\", \"NOTSET\"]): The logging level to use.\n        \"\"\"\n        self.logger.setLevel(level)\n        self.LOGGING_LEVEL = level\n        self.LOGGING_LEVEL_INT = logging.getLevelNamesMapping()[level]\n\n    def set_api_key(self, api_key: str, api_key_name: str) -&gt; None:\n        self.API_KEYS[api_key_name] = api_key\n\n    def get_api_key(self, api_key_name: str) -&gt; str:\n        return self.API_KEYS[api_key_name]\n\n    def load_settings(self, settings: dict):\n        for item in settings:\n            setattr(self, item, settings[item])\n\n        # self.logger = logging.getLogger(\"rich\")\n        # self.logger.setLevel(self.LOGGING_LEVEL)\n        # for handler in self.logger.handlers[:]:\n        #     self.logger.removeHandler(handler)\n        # self.logger.addHandler(RichHandler(rich_tracebacks=True, markup=True))\n\n    @classmethod\n    def from_smart_setup(cls):\n        settings = cls()\n        settings.set_from_env()\n        settings.smart_setup()\n        return settings\n\n    @classmethod\n    def from_env_vars(cls):\n        settings = cls()\n        settings.set_from_env()\n        return settings\n\n    def set_from_env(self):\n        self.BASE_MODEL = os.getenv(\"BASE_MODEL\", None)\n        self.COMPLEX_MODEL = os.getenv(\"COMPLEX_MODEL\", None)\n        self.BASE_PROVIDER = os.getenv(\"BASE_PROVIDER\", None)\n        self.COMPLEX_PROVIDER = os.getenv(\"COMPLEX_PROVIDER\", None)\n        self.MODEL_API_BASE = os.getenv(\"MODEL_API_BASE\", None)\n        self.LOGGING_LEVEL = os.getenv(\"LOGGING_LEVEL\", \"NOTSET\")\n        self.WEAVIATE_IS_LOCAL = os.getenv(\"WEAVIATE_IS_LOCAL\", \"False\") == \"True\"\n        self.LOCAL_WEAVIATE_PORT = os.getenv(\"LOCAL_WEAVIATE_PORT\", 8080)\n        self.LOCAL_WEAVIATE_GRPC_PORT = os.getenv(\"LOCAL_WEAVIATE_GRPC_PORT\", 50051)\n        self.set_api_keys_from_env()\n\n    def set_api_keys_from_env(self):\n\n        self.WCD_URL = os.getenv(\n            \"WEAVIATE_URL\",\n            os.getenv(\"WCD_URL\", \"\"),\n        )\n        self.WCD_API_KEY = os.getenv(\n            \"WEAVIATE_API_KEY\",\n            os.getenv(\"WCD_API_KEY\", \"\"),\n        )\n        self.LOCAL_WEAVIATE_PORT = os.getenv(\"LOCAL_WEAVIATE_PORT\", 8080)\n        self.LOCAL_WEAVIATE_GRPC_PORT = os.getenv(\"LOCAL_WEAVIATE_GRPC_PORT\", 50051)\n\n        self.API_KEYS = {\n            env_var.lower(): os.getenv(env_var, \"\")\n            for env_var in os.environ\n            if is_api_key(env_var) and env_var.lower() != \"wcd_api_key\"\n        }\n        for api_key in self.API_KEYS:\n            self.set_api_key(self.API_KEYS[api_key], api_key)\n\n    def smart_setup(self):\n\n        # Check if the user has set the base model etc from the environment variables\n        if os.getenv(\"BASE_MODEL\", None):\n            self.BASE_MODEL = os.getenv(\"BASE_MODEL\")\n        if os.getenv(\"COMPLEX_MODEL\", None):\n            self.COMPLEX_MODEL = os.getenv(\"COMPLEX_MODEL\")\n        if os.getenv(\"BASE_PROVIDER\", None):\n            self.BASE_PROVIDER = os.getenv(\"BASE_PROVIDER\")\n        if os.getenv(\"COMPLEX_PROVIDER\", None):\n            self.COMPLEX_PROVIDER = os.getenv(\"COMPLEX_PROVIDER\")\n\n        self.MODEL_API_BASE = os.getenv(\"MODEL_API_BASE\", None)\n        self.LOGGING_LEVEL = os.getenv(\"LOGGING_LEVEL\", \"NOTSET\")\n\n        self.set_from_env()\n\n        # check what API keys are available\n        if (\n            self.BASE_MODEL is None\n            or self.COMPLEX_MODEL is None\n            or self.BASE_PROVIDER is None\n            or self.COMPLEX_PROVIDER is None\n        ):\n            if os.getenv(\"OPENROUTER_API_KEY\", None):\n                # use gemini 2.0 flash\n                self.BASE_PROVIDER = \"openrouter/google\"\n                self.COMPLEX_PROVIDER = \"openrouter/google\"\n                self.BASE_MODEL = \"gemini-2.0-flash-001\"\n                self.COMPLEX_MODEL = \"gemini-2.5-flash\"\n            elif os.getenv(\"GEMINI_API_KEY\", None):\n                # use gemini 2.0 flash\n                self.BASE_PROVIDER = \"gemini\"\n                self.COMPLEX_PROVIDER = \"gemini\"\n                self.BASE_MODEL = \"gemini-2.0-flash-001\"\n                self.COMPLEX_MODEL = \"gemini-2.5-flash\"\n            elif os.getenv(\"OPENAI_API_KEY\", None):\n                # use gpt family\n                self.BASE_PROVIDER = \"openai\"\n                self.COMPLEX_PROVIDER = \"openai\"\n                self.BASE_MODEL = \"gpt-4.1-mini\"\n                self.COMPLEX_MODEL = \"gpt-4.1\"\n            elif os.getenv(\"ANTHROPIC_API_KEY\", None):\n                # use claude family\n                self.BASE_PROVIDER = \"anthropic\"\n                self.COMPLEX_PROVIDER = \"anthropic\"\n                self.BASE_MODEL = \"claude-3-5-haiku-latest\"\n                self.COMPLEX_MODEL = \"claude-sonnet-4-0\"\n\n    def configure(\n        self,\n        replace: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Configure the settings for Elysia for the current Settings object.\n\n        Args:\n            replace (bool): Whether to override the current settings with the new settings.\n                When this is True, all existing settings are removed, and only the new settings are used.\n                Defaults to False.\n            **kwargs (str): One or more of the following:\n                - base_model (str): The base model to use. e.g. \"gpt-4o-mini\"\n                - complex_model (str): The complex model to use. e.g. \"gpt-4o\"\n                - base_provider (str): The provider to use for base_model. E.g. \"openai\" or \"openrouter/openai\"\n                - complex_provider (str): The provider to use for complex_model. E.g. \"openai\" or \"openrouter/openai\"\n                - model_api_base (str): The API base to use.\n                - wcd_url (str): The Weaviate cloud URL to use.\n                - wcd_api_key (str): The Weaviate cloud API key to use.\n                - weaviate_is_local (bool): Whether the Weaviate cluster is local.\n                - local_weaviate_port (int): The port to use for the local Weaviate cluster.\n                - local_weaviate_grpc_port (int): The gRPC port to use for the local Weaviate cluster.\n                - logging_level (str): The logging level to use. e.g. \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"\n                - use_feedback (bool): EXPERIMENTAL. Whether to use feedback from previous runs of the tree.\n                    If True, the tree will use TrainingUpdate objects that have been saved in previous runs of the decision tree.\n                    These are implemented via few-shot examples for the decision node.\n                    They are collected in the 'feedback' collection (ELYSIA_FEEDBACK__).\n                    Relevant examples are retrieved from the collection based on searching the collection via the user's prompt.\n                - base_use_reasoning (bool): Whether to use reasoning output for the base model.\n                    If True, the model will generate reasoning before coming to its solution.\n                - complex_use_reasoning (bool): Whether to use reasoning output for the complex model.\n                    If True, the model will generate reasoning before coming to its solution.\n                - Additional API keys to set. E.g. `openai_apikey=\"...\"`, if this argument ends with `apikey` or `api_key`,\n                    it will be added to the `API_KEYS` dictionary.\n\n        \"\"\"\n        if replace:\n            self.base_init()\n\n        # convert all kwargs to lowercase for consistency\n        kwargs = {kwarg.lower(): kwargs[kwarg] for kwarg in kwargs}\n\n        if \"base_model\" in kwargs:\n            if \"base_provider\" not in kwargs:\n                raise ValueError(\n                    \"Provider must be specified if base_model is set. \"\n                    \"E.g. `elysia.config.configure(base_model='gpt-4o-mini', base_provider='openai')`\"\n                )\n\n            if kwargs[\"base_provider\"] == \"ollama\" and (\n                not kwargs[\"model_api_base\"] or \"MODEL_API_BASE\" not in dir(self)\n            ):\n                raise ValueError(\n                    \"Using local models via ollama requires MODEL_API_BASE to be set. \"\n                    \"This is likely to be http://localhost:11434. \"\n                    \"e.g. `elysia.config.configure(model_api_base='http://localhost:11434')`\"\n                )\n\n            self.BASE_MODEL = kwargs[\"base_model\"]\n            self.BASE_PROVIDER = kwargs[\"base_provider\"]\n\n            kwargs.pop(\"base_model\")\n            kwargs.pop(\"base_provider\")\n\n            # self.load_base_dspy_model()\n\n        if \"complex_model\" in kwargs:\n            if \"complex_provider\" not in kwargs:\n                raise ValueError(\n                    \"Provider must be specified if complex_model is set. \"\n                    \"E.g. `elysia.config.configure(complex_model='gpt-4o', complex_provider='openai')`\"\n                )\n\n            if kwargs[\"complex_provider\"] == \"ollama\" and (\n                not kwargs[\"model_api_base\"] or \"MODEL_API_BASE\" not in dir(self)\n            ):\n                raise ValueError(\n                    \"Using local models via ollama requires MODEL_API_BASE to be set. \"\n                    \"This is likely to be http://localhost:11434. \"\n                    \"e.g. `elysia.config.configure(model_api_base='http://localhost:11434')`\"\n                )\n\n            self.COMPLEX_MODEL = kwargs[\"complex_model\"]\n            self.COMPLEX_PROVIDER = kwargs[\"complex_provider\"]\n\n            kwargs.pop(\"complex_model\")\n            kwargs.pop(\"complex_provider\")\n\n            # self.load_complex_dspy_model()\n\n        if \"model_api_base\" in kwargs:\n            self.MODEL_API_BASE = kwargs[\"model_api_base\"]\n            kwargs.pop(\"model_api_base\")\n\n        if \"wcd_url\" in kwargs:\n            self.WCD_URL = kwargs[\"wcd_url\"]\n            kwargs.pop(\"wcd_url\")\n\n        if \"wcd_api_key\" in kwargs:\n            self.WCD_API_KEY = kwargs[\"wcd_api_key\"]\n            kwargs.pop(\"wcd_api_key\")\n\n        if \"weaviate_is_local\" in kwargs:\n            self.WEAVIATE_IS_LOCAL = kwargs[\"weaviate_is_local\"]\n            kwargs.pop(\"weaviate_is_local\")\n\n        if \"local_weaviate_port\" in kwargs:\n            self.LOCAL_WEAVIATE_PORT = kwargs[\"local_weaviate_port\"]\n            kwargs.pop(\"local_weaviate_port\")\n\n        if \"local_weaviate_grpc_port\" in kwargs:\n            self.LOCAL_WEAVIATE_GRPC_PORT = kwargs[\"local_weaviate_grpc_port\"]\n            kwargs.pop(\"local_weaviate_grpc_port\")\n\n        if \"weaviate_url\" in kwargs:\n            self.WCD_URL = kwargs[\"weaviate_url\"]\n            kwargs.pop(\"weaviate_url\")\n\n        if \"weaviate_api_key\" in kwargs:\n            self.WCD_API_KEY = kwargs[\"weaviate_api_key\"]\n            kwargs.pop(\"weaviate_api_key\")\n\n        if \"logging_level\" in kwargs or \"logger_level\" in kwargs:\n\n            self.LOGGING_LEVEL = (\n                kwargs[\"logging_level\"]\n                if \"logging_level\" in kwargs\n                else kwargs[\"logger_level\"]\n            )\n            self.LOGGING_LEVEL_INT = logging.getLevelNamesMapping()[self.LOGGING_LEVEL]\n            self.logger.setLevel(self.LOGGING_LEVEL)\n            if \"logging_level\" in kwargs:\n                kwargs.pop(\"logging_level\")\n            if \"logger_level\" in kwargs:\n                kwargs.pop(\"logger_level\")\n            if \"logging_level_int\" in kwargs:\n                kwargs.pop(\"logging_level_int\")\n            if \"logger_level_int\" in kwargs:\n                kwargs.pop(\"logger_level_int\")\n\n        if \"logging_level_int\" in kwargs or \"logger_level_int\" in kwargs:\n\n            self.LOGGING_LEVEL_INT = (\n                kwargs[\"logging_level_int\"]\n                if \"logging_level_int\" in kwargs\n                else kwargs[\"logger_level_int\"]\n            )\n            self.LOGGING_LEVEL = {\n                v: k for k, v in logging.getLevelNamesMapping().items()\n            }[self.LOGGING_LEVEL_INT]\n            self.logger.setLevel(self.LOGGING_LEVEL)\n\n        if \"settings_id\" in kwargs:\n            self.SETTINGS_ID = kwargs[\"settings_id\"]\n            kwargs.pop(\"settings_id\")\n\n        if \"use_feedback\" in kwargs:\n            self.USE_FEEDBACK = kwargs[\"use_feedback\"]\n            kwargs.pop(\"use_feedback\")\n\n        if \"base_use_reasoning\" in kwargs:\n            self.BASE_USE_REASONING = kwargs[\"base_use_reasoning\"]\n            kwargs.pop(\"base_use_reasoning\")\n\n        if \"complex_use_reasoning\" in kwargs:\n            self.COMPLEX_USE_REASONING = kwargs[\"complex_use_reasoning\"]\n            kwargs.pop(\"complex_use_reasoning\")\n\n        if \"api_keys\" in kwargs and isinstance(kwargs[\"api_keys\"], dict):\n            for key, value in kwargs[\"api_keys\"].items():\n                self.set_api_key(value, key)\n            kwargs.pop(\"api_keys\")\n\n        # remainder of kwargs are API keys or saved there\n        removed_kwargs = []\n        for key, value in kwargs.items():\n            if is_api_key(key):\n                self.set_api_key(value, key)\n                removed_kwargs.append(key)\n\n        for key in removed_kwargs:\n            kwargs.pop(key)\n\n        # remaining kwargs are not API keys\n        if len(kwargs) &gt; 0:\n            self.logger.warning(\n                \"Unknown arguments to configure: \" + \", \".join(kwargs.keys())\n            )\n\n    def __repr__(self) -&gt; str:\n        out = \"\"\n        if \"BASE_MODEL\" in dir(self) and self.BASE_MODEL is not None:\n            out += f\"Base model: {self.BASE_MODEL}\\n\"\n        else:\n            out += \"Base model: not set\\n\"\n        if \"COMPLEX_MODEL\" in dir(self) and self.COMPLEX_MODEL is not None:\n            out += f\"Complex model: {self.COMPLEX_MODEL}\\n\"\n        else:\n            out += \"Complex model: not set\\n\"\n        if \"BASE_PROVIDER\" in dir(self) and self.BASE_PROVIDER is not None:\n            out += f\"Base provider: {self.BASE_PROVIDER}\\n\"\n        else:\n            out += \"Base provider: not set\\n\"\n        if \"COMPLEX_PROVIDER\" in dir(self) and self.COMPLEX_PROVIDER is not None:\n            out += f\"Complex provider: {self.COMPLEX_PROVIDER}\\n\"\n        else:\n            out += \"Complex provider: not set\\n\"\n        if \"MODEL_API_BASE\" in dir(self):\n            out += f\"Model API base: {self.MODEL_API_BASE}\\n\"\n        else:\n            out += \"Model API base: not set\\n\"\n        return out\n\n    def to_json(self):\n        return {\n            item: getattr(self, item)\n            for item in dir(self)\n            if not item.startswith(\"_\")\n            and not isinstance(getattr(self, item), Callable)\n            and item not in [\"BASE_MODEL_LM\", \"COMPLEX_MODEL_LM\", \"logger\"]\n        }\n\n    @classmethod\n    def from_json(cls, json_data: dict):\n        settings = cls()\n        for item in json_data:\n            if item not in [\"logger\"]:\n                setattr(settings, item, json_data[item])\n\n        settings.logger.setLevel(settings.LOGGING_LEVEL)\n\n        return settings\n\n    def check(self):\n        return {\n            \"base_model\": self.BASE_MODEL is not None and self.BASE_MODEL != \"\",\n            \"base_provider\": self.BASE_PROVIDER is not None\n            and self.BASE_PROVIDER != \"\",\n            \"complex_model\": self.COMPLEX_MODEL is not None\n            and self.COMPLEX_MODEL != \"\",\n            \"complex_provider\": self.COMPLEX_PROVIDER is not None\n            and self.COMPLEX_PROVIDER != \"\",\n            \"wcd_url\": self.WCD_URL != \"\",\n            \"wcd_api_key\": self.WCD_API_KEY != \"\",\n            \"weaviate_is_local\": self.WEAVIATE_IS_LOCAL,\n            \"local_weaviate_port\": self.LOCAL_WEAVIATE_PORT != 8080,\n            \"local_weaviate_grpc_port\": self.LOCAL_WEAVIATE_GRPC_PORT != 50051,\n        }\n</code></pre>"},{"location":"Reference/Settings/#elysia.config.Settings.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the settings for Elysia. These are all settings initialised to None, and should be set using the <code>configure</code> method.</p> Source code in <code>elysia/config.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the settings for Elysia.\n    These are all settings initialised to None, and should be set using the `configure` method.\n    \"\"\"\n    # Default settings\n    self.SETTINGS_ID = str(random.randint(100000000000000, 999999999999999))\n\n    self.base_init()\n</code></pre>"},{"location":"Reference/Settings/#elysia.config.Settings.configure","title":"<code>configure(replace=False, **kwargs)</code>","text":"<p>Configure the settings for Elysia for the current Settings object.</p> <p>Parameters:</p> Name Type Description Default <code>replace</code> <code>bool</code> <p>Whether to override the current settings with the new settings. When this is True, all existing settings are removed, and only the new settings are used. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>str</code> <p>One or more of the following: - base_model (str): The base model to use. e.g. \"gpt-4o-mini\" - complex_model (str): The complex model to use. e.g. \"gpt-4o\" - base_provider (str): The provider to use for base_model. E.g. \"openai\" or \"openrouter/openai\" - complex_provider (str): The provider to use for complex_model. E.g. \"openai\" or \"openrouter/openai\" - model_api_base (str): The API base to use. - wcd_url (str): The Weaviate cloud URL to use. - wcd_api_key (str): The Weaviate cloud API key to use. - weaviate_is_local (bool): Whether the Weaviate cluster is local. - local_weaviate_port (int): The port to use for the local Weaviate cluster. - local_weaviate_grpc_port (int): The gRPC port to use for the local Weaviate cluster. - logging_level (str): The logging level to use. e.g. \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\" - use_feedback (bool): EXPERIMENTAL. Whether to use feedback from previous runs of the tree.     If True, the tree will use TrainingUpdate objects that have been saved in previous runs of the decision tree.     These are implemented via few-shot examples for the decision node.     They are collected in the 'feedback' collection (ELYSIA_FEEDBACK__).     Relevant examples are retrieved from the collection based on searching the collection via the user's prompt. - base_use_reasoning (bool): Whether to use reasoning output for the base model.     If True, the model will generate reasoning before coming to its solution. - complex_use_reasoning (bool): Whether to use reasoning output for the complex model.     If True, the model will generate reasoning before coming to its solution. - Additional API keys to set. E.g. <code>openai_apikey=\"...\"</code>, if this argument ends with <code>apikey</code> or <code>api_key</code>,     it will be added to the <code>API_KEYS</code> dictionary.</p> <code>{}</code> Source code in <code>elysia/config.py</code> <pre><code>def configure(\n    self,\n    replace: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Configure the settings for Elysia for the current Settings object.\n\n    Args:\n        replace (bool): Whether to override the current settings with the new settings.\n            When this is True, all existing settings are removed, and only the new settings are used.\n            Defaults to False.\n        **kwargs (str): One or more of the following:\n            - base_model (str): The base model to use. e.g. \"gpt-4o-mini\"\n            - complex_model (str): The complex model to use. e.g. \"gpt-4o\"\n            - base_provider (str): The provider to use for base_model. E.g. \"openai\" or \"openrouter/openai\"\n            - complex_provider (str): The provider to use for complex_model. E.g. \"openai\" or \"openrouter/openai\"\n            - model_api_base (str): The API base to use.\n            - wcd_url (str): The Weaviate cloud URL to use.\n            - wcd_api_key (str): The Weaviate cloud API key to use.\n            - weaviate_is_local (bool): Whether the Weaviate cluster is local.\n            - local_weaviate_port (int): The port to use for the local Weaviate cluster.\n            - local_weaviate_grpc_port (int): The gRPC port to use for the local Weaviate cluster.\n            - logging_level (str): The logging level to use. e.g. \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"\n            - use_feedback (bool): EXPERIMENTAL. Whether to use feedback from previous runs of the tree.\n                If True, the tree will use TrainingUpdate objects that have been saved in previous runs of the decision tree.\n                These are implemented via few-shot examples for the decision node.\n                They are collected in the 'feedback' collection (ELYSIA_FEEDBACK__).\n                Relevant examples are retrieved from the collection based on searching the collection via the user's prompt.\n            - base_use_reasoning (bool): Whether to use reasoning output for the base model.\n                If True, the model will generate reasoning before coming to its solution.\n            - complex_use_reasoning (bool): Whether to use reasoning output for the complex model.\n                If True, the model will generate reasoning before coming to its solution.\n            - Additional API keys to set. E.g. `openai_apikey=\"...\"`, if this argument ends with `apikey` or `api_key`,\n                it will be added to the `API_KEYS` dictionary.\n\n    \"\"\"\n    if replace:\n        self.base_init()\n\n    # convert all kwargs to lowercase for consistency\n    kwargs = {kwarg.lower(): kwargs[kwarg] for kwarg in kwargs}\n\n    if \"base_model\" in kwargs:\n        if \"base_provider\" not in kwargs:\n            raise ValueError(\n                \"Provider must be specified if base_model is set. \"\n                \"E.g. `elysia.config.configure(base_model='gpt-4o-mini', base_provider='openai')`\"\n            )\n\n        if kwargs[\"base_provider\"] == \"ollama\" and (\n            not kwargs[\"model_api_base\"] or \"MODEL_API_BASE\" not in dir(self)\n        ):\n            raise ValueError(\n                \"Using local models via ollama requires MODEL_API_BASE to be set. \"\n                \"This is likely to be http://localhost:11434. \"\n                \"e.g. `elysia.config.configure(model_api_base='http://localhost:11434')`\"\n            )\n\n        self.BASE_MODEL = kwargs[\"base_model\"]\n        self.BASE_PROVIDER = kwargs[\"base_provider\"]\n\n        kwargs.pop(\"base_model\")\n        kwargs.pop(\"base_provider\")\n\n        # self.load_base_dspy_model()\n\n    if \"complex_model\" in kwargs:\n        if \"complex_provider\" not in kwargs:\n            raise ValueError(\n                \"Provider must be specified if complex_model is set. \"\n                \"E.g. `elysia.config.configure(complex_model='gpt-4o', complex_provider='openai')`\"\n            )\n\n        if kwargs[\"complex_provider\"] == \"ollama\" and (\n            not kwargs[\"model_api_base\"] or \"MODEL_API_BASE\" not in dir(self)\n        ):\n            raise ValueError(\n                \"Using local models via ollama requires MODEL_API_BASE to be set. \"\n                \"This is likely to be http://localhost:11434. \"\n                \"e.g. `elysia.config.configure(model_api_base='http://localhost:11434')`\"\n            )\n\n        self.COMPLEX_MODEL = kwargs[\"complex_model\"]\n        self.COMPLEX_PROVIDER = kwargs[\"complex_provider\"]\n\n        kwargs.pop(\"complex_model\")\n        kwargs.pop(\"complex_provider\")\n\n        # self.load_complex_dspy_model()\n\n    if \"model_api_base\" in kwargs:\n        self.MODEL_API_BASE = kwargs[\"model_api_base\"]\n        kwargs.pop(\"model_api_base\")\n\n    if \"wcd_url\" in kwargs:\n        self.WCD_URL = kwargs[\"wcd_url\"]\n        kwargs.pop(\"wcd_url\")\n\n    if \"wcd_api_key\" in kwargs:\n        self.WCD_API_KEY = kwargs[\"wcd_api_key\"]\n        kwargs.pop(\"wcd_api_key\")\n\n    if \"weaviate_is_local\" in kwargs:\n        self.WEAVIATE_IS_LOCAL = kwargs[\"weaviate_is_local\"]\n        kwargs.pop(\"weaviate_is_local\")\n\n    if \"local_weaviate_port\" in kwargs:\n        self.LOCAL_WEAVIATE_PORT = kwargs[\"local_weaviate_port\"]\n        kwargs.pop(\"local_weaviate_port\")\n\n    if \"local_weaviate_grpc_port\" in kwargs:\n        self.LOCAL_WEAVIATE_GRPC_PORT = kwargs[\"local_weaviate_grpc_port\"]\n        kwargs.pop(\"local_weaviate_grpc_port\")\n\n    if \"weaviate_url\" in kwargs:\n        self.WCD_URL = kwargs[\"weaviate_url\"]\n        kwargs.pop(\"weaviate_url\")\n\n    if \"weaviate_api_key\" in kwargs:\n        self.WCD_API_KEY = kwargs[\"weaviate_api_key\"]\n        kwargs.pop(\"weaviate_api_key\")\n\n    if \"logging_level\" in kwargs or \"logger_level\" in kwargs:\n\n        self.LOGGING_LEVEL = (\n            kwargs[\"logging_level\"]\n            if \"logging_level\" in kwargs\n            else kwargs[\"logger_level\"]\n        )\n        self.LOGGING_LEVEL_INT = logging.getLevelNamesMapping()[self.LOGGING_LEVEL]\n        self.logger.setLevel(self.LOGGING_LEVEL)\n        if \"logging_level\" in kwargs:\n            kwargs.pop(\"logging_level\")\n        if \"logger_level\" in kwargs:\n            kwargs.pop(\"logger_level\")\n        if \"logging_level_int\" in kwargs:\n            kwargs.pop(\"logging_level_int\")\n        if \"logger_level_int\" in kwargs:\n            kwargs.pop(\"logger_level_int\")\n\n    if \"logging_level_int\" in kwargs or \"logger_level_int\" in kwargs:\n\n        self.LOGGING_LEVEL_INT = (\n            kwargs[\"logging_level_int\"]\n            if \"logging_level_int\" in kwargs\n            else kwargs[\"logger_level_int\"]\n        )\n        self.LOGGING_LEVEL = {\n            v: k for k, v in logging.getLevelNamesMapping().items()\n        }[self.LOGGING_LEVEL_INT]\n        self.logger.setLevel(self.LOGGING_LEVEL)\n\n    if \"settings_id\" in kwargs:\n        self.SETTINGS_ID = kwargs[\"settings_id\"]\n        kwargs.pop(\"settings_id\")\n\n    if \"use_feedback\" in kwargs:\n        self.USE_FEEDBACK = kwargs[\"use_feedback\"]\n        kwargs.pop(\"use_feedback\")\n\n    if \"base_use_reasoning\" in kwargs:\n        self.BASE_USE_REASONING = kwargs[\"base_use_reasoning\"]\n        kwargs.pop(\"base_use_reasoning\")\n\n    if \"complex_use_reasoning\" in kwargs:\n        self.COMPLEX_USE_REASONING = kwargs[\"complex_use_reasoning\"]\n        kwargs.pop(\"complex_use_reasoning\")\n\n    if \"api_keys\" in kwargs and isinstance(kwargs[\"api_keys\"], dict):\n        for key, value in kwargs[\"api_keys\"].items():\n            self.set_api_key(value, key)\n        kwargs.pop(\"api_keys\")\n\n    # remainder of kwargs are API keys or saved there\n    removed_kwargs = []\n    for key, value in kwargs.items():\n        if is_api_key(key):\n            self.set_api_key(value, key)\n            removed_kwargs.append(key)\n\n    for key in removed_kwargs:\n        kwargs.pop(key)\n\n    # remaining kwargs are not API keys\n    if len(kwargs) &gt; 0:\n        self.logger.warning(\n            \"Unknown arguments to configure: \" + \", \".join(kwargs.keys())\n        )\n</code></pre>"},{"location":"Reference/Settings/#elysia.config.Settings.configure_logger","title":"<code>configure_logger(level='NOTSET')</code>","text":"<p>Configure the logger with a RichHandler.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>Literal['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL', 'NOTSET']</code> <p>The logging level to use.</p> <code>'NOTSET'</code> Source code in <code>elysia/config.py</code> <pre><code>def configure_logger(\n    self,\n    level: Literal[\n        \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\", \"NOTSET\"\n    ] = \"NOTSET\",\n):\n    \"\"\"\n    Configure the logger with a RichHandler.\n\n    Args:\n        level (Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\", \"NOTSET\"]): The logging level to use.\n    \"\"\"\n    self.logger.setLevel(level)\n    self.LOGGING_LEVEL = level\n    self.LOGGING_LEVEL_INT = logging.getLevelNamesMapping()[level]\n</code></pre>"},{"location":"Reference/Settings/#elysia.config.Settings.setup_app_logger","title":"<code>setup_app_logger(logger)</code>","text":"<p>Override existing logger with the app-level logger.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>The logger to use.</p> required Source code in <code>elysia/config.py</code> <pre><code>def setup_app_logger(self, logger: logging.Logger):\n    \"\"\"\n    Override existing logger with the app-level logger.\n\n    Args:\n        logger (Logger): The logger to use.\n    \"\"\"\n    self.logger = logger\n    self.LOGGING_LEVEL_INT = logger.level\n    inverted_logging_mapping = {\n        v: k for k, v in logging.getLevelNamesMapping().items()\n    }\n    self.LOGGING_LEVEL = inverted_logging_mapping[self.LOGGING_LEVEL_INT]\n</code></pre>"},{"location":"Reference/Settings/#elysia.config.configure","title":"<code>configure(**kwargs)</code>","text":"<p>Configure the settings for Elysia for the global settings object.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>str</code> <p>One or more of the following: - base_model (str): The base model to use. e.g. \"gpt-4o-mini\" - complex_model (str): The complex model to use. e.g. \"gpt-4o\" - base_provider (str): The provider to use for base_model. E.g. \"openai\" or \"openrouter/openai\" - complex_provider (str): The provider to use for complex_model. E.g. \"openai\" or \"openrouter/openai\" - model_api_base (str): The API base to use. - wcd_url (str): The Weaviate cloud URL to use. - wcd_api_key (str): The Weaviate cloud API key to use. - weaviate_is_local (bool): Whether the Weaviate cluster is local. - local_weaviate_port (int): The port to use for the local Weaviate cluster. - local_weaviate_grpc_port (int): The gRPC port to use for the local Weaviate cluster. - logging_level (str): The logging level to use. e.g. \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\" - use_feedback (bool): EXPERIMENTAL. Whether to use feedback from previous runs of the tree.     If True, the tree will use TrainingUpdate objects that have been saved in previous runs of the decision tree.     These are implemented via few-shot examples for the decision node.     They are collected in the 'feedback' collection (ELYSIA_FEEDBACK__).     Relevant examples are retrieved from the collection based on searching the collection via the user's prompt. - Additional API keys to set. E.g. <code>openai_apikey=\"...\"</code>, if this argument ends with <code>apikey</code> or <code>api_key</code>,     it will be added to the <code>API_KEYS</code> dictionary.</p> <code>{}</code> Source code in <code>elysia/config.py</code> <pre><code>def configure(**kwargs) -&gt; None:\n    \"\"\"\n    Configure the settings for Elysia for the global settings object.\n\n    Args:\n        **kwargs (str): One or more of the following:\n            - base_model (str): The base model to use. e.g. \"gpt-4o-mini\"\n            - complex_model (str): The complex model to use. e.g. \"gpt-4o\"\n            - base_provider (str): The provider to use for base_model. E.g. \"openai\" or \"openrouter/openai\"\n            - complex_provider (str): The provider to use for complex_model. E.g. \"openai\" or \"openrouter/openai\"\n            - model_api_base (str): The API base to use.\n            - wcd_url (str): The Weaviate cloud URL to use.\n            - wcd_api_key (str): The Weaviate cloud API key to use.\n            - weaviate_is_local (bool): Whether the Weaviate cluster is local.\n            - local_weaviate_port (int): The port to use for the local Weaviate cluster.\n            - local_weaviate_grpc_port (int): The gRPC port to use for the local Weaviate cluster.\n            - logging_level (str): The logging level to use. e.g. \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"\n            - use_feedback (bool): EXPERIMENTAL. Whether to use feedback from previous runs of the tree.\n                If True, the tree will use TrainingUpdate objects that have been saved in previous runs of the decision tree.\n                These are implemented via few-shot examples for the decision node.\n                They are collected in the 'feedback' collection (ELYSIA_FEEDBACK__).\n                Relevant examples are retrieved from the collection based on searching the collection via the user's prompt.\n            - Additional API keys to set. E.g. `openai_apikey=\"...\"`, if this argument ends with `apikey` or `api_key`,\n                it will be added to the `API_KEYS` dictionary.\n\n    \"\"\"\n    settings.configure(**kwargs)\n</code></pre>"},{"location":"Reference/Tree/","title":"Tree","text":""},{"location":"Reference/Tree/#elysia.tree.tree.Tree","title":"<code>Tree</code>","text":"<p>The main class for the Elysia decision tree. Calling this method will execute the decision tree based on the user's prompt, and available collections and tools.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>class Tree:\n    \"\"\"\n    The main class for the Elysia decision tree.\n    Calling this method will execute the decision tree based on the user's prompt, and available collections and tools.\n    \"\"\"\n\n    def __init__(\n        self,\n        branch_initialisation: Literal[\n            \"default\", \"one_branch\", \"multi_branch\", \"empty\"\n        ] = \"default\",\n        style: str = \"No style provided.\",\n        agent_description: str = \"No description provided.\",\n        end_goal: str = \"No end goal provided.\",\n        user_id: str | None = None,\n        conversation_id: str | None = None,\n        low_memory: bool = False,\n        use_elysia_collections: bool = True,\n        settings: Settings | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            branch_initialisation (str): The initialisation method for the branches,\n                currently supports some pre-defined initialisations: \"multi_branch\", \"one_branch\".\n                Set to \"empty\" to start with no branches and to add them, and the tools, yourself.\n            style (str): The writing style of the agent. Automatically set for \"multi_branch\" and \"one_branch\" initialisation, but overrided if non-empty.\n            agent_description (str): The description of the agent. Automatically set for \"multi_branch\" and \"one_branch\" initialisation, but overrided if non-empty.\n            end_goal (str): The end goal of the agent. Automatically set for \"multi_branch\" and \"one_branch\" initialisation, but overrided if non-empty.\n            user_id (str): The id of the user, e.g. \"123-456\",\n                unneeded outside of user management/hosting Elysia app\n            conversation_id (str): The id of the conversation, e.g. \"123-456\",\n                unneeded outside of conversation management/hosting Elysia app\n            low_memory (bool): Whether to run the tree in low memory mode.\n                If True, the tree will not load the (dspy) models within the tree.\n                Set to False for normal operation.\n            use_elysia_collections (bool): Whether to use weaviate collections as processed by Elysia.\n                If False, the tree will not use the processed collections.\n            settings (Settings): The settings for the tree, an object of elysia.Settings.\n                This is automatically set to the environment settings if not provided.\n        \"\"\"\n        # Define base variables of the tree\n        if user_id is None:\n            self.user_id = str(uuid.uuid4())\n        else:\n            self.user_id = user_id\n\n        if conversation_id is None:\n            self.conversation_id = str(uuid.uuid4())\n        else:\n            self.conversation_id = conversation_id\n\n        if settings is None:\n            self.settings = environment_settings\n        else:\n            assert isinstance(\n                settings, Settings\n            ), \"settings must be an instance of Settings\"\n            self.settings = settings\n\n        self.use_elysia_collections = use_elysia_collections\n\n        # Initialise some tree variables\n        self.decision_nodes: dict[str, DecisionNode] = {}\n        self.decision_history = [[]]\n        self.tree_index = -1\n        self.suggestions = []\n        self.actions_called = {}\n        self.query_id_to_prompt = {}\n        self.prompt_to_query_id = {}\n        self.retrieved_objects = []\n        self.store_retrieved_objects = False\n        self.conversation_title = None\n        self.low_memory = low_memory\n        self._base_lm = None\n        self._complex_lm = None\n        self._config_modified = False\n        self.root = None\n\n        # Define the inputs to prompts\n        self.tree_data = TreeData(\n            environment=Environment(),\n            collection_data=CollectionData(\n                collection_names=[], logger=self.settings.logger\n            ),\n            atlas=Atlas(\n                style=style,\n                agent_description=agent_description,\n                end_goal=end_goal,\n            ),\n            recursion_limit=5,\n            settings=self.settings,\n        )\n\n        # initialise the timers\n        self.tracker = Tracker(\n            tracker_names=[\"decision_node\"],\n            logger=self.settings.logger,\n        )\n\n        # Set the initialisations\n        self.tools = {}\n        self.set_branch_initialisation(branch_initialisation)\n        self.tree_data.atlas.style = style\n        self.tree_data.atlas.agent_description = agent_description\n        self.tree_data.atlas.end_goal = end_goal\n\n        self.tools[\"forced_text_response\"] = ForcedTextResponse()\n\n        # some variables for storing feedback\n        self.action_information = []\n        self.history = {}\n        self.training_updates = []\n\n        # -- Get the root node and construct the tree\n        self._get_root()\n        self.tree = {}\n        self._construct_tree(self.root, self.tree)\n\n        # initialise the returner (for frontend)\n        self.returner = TreeReturner(\n            user_id=self.user_id,\n            conversation_id=self.conversation_id,\n        )\n\n        # Print the tree if required\n        self.settings.logger.debug(\n            \"Initialised tree with the following decision nodes:\"\n        )\n        for decision_node in self.decision_nodes.values():\n            self.settings.logger.debug(\n                f\"  - [magenta]{decision_node.id}[/magenta]: {list(decision_node.options.keys())}\"\n            )\n\n    @property\n    def base_lm(self) -&gt; dspy.LM:\n        if self.low_memory:\n            return load_base_lm(self.settings)\n        else:\n            if self._base_lm is None:\n                self._base_lm = load_base_lm(self.settings)\n            return self._base_lm\n\n    @property\n    def complex_lm(self) -&gt; dspy.LM:\n        if self.low_memory:\n            return load_complex_lm(self.settings)\n        else:\n            if self._complex_lm is None:\n                self._complex_lm = load_complex_lm(self.settings)\n            return self._complex_lm\n\n    def multi_branch_init(self) -&gt; None:\n        \"\"\"Initialize tree with multi-branch configuration using modular tool loading.\"\"\"\n        load_default_tools_for_mode(self, \"multi_branch\", logger=self.settings.logger)\n\n    def one_branch_init(self) -&gt; None:\n        \"\"\"Initialize tree with one-branch configuration using modular tool loading.\"\"\"\n        load_default_tools_for_mode(self, \"one_branch\", logger=self.settings.logger)\n\n    def empty_init(self) -&gt; None:\n        \"\"\"Initialize tree with empty configuration using modular tool loading.\"\"\"\n        load_default_tools_for_mode(self, \"empty\", logger=self.settings.logger)\n\n    def clear_tree(self) -&gt; None:\n        self.decision_nodes = {}\n        self.root = None\n\n    def set_branch_initialisation(self, initialisation: str | None) -&gt; None:\n        self.clear_tree()\n\n        if (\n            initialisation is None\n            or initialisation == \"\"\n            or initialisation == \"one_branch\"\n            or initialisation == \"default\"\n        ):\n            self.one_branch_init()\n        elif initialisation == \"multi_branch\":\n            self.multi_branch_init()\n        elif initialisation == \"empty\":\n            self.empty_init()\n        else:\n            raise ValueError(f\"Invalid branch initialisation: {initialisation}\")\n\n        self.branch_initialisation = initialisation\n\n        # Auto-load additional tools (e.g., MCP tools) after base initialization\n        self._load_additional_discovered_tools()\n\n    def _load_additional_discovered_tools(self) -&gt; None:\n        \"\"\"\n        Load additional tools that have been discovered (e.g., MCP tools).\n        This runs after base initialization to add any dynamically discovered tools.\n\n        Note: This method is now deprecated as MCP tools are auto-loaded in\n        load_default_tools_for_mode. Kept for backwards compatibility.\n        \"\"\"\n        # MCP tools are now auto-loaded in load_default_tools_for_mode\n        # This method is kept for backwards compatibility and custom tool discovery\n        pass\n\n    def smart_setup(self) -&gt; None:\n        \"\"\"\n        Configures the `settings` object of the tree with the `Settings.smart_setup()` method.\n        \"\"\"\n\n        self.settings = deepcopy(self.settings)\n        self.settings.SETTINGS_ID = str(uuid.uuid4())\n        self._config_modified = True\n        self.settings.smart_setup()\n\n    def configure(self, **kwargs) -&gt; None:\n        \"\"\"\n        Configure the tree with new settings.\n        Wrapper for the settings.configure() method.\n        Will not affect any settings preceding this (e.g. in TreeManager).\n        \"\"\"\n        self.settings = deepcopy(self.settings)\n        self.settings.SETTINGS_ID = str(uuid.uuid4())\n        self._config_modified = True\n        self.tree_data.settings = self.settings\n        self.settings.configure(**kwargs)\n\n    def change_style(self, style: str) -&gt; None:\n        self.tree_data.atlas.style = style\n        self._config_modified = True\n\n    def change_agent_description(self, agent_description: str) -&gt; None:\n        self.tree_data.atlas.agent_description = agent_description\n        self._config_modified = True\n\n    def change_end_goal(self, end_goal: str) -&gt; None:\n        self.tree_data.atlas.end_goal = end_goal\n        self._config_modified = True\n\n    def _get_root(self) -&gt; None:\n        for decision_node in self.decision_nodes.values():\n            if decision_node.root:\n                if self.root is not None and self.root != decision_node.id:\n                    raise ValueError(\"Multiple root decision nodes found\")\n\n                self.root = decision_node.id\n\n        if self.root is None:\n            raise ValueError(\"No root decision node found\")\n\n    def _construct_tree(\n        self, node_id: str | None, tree: dict, branch: bool = True\n    ) -&gt; dict:\n        if node_id is None:\n            raise ValueError(\"Node ID is None\")\n\n        decision_node = self.decision_nodes[node_id]\n\n        # Ensure the order of the keys in each option is the same\n        key_order = [\n            \"name\",\n            \"id\",\n            \"description\",\n            \"instruction\",\n            \"reasoning\",\n            \"branch\",\n            \"options\",\n        ]\n\n        # Set the base node information\n        tree[\"name\"] = node_id.capitalize().replace(\"_\", \" \")\n        tree[\"id\"] = node_id\n        if node_id == self.root:\n            tree[\"description\"] = \"\"\n        tree[\"instruction\"] = remove_whitespace(\n            decision_node.instruction.replace(\"\\n\", \"\")\n        )\n        tree[\"reasoning\"] = \"\"\n        tree[\"branch\"] = branch\n        tree[\"options\"] = {}\n\n        # Order the top-level dictionary\n        tree = {key: tree[key] for key in key_order if key in tree}\n\n        # Initialize all options first with ordered dictionaries\n        for option in decision_node.options:\n            tree[\"options\"][option] = {\n                \"description\": remove_whitespace(\n                    str(decision_node.options[option][\"description\"]).replace(\"\\n\", \"\")\n                )\n            }\n\n        # Then handle the recursive cases\n        for option in decision_node.options:\n            next_node: DecisionNode | None = decision_node.options[option][\"next\"]  # type: ignore\n            if (\n                decision_node.options[option][\"action\"] is not None\n                and next_node is None\n            ):\n                tree[\"options\"][option][\"name\"] = option.capitalize().replace(\"_\", \" \")\n                tree[\"options\"][option][\"id\"] = option\n                tree[\"options\"][option][\"instruction\"] = \"\"\n                tree[\"options\"][option][\"reasoning\"] = \"\"\n                tree[\"options\"][option][\"branch\"] = False\n                tree[\"options\"][option][\"options\"] = {}\n\n            elif next_node is not None:\n                tree[\"options\"][option] = self._construct_tree(\n                    next_node.id,\n                    tree[\"options\"][option],\n                    branch=decision_node.options[option][\"action\"] is None,\n                )\n            else:\n                tree[\"options\"][option][\"name\"] = option.capitalize().replace(\"_\", \" \")\n                tree[\"options\"][option][\"id\"] = option\n                tree[\"options\"][option][\"instruction\"] = \"\"\n                tree[\"options\"][option][\"reasoning\"] = \"\"\n                tree[\"options\"][option][\"branch\"] = True\n                tree[\"options\"][option][\"options\"] = {}\n\n            # Order each option's dictionary\n            tree[\"options\"][option] = {\n                key: tree[\"options\"][option][key]\n                for key in key_order\n                if key in tree[\"options\"][option]\n            }\n\n        return tree\n\n    async def set_collection_names(\n        self,\n        collection_names: list[str],\n        client_manager: ClientManager,\n    ) -&gt; None:\n        self.settings.logger.debug(\n            f\"Using the following collection names: {collection_names}\"\n        )\n\n        collection_names = await self.tree_data.set_collection_names(\n            collection_names, client_manager\n        )\n\n    def _remove_empty_branches(self) -&gt; list[str]:\n        empty_branches = []\n        for branch_id, branch in self.decision_nodes.items():\n            if len(branch.options) == 0:\n                empty_branches.append(branch_id)\n\n        for branch_id in self.decision_nodes:\n            for empty_branch in empty_branches:\n                self.decision_nodes[branch_id].remove_option(empty_branch)\n\n        for empty_branch in empty_branches:\n            if empty_branch != self.root:\n                self.settings.logger.warning(\n                    f\"Removing empty branch: {empty_branch} \"\n                    \"No tools are attached to this branch, so it has been removed. \"\n                    f\"To add a tool to this branch, use .add_tool(tool_name, branch_id={empty_branch})\"\n                )\n                del self.decision_nodes[empty_branch]\n\n        return empty_branches\n\n    def _get_function_inputs(self, tool_name: str, inputs: dict) -&gt; dict:\n        if tool_name in self.tools:\n            # any non-provided inputs are set to the default\n            default_inputs = self.tools[tool_name].get_default_inputs()\n            for default_input_name in default_inputs:\n                if default_input_name not in inputs:\n                    inputs[default_input_name] = default_inputs[default_input_name]\n\n            # if the inputs match the 'schema' of keys: description, type, default, value, then take the value\n            for input_name in inputs:\n                if (\n                    isinstance(inputs[input_name], dict)\n                    and \"value\" in inputs[input_name]\n                ):\n                    inputs[input_name] = inputs[input_name][\"value\"]\n\n            return inputs\n        else:\n            return {}\n\n    async def _check_rules(\n        self, branch_id: str, client_manager: ClientManager\n    ) -&gt; tuple[list[str], dict]:\n        branch = self.decision_nodes[branch_id]\n        nodes_with_rules_met = []\n        rule_tool_inputs = {}\n        for function_name, option in branch.options.items():\n            if function_name not in self.tools:\n                pass\n            elif \"run_if_true\" in dir(self.tools[function_name]):\n                rule_met, rule_tool_inputs = await self.tools[\n                    function_name\n                ].run_if_true(\n                    tree_data=self.tree_data,\n                    client_manager=client_manager,\n                    base_lm=self.base_lm,\n                    complex_lm=self.complex_lm,\n                )\n                if rule_met:\n                    nodes_with_rules_met.append(function_name)\n                    if rule_tool_inputs is None or rule_tool_inputs == {}:\n                        rule_tool_inputs[function_name] = self.tools[\n                            function_name\n                        ].get_default_inputs()\n                    else:\n                        rule_tool_inputs[function_name] = rule_tool_inputs\n\n        return nodes_with_rules_met, rule_tool_inputs\n\n    def set_conversation_id(self, conversation_id: str) -&gt; None:\n        self.conversation_id = conversation_id\n        self.returner.conversation_id = conversation_id\n\n    def set_user_id(self, user_id: str) -&gt; None:\n        self.user_id = user_id\n        self.returner.user_id = user_id\n\n    def soft_reset(self) -&gt; None:\n        # conversation history is not reset\n        # environment is not reset\n        if self.low_memory:\n            self.history = {}\n\n        self.recursion_counter = 0\n        self.tree_data.num_trees_completed = 0\n        self.decision_history = [[]]\n        self.training_updates = []\n        self.tree_data.soft_reset()\n        self.action_information = []\n        self.tree_index += 1\n        self.retrieved_objects = []\n        self.returner.set_tree_index(self.tree_index)\n\n    def save_history(self, query_id: str, time_taken_seconds: float) -&gt; None:\n        \"\"\"\n        What the tree did, results for saving feedback.\n        \"\"\"\n        training_update = deepcopy(\n            [update.to_json() for update in self.training_updates]\n        )\n\n        self.history[query_id] = {\n            \"num_trees_completed\": self.tree_data.num_trees_completed,\n            \"tree_data\": deepcopy(self.tree_data),\n            \"action_information\": deepcopy(self.action_information),\n            \"decision_history\": [\n                item for sublist in deepcopy(self.decision_history) for item in sublist\n            ],\n            \"base_lm_used\": self.settings.BASE_MODEL,\n            \"complex_lm_used\": self.settings.COMPLEX_MODEL,\n            \"time_taken_seconds\": time_taken_seconds,\n            \"training_updates\": training_update,\n            \"initialisation\": f\"{self.branch_initialisation}\",\n        }\n        # can reset training updates now\n        self.training_updates = []\n\n    def set_start_time(self) -&gt; None:\n        self.start_time = time.time()\n\n    def add_tool(\n        self,\n        tool,\n        branch_id: str | None = None,\n        from_tool_ids: list[str] = [],\n        root: bool = False,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Add a Tool to a branch or on top of an existing tool.\n        The tool needs to be an instance of the Tool class.\n\n        Args:\n            tool (Tool): The tool to add\n            branch_id (str): The id of the branch to add the tool to\n                If not specified, the tool will be added to the root branch\n            from_tool_ids (list[str]): The ids of the tools to add the new tool after\n                If not specified, the tool will be added to the base of the branch\n            root (bool): Whether the tool is the root tool\n                If not specified, the tool will be added to the root branch\n            kwargs (any): Additional keyword arguments to pass to the initialisation of the tool\n\n        Example 1:\n            To add a tool, `Query`, to a branch called 'search', you can do this:\n            ```python\n            tree.add_tool(Query, branch_id=\"search\")\n            ```\n            This will add the `Query` tool to the branch 'search'.\n            If the branch 'search' doesn't exist, it will raise an error.\n            To add a branch, use the `.add_branch()` method.\n\n\n        Example 2:\n            Assume your tree has a \"search\" branch with two tools: 'query' and 'aggregate'.\n            You can add a tool, `CheckResult`, after the 'query' tool like this:\n            ```python\n            tree.add_tool(CheckResult, branch_id=\"search\", from_tool_ids=[\"query\"])\n            ```\n            This will add the `CheckResult` tool to the \"search\" branch, after the 'query' tool.\n            So the \"search\" branch will still only have two options: 'query' and 'aggregate'.\n            But after 'query', there will be a new option for the `CheckResult` tool.\n\n        Example 3:\n            You can add a tool, `SendEmail`, after the `CheckResult` (from Example 2) tool like this:\n            ```python\n            tree.add_tool(SendEmail, from_tool_ids=[\"query\", \"check_result\"], root=True)\n            ```\n            It will add an additional option to the root branch, after the 'query' and 'check_result' tools.\n        \"\"\"\n\n        if (\n            inspect.getfullargspec(tool.__init__).varkw is None\n            or inspect.getfullargspec(tool.__call__).varkw is None\n        ):\n            raise TypeError(\"tool __init__ and __call__ must accept **kwargs\")\n\n        if not inspect.isasyncgenfunction(tool.__call__):\n            raise TypeError(\n                \"__call__ must be an async generator function. \"\n                \"I.e. it must yield objects.\"\n            )\n\n        if isinstance(tool, Tool):\n            tool_instance = tool\n        else:\n            tool_instance = tool(\n                logger=self.settings.logger,\n                **kwargs,\n            )\n\n        if not isinstance(tool_instance, Tool):\n            raise TypeError(\"tool must be an instance of the Tool class\")\n\n        if \"__call__\" not in dir(tool_instance):\n            raise TypeError(\"tool must be callable (have a __call__ method)\")\n\n        if \"__init__\" not in dir(tool_instance):\n            raise TypeError(\"tool must have an __init__ method\")\n\n        if hasattr(tool_instance, \"is_tool_available\"):\n            if not inspect.iscoroutinefunction(tool_instance.is_tool_available):\n                raise TypeError(\n                    \"is_tool_available must be an async function that returns a single boolean value\"\n                )\n\n        if hasattr(tool_instance, \"run_if_true\"):\n            if not inspect.iscoroutinefunction(tool_instance.run_if_true):\n                raise TypeError(\n                    \"run_if_true must be an async function that returns a single boolean value\"\n                )\n\n        if root:\n            if branch_id is not None:\n                self.settings.logger.warning(\n                    f\"In .add_tool(), `root` is True, so `branch_id` ('{branch_id}') will be ignored. \"\n                    f\"Tool: '{tool_instance.name}' will be added to the root branch ('{self.root}').\"\n                )\n            branch_id = self.root\n\n        if branch_id is None:\n            branch_id = self.root\n\n        if branch_id not in self.decision_nodes:\n            raise ValueError(\n                f\"Branch '{branch_id}' not found. Use .add_branch() to add a branch before adding a tool. \"\n                f\"Or, set `root=True` to add the tool to the root branch ('{self.root}').\"\n            )\n\n        current_decision_node = self.decision_nodes[branch_id]\n        for from_tool_id in from_tool_ids:\n            if isinstance(current_decision_node, DecisionNode):\n                if from_tool_id not in current_decision_node.options:\n                    raise ValueError(\n                        f\"Tool '{from_tool_id}' not found in branch '{branch_id}'. \"\n                        f\"Available options are: {list(current_decision_node.options.keys())}\"\n                    )\n\n                current_decision_node = current_decision_node.options[from_tool_id][\n                    \"next\"\n                ]\n\n        self.tools[tool_instance.name] = tool_instance\n\n        if from_tool_ids == []:\n            self.decision_nodes[branch_id].add_option(\n                id=tool_instance.name,\n                description=tool_instance.description,\n                inputs=tool_instance.inputs,\n                action=self.tools[tool_instance.name],\n                end=tool_instance.end,\n                status=tool_instance.status,\n            )\n        else:\n\n            new_branch_id = branch_id\n            for from_tool_id in from_tool_ids:\n                new_branch_id += f\".{from_tool_id}\"\n\n            # only create a new decision node if one doesn't exist here\n            if new_branch_id not in self.decision_nodes:\n                decision_node = DecisionNode(\n                    id=new_branch_id,\n                    instruction=f\"Choose one of the actions based on their descriptions and the user prompt.\",\n                    options={},\n                    root=False,\n                    logger=self.settings.logger,\n                    use_elysia_collections=self.use_elysia_collections,\n                )\n                self.decision_nodes[new_branch_id] = decision_node\n\n                prev_branch_id = branch_id\n                for from_tool_id in from_tool_ids[:-1]:\n                    prev_branch_id += f\".{from_tool_id}\"\n\n                self.decision_nodes[prev_branch_id].options[from_tool_ids[-1]][\n                    \"next\"\n                ] = self.decision_nodes[new_branch_id]\n\n            # add the tool to the new decision node\n            self.decision_nodes[new_branch_id].add_option(\n                id=tool_instance.name,\n                description=tool_instance.description,\n                inputs=tool_instance.inputs,\n                action=self.tools[tool_instance.name],\n                end=tool_instance.end,\n                status=tool_instance.status,\n            )\n\n        self.tracker.add_tracker(tool_instance.name)\n\n        # reconstruct tree\n        self._get_root()\n        self.tree = {}\n        self._construct_tree(self.root, self.tree)\n\n    def remove_tool(\n        self,\n        tool_name: str,\n        branch_id: str | None = None,\n        from_tool_ids: list[str] = [],\n        root: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Remove a Tool from a branch.\n\n        Args:\n            tool_name (str): The name of the tool to remove.\n            branch_id (str): The id of the branch to remove the tool from,\n                if not specified, the tool will be removed from the root branch.\n            from_tool_ids (list[str]): The ids of the tools to which precedes the tool to remove.\n            root (bool): Whether the branch the tool is in is the root branch.\n        \"\"\"\n        if root:\n            if branch_id is not None:\n                self.settings.logger.warning(\n                    f\"In .add_tool(), `root` is True, so `branch_id` ('{branch_id}') will be ignored. \"\n                    f\"Tool: '{tool_name}' will be removed from the root branch ('{self.root}').\"\n                )\n            branch_id = self.root\n\n        if branch_id is None:\n            branch_id = self.root\n\n        if branch_id not in self.decision_nodes:\n            raise ValueError(f\"Branch {branch_id} not found.\")\n\n        if (\n            tool_name not in self.decision_nodes[branch_id].options\n            and from_tool_ids == []\n        ):\n            raise ValueError(f\"Tool {tool_name} not found in branch {branch_id}.\")\n\n        current_decision_node = self.decision_nodes[branch_id]\n        for from_tool_id in from_tool_ids:\n            if isinstance(current_decision_node, DecisionNode):\n                if from_tool_id not in current_decision_node.options:\n                    raise ValueError(\n                        f\"Tool '{from_tool_id}' not found in branch '{current_decision_node.id}'. \"\n                        f\"Available options are: {list(current_decision_node.options.keys())}\"\n                    )\n                current_decision_node = current_decision_node.options[from_tool_id][\n                    \"next\"\n                ]\n\n        if (\n            isinstance(current_decision_node, DecisionNode)\n            and tool_name not in current_decision_node.options\n        ):\n            raise ValueError(\n                f\"Tool '{tool_name}' not found in branch '{current_decision_node.id}'. \"\n                f\"Available options are: {list(current_decision_node.options.keys())}\"\n            )\n\n        if from_tool_ids == []:\n            self.decision_nodes[branch_id].remove_option(tool_name)\n        else:\n            tool_branch_id = branch_id\n            for from_tool_id in from_tool_ids:\n                tool_branch_id += f\".{from_tool_id}\"\n            tool_branch_id += f\".{tool_name}\"\n\n            prev_branch_id = branch_id\n            for from_tool_id in from_tool_ids:\n                prev_branch_id += f\".{from_tool_id}\"\n\n            self.decision_nodes[prev_branch_id].remove_option(tool_name)\n            if self.decision_nodes[prev_branch_id].options == {}:\n                del self.decision_nodes[prev_branch_id]\n                stem_branch_id = prev_branch_id[: prev_branch_id.rfind(\".\")]\n                for stem_branch_option in self.decision_nodes[\n                    stem_branch_id\n                ].options.values():\n                    if (\n                        stem_branch_option[\"next\"] is not None\n                        and isinstance(stem_branch_option[\"next\"], DecisionNode)\n                        and stem_branch_option[\"next\"].id == prev_branch_id\n                    ):\n                        stem_branch_option[\"next\"] = None\n\n            if (\n                tool_branch_id in self.decision_nodes\n                and self.decision_nodes[tool_branch_id].options != {}\n            ):\n                self.settings.logger.warning(\n                    f\"The following tools stem from '{tool_branch_id}', \"\n                    f\"and have also been removed: {list(self.decision_nodes[tool_branch_id].options.keys())}\"\n                )\n\n            # find any decision nodes that stem from this\n            nodes_to_remove = []\n            for decision_node_id in self.decision_nodes:\n                if decision_node_id.startswith(tool_branch_id):\n                    if decision_node_id != tool_branch_id:\n                        self.settings.logger.warning(\n                            f\"Decision node '{decision_node_id}' stems from '{tool_branch_id}'. \"\n                            f\"Removing tool '{tool_name}' has also removed '{decision_node_id}'.\"\n                        )\n                    nodes_to_remove.append(decision_node_id)\n\n            for decision_node_id in nodes_to_remove:\n                del self.decision_nodes[decision_node_id]\n\n        del self.tools[tool_name]\n        self.tracker.remove_tracker(tool_name)\n\n        # reconstruct tree\n        self._get_root()\n        self.tree = {}\n        self._construct_tree(self.root, self.tree)\n\n    def add_branch(\n        self,\n        branch_id: str,\n        instruction: str,\n        description: str = \"\",\n        root: bool = False,\n        from_branch_id: str = \"\",\n        from_tool_ids: list[str] = [],\n        status: str = \"\",\n    ) -&gt; None:\n        \"\"\"\n        Add a branch to the tree.\n\n        args:\n            branch_id (str): The id of the branch being added.\n            instruction (str): The general instruction for the branch, what is this branch containing?\n                What kind of tools or actions are being decided on this branch?\n                Only displayed to the decision maker when this branch is chosen.\n            description (str): A description of the branch, if it is to be chosen from a previous branch.\n                How does the model know whether to choose this branch or not?\n            root (bool): Whether this is the root branch, i.e. the beginning of the tree.\n            from_branch_id (str): The id of the branch that this branch is stemming from.\n            from_tool_ids (list[str]): The ids of the tools that precede this branch being added (after the `from_branch_id` branch).\n            status (str): The status message to be displayed when this branch is chosen.\n        \"\"\"\n        if not root and description == \"\":\n            raise ValueError(\"Description is required for non-root branches.\")\n        if not root and from_branch_id == \"\":\n            raise ValueError(\n                \"`from_branch_id` is required for non-root branches. \"\n                \"Set `root=True` to create a root branch or choose where this branch stems from.\"\n            )\n        if root and description != \"\":\n            self.settings.logger.warning(f\"Description is not used for root branches. \")\n            description = \"\"\n\n        if root and from_branch_id != \"\":\n            self.settings.logger.warning(\n                \"`from_branch_id` is not used for root branches. \"\n                \"(As this is the root branch, it does not stem from any other branch.)\"\n                \"If you wish this to be stemming from a previous branch, set `root=False`.\"\n            )\n            from_branch_id = \"\"\n\n        if status == \"\":\n            status = f\"Running {branch_id}...\"\n\n        decision_node = DecisionNode(\n            id=branch_id,\n            instruction=instruction,\n            options={},\n            root=root,\n            logger=self.settings.logger,\n            use_elysia_collections=self.use_elysia_collections,\n        )\n        self.decision_nodes[branch_id] = decision_node\n\n        if not root:\n\n            if from_tool_ids == []:\n                self.decision_nodes[from_branch_id].add_option(\n                    id=branch_id,\n                    description=description,\n                    inputs={},\n                    action=None,\n                    end=False,\n                    status=status,\n                    next=self.decision_nodes[branch_id],\n                )\n\n            else:\n\n                current_decision_node = self.decision_nodes[from_branch_id]\n                for from_tool_id in from_tool_ids:\n                    if isinstance(current_decision_node, DecisionNode):\n                        if from_tool_id not in current_decision_node.options:\n                            raise ValueError(\n                                f\"Tool '{from_tool_id}' not found in branch '{from_branch_id}'. \"\n                                f\"Available options are: {list(current_decision_node.options.keys())}\"\n                            )\n                        current_decision_node = current_decision_node.options[\n                            from_tool_id\n                        ][\"next\"]\n\n                new_branch_id = from_branch_id\n                for from_tool_id in from_tool_ids:\n                    new_branch_id += f\".{from_tool_id}\"\n\n                # only create a new decision node if one doesn't exist here\n                if new_branch_id not in self.decision_nodes:\n                    decision_node = DecisionNode(\n                        id=new_branch_id,\n                        instruction=f\"Choose one of the actions based on their descriptions and the user prompt.\",\n                        options={},\n                        root=False,\n                        logger=self.settings.logger,\n                        use_elysia_collections=self.use_elysia_collections,\n                    )\n                    self.decision_nodes[new_branch_id] = decision_node\n\n                    prev_branch_id = branch_id\n                    for from_tool_id in from_tool_ids[:-1]:\n                        prev_branch_id += f\".{from_tool_id}\"\n\n                    self.decision_nodes[prev_branch_id].options[from_tool_ids[-1]][\n                        \"next\"\n                    ] = self.decision_nodes[new_branch_id]\n\n                # add the tool to the new decision node\n                self.decision_nodes[new_branch_id].add_option(\n                    id=branch_id,\n                    description=description,\n                    inputs={},\n                    action=None,\n                    end=False,\n                    status=status,\n                    next=self.decision_nodes[branch_id],\n                )\n\n        if root and (self.root is not None):\n            # replace root branch with this one\n            self.decision_nodes[self.root] = decision_node\n            self.settings.logger.debug(\n                f\"Replacing root branch '{self.root}' with '{branch_id}'.\"\n            )\n            old_root = self.root\n            self.root = branch_id\n            self.remove_branch(old_root)\n\n        # reconstruct tree\n        self._get_root()\n        self.tree = {}\n        self._construct_tree(self.root, self.tree)\n\n    def remove_branch(self, branch_id: str) -&gt; None:\n        \"\"\"\n        Remove a branch from the tree.\n\n        Args:\n            branch_id (str): The id of the branch to remove\n        \"\"\"\n        # Validate branch exists\n        if branch_id not in self.decision_nodes:\n            self.settings.logger.warning(\n                f\"Branch {branch_id} not found, nothing to remove.\"\n            )\n            return\n\n        # Special handling for root node\n        if (\n            branch_id == self.root\n            and sum(1 for node in self.decision_nodes.values() if node.root) == 1\n        ):\n            self.settings.logger.error(\n                \"Cannot remove root branch if there is only one root branch.\"\n            )\n            raise ValueError(\n                \"Cannot remove the root branch when there is only one root branch. \"\n                \"Create a new root branch via .add_branch(..., root=True) first. \"\n                \"(You could be trying to replace a root branch with the same ID as the one you are trying to remove. \"\n                \"Try a different name for the new root branch.)\"\n            )\n\n        for decision_node_id in self.decision_nodes:\n            self.decision_nodes[decision_node_id].remove_option(branch_id)\n\n        if branch_id in self.decision_nodes:\n            del self.decision_nodes[branch_id]\n\n        # reconstruct tree\n        self._get_root()\n        self.tree = {}\n        self._construct_tree(self.root, self.tree)\n\n    def view(\n        self,\n        indent: int = 0,\n        prefix: str = \"\",\n        max_width: int = 80,\n        tree_dict: dict | None = None,\n    ):\n        \"\"\"\n        Format a tree dictionary into a nice hierarchical text representation.\n\n        Args:\n            tree_dict: The tree dictionary to format\n            indent: Current indentation level\n            prefix: Prefix for the current line (for tree structure visualization)\n            max_width: Maximum width for text wrapping\n\n        Returns:\n            str: Formatted tree string\n        \"\"\"\n        if tree_dict is None:\n            tree_dict = self.tree\n\n        result = []\n\n        name = tree_dict.get(\"name\", \"Unknown\")\n        node_id = tree_dict.get(\"id\", \"\")\n        description = tree_dict.get(\"description\", \"\")\n        is_branch = tree_dict.get(\"branch\", False)\n\n        indent_str = \"  \" * indent\n        node_line = (\n            f\"{indent_str}{prefix}\ud83d\udcc1 {name}\"\n            if is_branch\n            else f\"{indent_str}{prefix}\ud83d\udd27 {name}\"\n        )\n\n        result.append(node_line)\n\n        if description:\n            desc_indent = len(indent_str) + 4  # Extra space for description\n            available_width = max_width - desc_indent\n\n            wrapped_desc = textwrap.fill(\n                description,\n                width=available_width,\n                initial_indent=\"\",\n                subsequent_indent=\"\",\n            )\n\n            for i, line in enumerate(wrapped_desc.split(\"\\n\")):\n                if i == 0:\n                    result.append(f\"{indent_str}    \ud83d\udcac {line}\")\n                else:\n                    result.append(f\"{indent_str}       {line}\")\n\n            result.append(\"\")\n\n        options = tree_dict.get(\"options\", {})\n        if options:\n            option_items = list(options.items())\n            for i, (key, option) in enumerate(option_items):\n                is_last = i == len(option_items) - 1\n                child_prefix = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n                child_result = self.view(\n                    indent + 1, child_prefix, max_width, tree_dict=option\n                )\n                result.append(child_result)\n\n                if indent == 0 and not is_last:\n                    result.append(\"\")\n\n        return \"\\n\".join(result)\n\n    @property\n    def conversation_history(self):\n        return self.tree_data.conversation_history\n\n    @property\n    def environment(self):\n        return self.tree_data.environment\n\n    async def create_conversation_title_async(self) -&gt; str:\n        \"\"\"\n        Create a title for the tree (async) using the base LM.\n        Also assigns the `conversation_title` attribute to the tree.\n\n        Returns:\n            (str): The title for the tree.\n        \"\"\"\n        with ElysiaKeyManager(self.settings):\n            self.conversation_title = await create_conversation_title(\n                self.tree_data.conversation_history, self.base_lm\n            )\n        return self.conversation_title\n\n    def create_conversation_title(self) -&gt; str:\n        \"\"\"\n        Create a title for the tree using the base LM.\n        Also assigns the `conversation_title` attribute to the tree.\n\n        Returns:\n            (str): The title for the tree.\n        \"\"\"\n        return asyncio_run(self.create_conversation_title_async())\n\n    async def get_follow_up_suggestions_async(\n        self, context: str | None = None, num_suggestions: int = 2\n    ) -&gt; list[str]:\n        \"\"\"\n        Get follow-up suggestions for the current user prompt via a base model LLM call.\n\n        E.g., if the user asks \"What was the most recent Github Issue?\",\n            and the results show a message from 'Jane Doe',\n            the follow-up suggestions might be \"What other issues did Jane Doe work on?\"\n\n        Args:\n            context (str | None): A description of the type of follow-up questions to suggest\n            num_suggestions (int): The number of follow-up suggestions to return (length of the list output)\n\n        Returns:\n            (list[str]): A list of follow-up suggestions\n        \"\"\"\n        with ElysiaKeyManager(self.settings):\n            suggestions = await get_follow_up_suggestions(\n                self.tree_data,\n                self.suggestions,\n                self.base_lm,\n                context=context,\n                num_suggestions=num_suggestions,\n            )\n        if suggestions != []:\n            self.settings.logger.debug(f\"Follow-up suggestions: {suggestions}\")\n        else:\n            self.settings.logger.error(\"No follow-up suggestions found.\")\n\n        self.suggestions.extend(suggestions)\n        return suggestions\n\n    def get_follow_up_suggestions(\n        self,\n        context: str | None = None,\n        num_suggestions: int = 2,\n    ) -&gt; list[str]:\n        \"\"\"\n        Get follow-up suggestions for the current user prompt via a base model LLM call (sync wrapper for get_follow_up_suggestions_async).\n\n        E.g., if the user asks \"What was the most recent Github Issue?\",\n            and the results show a message from 'Jane Doe',\n            the follow-up suggestions might be \"What other issues did Jane Doe work on?\"\n\n        Args:\n            context (str | None): A description of the type of follow-up questions to suggest\n            num_suggestions (int): The number of follow-up suggestions to return (length of the list output)\n\n        Returns:\n            (list[str]): A list of follow-up suggestions\n        \"\"\"\n        return asyncio_run(\n            self.get_follow_up_suggestions_async(context, num_suggestions)\n        )\n\n    def _update_conversation_history(self, role: str, message: str) -&gt; None:\n        if message != \"\":\n            # If the first message, create a new message\n            if len(self.tree_data.conversation_history) == 0:\n                self.tree_data.update_list(\n                    \"conversation_history\", {\"role\": role, \"content\": message}\n                )\n            # If the last message is from the same role, append to the content\n            elif self.tree_data.conversation_history[-1][\"role\"] == role:\n                if self.tree_data.conversation_history[-1][\"content\"].endswith(\" \"):\n                    self.tree_data.conversation_history[-1][\"content\"] += message\n                else:\n                    self.tree_data.conversation_history[-1][\"content\"] += \" \" + message\n            # Otherwise, create a new message\n            else:\n                self.tree_data.update_list(\n                    \"conversation_history\", {\"role\": role, \"content\": message}\n                )\n\n    def _update_actions_called(self, result: Result, decision: Decision) -&gt; None:\n        if self.user_prompt not in self.actions_called:\n            self.actions_called[self.user_prompt] = []\n            self.actions_called[self.user_prompt].append(\n                {\n                    \"name\": decision.function_name,\n                    \"inputs\": decision.function_inputs,\n                    \"reasoning\": decision.reasoning,\n                    \"output\": None,\n                }\n            )\n        if not self.low_memory:\n            self.actions_called[self.user_prompt][-1][\"output\"] = result.objects\n        else:\n            self.actions_called[self.user_prompt][-1][\"output\"] = []\n\n    def _add_refs(self, objects: list[dict], tool_name: str, name: str) -&gt; None:\n\n        if (\n            tool_name not in self.tree_data.environment.environment\n            or name not in self.tree_data.environment.environment[tool_name]\n        ):\n            len_objects = 0\n        else:\n            len_objects = len(self.tree_data.environment.environment[tool_name][name])\n\n        for i, obj in enumerate(objects):\n            if \"_REF_ID\" not in obj:\n                _REF_ID = f\"{tool_name}_{name}_{len_objects}_{i}\"\n                obj[\"_REF_ID\"] = _REF_ID\n\n    def _update_environment(self, result: Result, decision: Decision) -&gt; None:\n        \"\"\"\n        Given a yielded result from an action or otherwise, update the environment.\n        I.e. the items within the LLM knowledge base/prompt for future decisions/actions\n        All Result subclasses have their .to_json() method added to the environment.\n        As well, all Result subclasses have their llm_parse() method added to the tasks_completed.\n        \"\"\"\n\n        # add to environment (store of retrieved/called objects)\n        self.tree_data.environment.add(decision.function_name, result)\n\n        # make note of which objects were retrieved _this session_ (for returning)\n        if self.store_retrieved_objects:\n            self.retrieved_objects.append(result.to_json(mapping=False))\n\n        # add to log of actions called\n        self.action_information.append(\n            {\n                \"action_name\": decision.function_name,\n                **{key: value for key, value in result.metadata.items()},\n            }\n        )\n\n        # add to tasks completed (parsed info / train of thought for LLM)\n        self.tree_data.update_tasks_completed(\n            prompt=self.user_prompt,\n            task=decision.function_name,\n            num_trees_completed=self.tree_data.num_trees_completed,\n            reasoning=decision.reasoning,\n            inputs=decision.function_inputs,\n            parsed_info=result.llm_parse(),\n            action=True,\n        )\n\n        # add to log of actions called\n        self._update_actions_called(result, decision)\n\n    def _add_error(self, function_name: str, error: Error) -&gt; None:\n        if function_name not in self.tree_data.errors:\n            self.tree_data.errors[function_name] = []\n\n        self.tree_data.update_tasks_completed(\n            prompt=self.user_prompt,\n            task=function_name,\n            num_trees_completed=self.tree_data.num_trees_completed,\n            error=True,\n        )\n\n        if error.feedback != \"An unknown issue occurred.\":\n            self.tree_data.errors[function_name].append(\n                \"Avoidable error: \"\n                f\"{error.feedback} \"\n                \"(this error is likely to be solved by incorporating the feedback in a future tool call)\"\n            )\n        else:\n            self.tree_data.errors[function_name].append(\n                \"Unknown error: \"\n                f\"{error.error_message} \"\n                \"(this error is likely outside of your capacity to be solved - \"\n                \"judge the error message based on other information and if it seems fixable, call this tool again \"\n                \"if it is repeated, you may need to try something else or inform the user of the issue)\"\n            )\n\n    async def _evaluate_result(\n        self,\n        result: Result | TreeUpdate | Error | TrainingUpdate | Text | Update,\n        decision: Decision,\n    ) -&gt; tuple[dict | None, bool]:\n        error = False\n\n        if isinstance(result, Result):\n            self._add_refs(result.objects, decision.function_name, result.name)\n            self._update_environment(result, decision)\n\n        if isinstance(result, TrainingUpdate):\n            self.training_updates.append(result)\n            return None, error\n\n        if isinstance(result, Error):\n            self._add_error(decision.function_name, result)\n            if self.settings.LOGGING_LEVEL_INT &lt;= 20:\n                print(\n                    Panel.fit(\n                        (\n                            result.error_message\n                            if result.feedback == \"An unknown issue occurred.\"\n                            else result.feedback\n                        ),\n                        title=\"Error\",\n                        border_style=\"red\",\n                        padding=(1, 1),\n                    )\n                )\n            error = True\n\n        if isinstance(result, Text):\n            self._update_conversation_history(\"assistant\", result.text)\n            if self.settings.LOGGING_LEVEL_INT &lt;= 20:\n                print(\n                    Panel.fit(\n                        result.text,\n                        title=\"Assistant response\",\n                        border_style=\"cyan\",\n                        padding=(1, 1),\n                    )\n                )\n\n        return (\n            await self.returner(\n                result,\n                self.prompt_to_query_id[self.user_prompt],\n            ),\n            error,\n        )\n\n    async def _get_available_tools(\n        self, current_decision_node: DecisionNode, client_manager: ClientManager\n    ) -&gt; tuple[list[str], list[tuple[str, str]]]:\n        available_tools = []\n        unavailable_tools = []\n        for tool in current_decision_node.options.keys():\n            if current_decision_node.options[tool][\"action\"] is None:\n                available_tools.append(tool)\n            elif \"is_tool_available\" in dir(self.tools[tool]) and await self.tools[\n                tool\n            ].is_tool_available(\n                tree_data=self.tree_data,\n                base_lm=self.base_lm,\n                complex_lm=self.complex_lm,\n                client_manager=client_manager,\n            ):\n                available_tools.append(tool)\n            else:\n                is_tool_available_doc = (\n                    self.tools[tool].is_tool_available.__doc__.strip()\n                    if self.tools[tool].is_tool_available.__doc__\n                    else \"\"\n                )\n                unavailable_tools.append((tool, is_tool_available_doc))\n        return available_tools, unavailable_tools\n\n    def _get_successive_actions(\n        self, successive_actions: dict, current_options: dict\n    ) -&gt; dict:\n\n        for branch in current_options:\n            successive_actions[branch] = {}\n            if current_options[branch][\"options\"] != {}:\n                successive_actions[branch] = self._get_successive_actions(\n                    successive_actions[branch], current_options[branch][\"options\"]\n                )\n        return successive_actions\n\n    def log_token_usage(self) -&gt; None:\n        if not self.low_memory:\n            avg_input_base = self.tracker.get_average_input_tokens(\"base_lm\")\n            avg_output_base = self.tracker.get_average_output_tokens(\"base_lm\")\n            total_input_base = self.tracker.get_total_input_tokens(\"base_lm\")\n            total_output_base = self.tracker.get_total_output_tokens(\"base_lm\")\n            avg_input_complex = self.tracker.get_average_input_tokens(\"complex_lm\")\n            avg_output_complex = self.tracker.get_average_output_tokens(\"complex_lm\")\n            total_input_complex = self.tracker.get_total_input_tokens(\"complex_lm\")\n            total_output_complex = self.tracker.get_total_output_tokens(\"complex_lm\")\n            total_cost_base = self.tracker.get_total_cost(\"base_lm\")\n            total_cost_complex = self.tracker.get_total_cost(\"complex_lm\")\n            avg_cost_base = self.tracker.get_average_cost(\"base_lm\")\n            avg_cost_complex = self.tracker.get_average_cost(\"complex_lm\")\n            num_calls_base = self.tracker.get_num_calls(\"base_lm\")\n            num_calls_complex = self.tracker.get_num_calls(\"complex_lm\")\n\n            if num_calls_base &gt; 0:\n                self.settings.logger.debug(\n                    f\"Base Model Usage: \\n\"\n                    f\"  - Calls: [magenta]{num_calls_base}[/magenta]\\n\"\n                    f\"  - Input Tokens: [magenta]{total_input_base}[/magenta] (Avg. [magenta]{int(avg_input_base)}[/magenta] per call)\\n\"\n                    f\"  - Output Tokens: [cyan]{total_output_base}[/cyan] (Avg. [cyan]{int(avg_output_base)}[/cyan] per call)\\n\"\n                    f\"  - Total Cost: [yellow]${total_cost_base:.4f}[/yellow] (Avg. [yellow]${avg_cost_base:.4f}[/yellow] per call)\\n\"\n                )\n            else:\n                self.settings.logger.debug(\n                    f\"Base Model Usage: [magenta]0[/magenta] calls\"\n                )\n            if num_calls_complex &gt; 0:\n                self.settings.logger.debug(\n                    f\"Complex Model Usage: \\n\"\n                    f\"  - Calls: [magenta]{num_calls_complex}[/magenta]\\n\"\n                    f\"  - Input Tokens: [magenta]{total_input_complex}[/magenta] (Avg. [magenta]{int(avg_input_complex)}[/magenta] per call)\\n\"\n                    f\"  - Output Tokens: [cyan]{total_output_complex}[/cyan] (Avg. [cyan]{int(avg_output_complex)}[/cyan] per call)\\n\"\n                    f\"  - Total Cost: [yellow]${total_cost_complex:.4f}[/yellow] (Avg. [yellow]${avg_cost_complex:.4f}[/yellow] per call)\\n\"\n                )\n            else:\n                self.settings.logger.debug(\n                    f\"Complex Model Usage: [magenta]0[/magenta] calls\"\n                )\n\n    async def async_run(\n        self,\n        user_prompt: str,\n        collection_names: list[str] = [],\n        client_manager: ClientManager | None = None,\n        training_route: str = \"\",\n        query_id: str | None = None,\n        close_clients_after_completion: bool = True,\n        _first_run: bool = True,\n        **kwargs,\n    ) -&gt; AsyncGenerator[dict | None, None]:\n        \"\"\"\n        Async version of .run() for running Elysia in an async environment.\n        See .run() for full documentation.\n        \"\"\"\n\n        if client_manager is None:\n            client_manager = ClientManager(\n                wcd_url=self.settings.WCD_URL,\n                wcd_api_key=self.settings.WCD_API_KEY,\n                logger=self.settings.logger,\n                client_timeout=None,\n                **self.settings.API_KEYS,\n            )\n\n        # If training route is provided, split it into a list\n        if training_route != \"\":\n            route_list = training_route.split(\"/\")\n        else:\n            route_list = []\n\n        # Some initial steps if this is the first run (no recursion yet)\n        if _first_run:\n\n            self.settings.logger.debug(f\"Style: {self.tree_data.atlas.style}\")\n            self.settings.logger.debug(\n                f\"Agent description: {self.tree_data.atlas.agent_description}\"\n            )\n            self.settings.logger.debug(f\"End goal: {self.tree_data.atlas.end_goal}\")\n\n            if query_id is None:\n                query_id = str(uuid.uuid4())\n\n            self.returner.add_prompt(user_prompt, query_id)\n\n            # Reset the tree (clear temporary data specific to the last user prompt)\n            self.soft_reset()\n\n            check_base_lm_settings(self.settings)\n            check_complex_lm_settings(self.settings)\n\n            # Initialise some objects\n            self.set_start_time()\n            self.query_id_to_prompt[query_id] = user_prompt\n            self.prompt_to_query_id[user_prompt] = query_id\n            self.tree_data.set_property(\"user_prompt\", user_prompt)\n            self._update_conversation_history(\"user\", user_prompt)\n            self.user_prompt = user_prompt\n\n            # check and start clients if not already started\n            if client_manager.is_client:\n                await client_manager.start_clients()\n\n                # Initialise the collections\n                if self.use_elysia_collections:\n                    if collection_names == []:\n                        async with client_manager.connect_to_async_client() as client:\n                            collection_names = await retrieve_all_collection_names(\n                                client\n                            )\n                    await self.set_collection_names(\n                        collection_names,\n                        client_manager,\n                    )\n\n            # If there are any empty branches, remove them (no tools attached to them)\n            self._remove_empty_branches()\n\n            if self.settings.LOGGING_LEVEL_INT &lt;= 20:\n                print(\n                    Panel.fit(\n                        user_prompt,\n                        title=\"User prompt\",\n                        border_style=\"yellow\",\n                        padding=(1, 1),\n                    )\n                )\n\n        # Start the tree at the root node\n        if self.root is not None:\n            current_decision_node: DecisionNode = self.decision_nodes[self.root]\n        else:\n            raise ValueError(\"No root node found!\")\n\n        # Loop through the tree until the end is reached\n        while True:\n\n            available_tools, unavailable_tools = await self._get_available_tools(\n                current_decision_node, client_manager\n            )\n\n            if len(available_tools) == 0:\n                self.settings.logger.error(\"No tools available to use!\")\n                raise ValueError(\n                    \"No tools available to use! \"\n                    \"Check the tool definitions and the `is_tool_available` methods.\"\n                )\n\n            init_options = deepcopy(self.tree[\"options\"])\n            successive_actions = self._get_successive_actions(\n                successive_actions={},\n                current_options=init_options,\n            )\n\n            # Evaluate any tools which have hardcoded rules that have been met\n            nodes_with_rules_met, rule_tool_inputs = await self._check_rules(\n                current_decision_node.id, client_manager\n            )\n\n            if len(nodes_with_rules_met) &gt; 0:\n                for rule in nodes_with_rules_met:\n                    rule_decision = Decision(rule, {}, \"\", False, False)\n                    with ElysiaKeyManager(self.settings):\n                        async for result in self.tools[rule](\n                            tree_data=self.tree_data,\n                            inputs=rule_tool_inputs[rule],\n                            base_lm=self.base_lm,\n                            complex_lm=self.complex_lm,\n                            client_manager=client_manager,\n                        ):\n                            action_result, _ = await self._evaluate_result(\n                                result, rule_decision\n                            )\n                            if action_result is not None:\n                                yield action_result\n\n            # If training route is provided, decide from the training route\n            if len(route_list) &gt; 0:\n                self.settings.logger.debug(f\"Route that will be used: {route_list}\")\n\n                (\n                    self.current_decision,\n                    training_route,\n                ) = current_decision_node.decide_from_route(route_list)\n\n                force_text_response = (\n                    self.current_decision.function_name == \"text_response\"\n                )\n\n            # Under normal circumstances decide from the decision node\n            else:\n                self.tracker.start_tracking(\"decision_node\")\n                self.tree_data.set_current_task(\"elysia_decision_node\")\n                with ElysiaKeyManager(self.settings):\n                    self.current_decision, results = await current_decision_node(\n                        tree_data=self.tree_data,\n                        base_lm=self.base_lm,\n                        complex_lm=self.complex_lm,\n                        available_tools=available_tools,\n                        unavailable_tools=unavailable_tools,\n                        successive_actions=successive_actions,\n                        client_manager=client_manager,\n                    )\n\n                for result in results:\n                    action_result, _ = await self._evaluate_result(\n                        result, self.current_decision\n                    )\n                    if action_result is not None:\n                        yield action_result\n\n                self.tracker.end_tracking(\n                    \"decision_node\",\n                    \"Decision Node\",\n                    self.base_lm if not self.low_memory else None,\n                    self.complex_lm if not self.low_memory else None,\n                )\n\n                # Force text response (later) if model chooses end actions\n                # but no response will be generated from the node, set flag now\n                force_text_response = (\n                    not current_decision_node.options[\n                        self.current_decision.function_name\n                    ][\"end\"]\n                    and self.current_decision.end_actions\n                )\n\n            # Set default values for the function inputs for current call\n            self.current_decision.function_inputs = self._get_function_inputs(\n                self.current_decision.function_name,\n                self.current_decision.function_inputs,\n            )\n\n            # end criteria, task picked is \"text_response\" or model chooses to end conversation\n            completed = (\n                self.current_decision.function_name == \"text_response\"\n                or self.current_decision.end_actions\n                or self.current_decision.impossible\n                or self.tree_data.num_trees_completed &gt; self.tree_data.recursion_limit\n            )\n\n            # assign action function\n            action_fn: Tool | None = current_decision_node.options[\n                self.current_decision.function_name\n            ][\n                \"action\"\n            ]  # type: ignore\n\n            # update the decision history\n            self.decision_history[-1].append(self.current_decision.function_name)\n\n            # print the current node information\n            if self.settings.LOGGING_LEVEL_INT &lt;= 20:\n                print(\n                    Panel.fit(\n                        f\"[bold]Node:[/bold] [magenta]{current_decision_node.id}[/magenta]\\n\"\n                        f\"[bold]Decision:[/bold] [green]{self.current_decision.function_name}[/green]\\n\"\n                        f\"[bold]Reasoning:[/bold] {self.current_decision.reasoning}\\n\",\n                        title=\"Current Decision\",\n                        border_style=\"magenta\",\n                        padding=(1, 1),\n                    )\n                )\n\n            self.tree_data.update_tasks_completed(\n                prompt=self.user_prompt,\n                task=self.current_decision.function_name,\n                num_trees_completed=self.tree_data.num_trees_completed,\n                reasoning=self.current_decision.reasoning,\n                action=action_fn is not None,\n            )\n\n            # evaluate the action if this is not a branch\n            if action_fn is not None:\n                self.tracker.start_tracking(self.current_decision.function_name)\n                self.tree_data.set_current_task(self.current_decision.function_name)\n                successful_action = True\n                with ElysiaKeyManager(self.settings):\n                    async for result in action_fn(\n                        tree_data=self.tree_data,\n                        inputs=self.current_decision.function_inputs,\n                        base_lm=self.base_lm,\n                        complex_lm=self.complex_lm,\n                        client_manager=client_manager,\n                        **kwargs,\n                    ):\n                        action_result, error = await self._evaluate_result(\n                            result, self.current_decision\n                        )\n\n                        if action_result is not None:\n                            yield action_result\n\n                        successful_action = not error and successful_action\n\n                if not successful_action:\n                    completed = (\n                        False\n                        or self.tree_data.num_trees_completed\n                        &gt; self.tree_data.recursion_limit\n                    )\n\n                if successful_action:\n                    self.tree_data.clear_error(self.current_decision.function_name)\n\n                self.tracker.end_tracking(\n                    self.current_decision.function_name,\n                    self.current_decision.function_name,\n                    self.base_lm if not self.low_memory else None,\n                    self.complex_lm if not self.low_memory else None,\n                )\n\n            yield (\n                await self._evaluate_result(\n                    TreeUpdate(\n                        from_node=current_decision_node.id,\n                        to_node=self.current_decision.function_name,\n                        reasoning=(\n                            self.current_decision.reasoning\n                            if self.settings.BASE_USE_REASONING\n                            else \"\"\n                        ),\n                        reset_tree=current_decision_node.options[\n                            self.current_decision.function_name\n                        ][\"next\"]\n                        is None\n                        and (not completed),\n                    ),\n                    self.current_decision,\n                )\n            )[0]\n\n            # check if the current node is the end of the tree\n            if (\n                current_decision_node.options[self.current_decision.function_name][\n                    \"next\"\n                ]\n                is None\n                or completed\n            ):\n                break\n            else:\n                current_decision_node = current_decision_node.options[\n                    self.current_decision.function_name\n                ][\n                    \"next\"\n                ]  # type: ignore\n\n        self.tree_data.num_trees_completed += 1\n\n        # end of all trees\n        if completed:\n\n            # firstly, if we reached the end of a tree at a node that shouldn't be the end, call text response tool here to respond\n            if (\n                not current_decision_node.options[self.current_decision.function_name][\n                    \"end\"\n                ]\n                or force_text_response\n            ):\n                with ElysiaKeyManager(self.settings):\n                    async for result in self.tools[\"forced_text_response\"](\n                        tree_data=self.tree_data,\n                        inputs={},\n                        base_lm=self.base_lm,\n                        complex_lm=self.complex_lm,\n                    ):\n                        action_result, _ = await self._evaluate_result(\n                            result, self.current_decision\n                        )\n                        if action_result is not None:\n                            yield action_result\n\n            self.save_history(\n                query_id=self.prompt_to_query_id[user_prompt],\n                time_taken_seconds=time.time() - self.start_time,\n            )\n\n            yield await self.returner(\n                Completed(), query_id=self.prompt_to_query_id[user_prompt]\n            )\n\n            self.settings.logger.debug(\n                f\"[bold green]Model identified overall goal as completed![/bold green]\"\n            )\n            self.settings.logger.debug(\n                f\"Total time taken for decision tree: {time.time() - self.start_time:.2f} seconds\"\n            )\n            self.settings.logger.debug(\n                f\"Decision Node Avg. Time: {self.tracker.get_average_time('decision_node'):.2f} seconds\"\n            )\n            self.log_token_usage()\n\n            avg_times = []\n            for i, iteration in enumerate(self.decision_history):\n                if iteration != []:\n                    avg_times = [\n                        (\n                            f\"  - {task} ([magenta]Avg. {self.tracker.get_average_time(task):.2f} seconds[/magenta])\\n\"\n                            if task in self.tracker.trackers\n                            else \"\"\n                        )\n                        for task in iteration\n                    ]\n                    self.settings.logger.debug(\n                        f\"Tasks completed (iteration {i+1}):\\n\" + \"\".join(avg_times)\n                    )\n\n            if close_clients_after_completion and client_manager.is_client:\n                await client_manager.close_clients()\n\n        # otherwise, end of the tree for this iteration, and recursively call process() to restart the tree\n        else:\n            self.settings.logger.debug(\n                f\"Model did [bold red]not[/bold red] yet complete overall goal! \"\n            )\n            self.settings.logger.debug(\n                f\"Restarting tree (Recursion: {self.tree_data.num_trees_completed+1}/{self.tree_data.recursion_limit})...\"\n            )\n\n            # recursive call to restart the tree since the goal was not completed\n            self.decision_history.append([])\n            async for result in self.async_run(\n                user_prompt,\n                collection_names,\n                client_manager,\n                training_route=training_route,\n                query_id=query_id,\n                _first_run=False,\n            ):\n                yield result\n\n    def run(\n        self,\n        user_prompt: str,\n        collection_names: list[str] = [],\n        client_manager: ClientManager | None = None,\n        training_route: str = \"\",\n        query_id: str | None = None,\n        close_clients_after_completion: bool = True,\n    ) -&gt; tuple[str, list[dict]]:\n        \"\"\"\n        Run the Elysia decision tree.\n\n        Args:\n            user_prompt (str): The input from the user.\n            collection_names (list[str]): The names of the collections to use.\n                If not provided, Elysia will attempt to retrieve all collection names from the client.\n            client_manager (ClientManager): The client manager to use.\n                If not provided, a new ClientManager will be created.\n            training_route (str): The route to use for training.\n                Separate tools/branches you want to use with a \"/\".\n                e.g. \"query/text_response\" will only use the \"query\" tool and the \"text_response\" tool, and end the tree there.\n            query_id (str): The id of the query.\n                Only necessary if you are hosting Elysia on a server with multiple users.\n                If not provided, a new query id will be generated.\n            close_clients_after_completion (bool): Whether to close the clients after the tree is completed.\n                Leave as True for most use cases, but if you don't want to close the clients for the ClientManager, set to False.\n                For example, if you are managing your own clients (e.g. in an app), you may want to set this to False.\n\n        Returns:\n            (str): The concatenation of all the responses from the tree.\n            (list[dict]): The retrieved objects from the tree.\n        \"\"\"\n\n        self.store_retrieved_objects = True\n\n        async def run_process():\n            async for result in self.async_run(\n                user_prompt,\n                collection_names,\n                client_manager,\n                training_route,\n                query_id,\n                close_clients_after_completion,\n            ):\n                pass\n            return self.retrieved_objects\n\n        async def run_with_live():\n            console = Console()\n\n            with console.status(\"[bold indigo]Thinking...\") as status:\n                async for result in self.async_run(\n                    user_prompt,\n                    collection_names,\n                    client_manager,\n                    training_route,\n                    query_id,\n                    close_clients_after_completion,\n                ):\n                    if (\n                        result is not None\n                        and \"type\" in result\n                        and result[\"type\"] == \"status\"\n                        and isinstance(result[\"payload\"], dict)\n                        and \"text\" in result[\"payload\"]\n                    ):\n                        payload: dict = result[\"payload\"]  # type: ignore\n                        status.update(f\"[bold indigo]{payload['text']}\")\n\n            return self.retrieved_objects\n\n        if self.settings.LOGGING_LEVEL_INT &lt;= 20:\n            yielded_results = asyncio_run(run_with_live())\n        else:\n            yielded_results = asyncio_run(run_process())\n\n        text = self.tree_data.conversation_history[-1][\"content\"]\n\n        return text, yielded_results\n\n    def detailed_memory_usage(self) -&gt; dict:\n        \"\"\"\n        Returns a detailed breakdown of memory usage for all major objects in the Tree class.\n\n        Returns:\n            dict: Dictionary containing memory sizes (in bytes) for each major component\n        \"\"\"\n        memory_usage = {}\n\n        # Core data structures\n        memory_usage[\"tree_data\"] = asizeof.asizeof(self.tree_data)\n        memory_usage[\"tree_data\"] = asizeof.asizeof(self.tree_data)\n        memory_usage[\"tree_data\"] = asizeof.asizeof(self.tree_data)\n        memory_usage[\"collection_data\"] = asizeof.asizeof(\n            self.tree_data.collection_data\n        )\n\n        # Decision nodes and tools\n        memory_usage[\"decision_nodes\"] = {\n            node_id: asizeof.asizeof(node)\n            for node_id, node in self.decision_nodes.items()\n        }\n        memory_usage[\"tools\"] = {\n            tool_name: asizeof.asizeof(tool) for tool_name, tool in self.tools.items()\n        }\n\n        # History and tracking\n        memory_usage[\"decision_history\"] = asizeof.asizeof(self.decision_history)\n        memory_usage[\"history\"] = asizeof.asizeof(self.history)\n        memory_usage[\"training_updates\"] = asizeof.asizeof(self.training_updates)\n        memory_usage[\"action_information\"] = asizeof.asizeof(self.action_information)\n\n        # Mapping dictionaries\n        memory_usage[\"query_mappings\"] = {\n            \"query_id_to_prompt\": asizeof.asizeof(self.query_id_to_prompt),\n            \"prompt_to_query_id\": asizeof.asizeof(self.prompt_to_query_id),\n        }\n\n        # Calculate total\n        memory_usage[\"total\"] = sum(\n            (v if isinstance(v, int) else sum(v.values()) if isinstance(v, dict) else 0)\n            for v in memory_usage.values()\n        )\n\n        return memory_usage\n\n    def export_to_json(self) -&gt; dict:\n        \"\"\"\n        Export the tree to a JSON object, to be used for loading the tree via import_from_json().\n\n        Returns:\n            (dict): The JSON object.\n        \"\"\"\n        try:\n            return {\n                \"user_id\": self.user_id,\n                \"conversation_id\": self.conversation_id,\n                \"conversation_title\": self.conversation_title,\n                \"branch_initialisation\": self.branch_initialisation,\n                \"use_elysia_collections\": self.use_elysia_collections,\n                \"tree_index\": self.tree_index,\n                \"store_retrieved_objects\": self.store_retrieved_objects,\n                \"low_memory\": self.low_memory,\n                \"tree_data\": self.tree_data.to_json(remove_unserialisable=True),\n                \"settings\": self.settings.to_json(),\n                \"tool_names\": list(self.tools.keys()),\n                \"frontend_rebuild\": self.returner.store,\n            }\n        except Exception as e:\n            self.settings.logger.error(f\"Error exporting tree to JSON: {str(e)}\")\n            raise e\n\n    async def export_to_weaviate(\n        self, collection_name: str, client_manager: ClientManager | None = None\n    ) -&gt; None:\n        \"\"\"\n        Export the tree to a Weaviate collection.\n\n        Args:\n            collection_name (str): The name of the collection to export to.\n            client_manager (ClientManager): The client manager to use.\n                If not provided, a new ClientManager will be created from environment variables.\n        \"\"\"\n        if client_manager is None:\n            client_manager = ClientManager()\n            close_after_use = True\n        else:\n            close_after_use = False\n\n        async with client_manager.connect_to_async_client() as client:\n\n            if not await client.collections.exists(collection_name):\n                await client.collections.create(\n                    collection_name,\n                    vectorizer_config=wc.Configure.Vectorizer.none(),\n                    inverted_index_config=wc.Configure.inverted_index(\n                        index_timestamps=True\n                    ),\n                    properties=[\n                        wc.Property(\n                            name=\"user_id\",\n                            data_type=wc.DataType.TEXT,\n                        ),\n                        wc.Property(\n                            name=\"conversation_id\",\n                            data_type=wc.DataType.TEXT,\n                        ),\n                        wc.Property(\n                            name=\"tree\",\n                            data_type=wc.DataType.TEXT,\n                        ),\n                        wc.Property(\n                            name=\"title\",\n                            data_type=wc.DataType.TEXT,\n                        ),\n                    ],\n                )\n\n            collection = client.collections.get(collection_name)\n\n            json_data_str = json.dumps(self.export_to_json())\n\n            uuid = generate_uuid5(self.conversation_id)\n\n            if await collection.data.exists(uuid):\n                await collection.data.update(\n                    uuid=uuid,\n                    properties={\n                        \"user_id\": self.user_id,\n                        \"conversation_id\": self.conversation_id,\n                        \"tree\": json_data_str,\n                        \"title\": self.conversation_title,\n                    },\n                )\n                self.settings.logger.info(\n                    f\"Successfully updated existing tree in collection '{collection_name}' with id '{self.conversation_id}'\"\n                )\n            else:\n                await collection.data.insert(\n                    uuid=uuid,\n                    properties={\n                        \"user_id\": self.user_id,\n                        \"conversation_id\": self.conversation_id,\n                        \"tree\": json_data_str,\n                        \"title\": self.conversation_title,\n                    },\n                )\n                self.settings.logger.info(\n                    f\"Successfully inserted new tree in collection '{collection_name}' with id '{self.conversation_id}'\"\n                )\n\n        if close_after_use:\n            await client_manager.close_clients()\n\n    @classmethod\n    def import_from_json(cls, json_data: dict) -&gt; \"Tree\":\n        \"\"\"\n        Import a tree from a JSON object, outputted by the export_to_json() method.\n\n        Args:\n            json_data (dict): The JSON object to import the tree from.\n\n        Returns:\n            (Tree): The new tree instance loaded from the JSON object.\n        \"\"\"\n        settings = Settings.from_json(json_data[\"settings\"])\n        logger = settings.logger\n        tree = cls(\n            user_id=json_data[\"user_id\"],\n            conversation_id=json_data[\"conversation_id\"],\n            branch_initialisation=json_data[\"branch_initialisation\"],\n            style=json_data[\"tree_data\"][\"atlas\"][\"style\"],\n            agent_description=json_data[\"tree_data\"][\"atlas\"][\"agent_description\"],\n            end_goal=json_data[\"tree_data\"][\"atlas\"][\"end_goal\"],\n            low_memory=json_data[\"low_memory\"],\n            use_elysia_collections=json_data[\"use_elysia_collections\"],\n            settings=settings,\n        )\n\n        tree.returner.store = json_data[\"frontend_rebuild\"]\n        tree.tree_data = TreeData.from_json(json_data[\"tree_data\"])\n        tree.set_branch_initialisation(json_data[\"branch_initialisation\"])\n\n        # check tools\n        for tool_name in json_data[\"tool_names\"]:\n            if tool_name not in tree.tools:\n                logger.warning(\n                    f\"In saved tree, custom tool '{tool_name}' found. \"\n                    \"This will not be loaded in the new tree. \"\n                    \"You will need to add it to the tree manually.\"\n                )\n\n        return tree\n\n    @classmethod\n    async def import_from_weaviate(\n        cls,\n        collection_name: str,\n        conversation_id: str,\n        client_manager: ClientManager | None = None,\n    ) -&gt; \"Tree\":\n        \"\"\"\n        Import a tree from a Weaviate collection.\n\n        Args:\n            collection_name (str): The name of the collection to import from.\n            conversation_id (str): The id of the conversation to import.\n            client_manager (ClientManager): The client manager to use.\n                If not provided, a new ClientManager will be created from environment variables.\n\n        Returns:\n            (Tree): The tree object.\n        \"\"\"\n\n        if client_manager is None:\n            client_manager = ClientManager()\n            close_after_use = True\n        else:\n            close_after_use = False\n\n        async with client_manager.connect_to_async_client() as client:\n\n            if not await client.collections.exists(collection_name):\n                raise ValueError(\n                    f\"Collection '{collection_name}' does not exist in this Weaviate instance.\"\n                )\n\n            collection = client.collections.get(collection_name)\n            uuid = generate_uuid5(conversation_id)\n            # if not await collection.data.exists(uuid):\n            #     raise ValueError(\n            #         f\"No tree found for conversation id '{conversation_id}' in collection '{collection_name}'.\"\n            #     )\n\n            response = await collection.query.fetch_object_by_id(uuid)\n\n        if close_after_use:\n            await client_manager.close_clients()\n\n        if response is None:\n            raise ValueError(\n                f\"No tree found for conversation id '{conversation_id}' in collection '{collection_name}'.\"\n            )\n\n        json_data_str = response.properties[\"tree\"]\n        json_data = json.loads(json_data_str)  # type: ignore\n\n        return cls.import_from_json(json_data)\n\n    def __call__(self, *args, **kwargs) -&gt; tuple[str, list[dict]]:\n        return self.run(*args, **kwargs)\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.__init__","title":"<code>__init__(branch_initialisation='default', style='No style provided.', agent_description='No description provided.', end_goal='No end goal provided.', user_id=None, conversation_id=None, low_memory=False, use_elysia_collections=True, settings=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>branch_initialisation</code> <code>str</code> <p>The initialisation method for the branches, currently supports some pre-defined initialisations: \"multi_branch\", \"one_branch\". Set to \"empty\" to start with no branches and to add them, and the tools, yourself.</p> <code>'default'</code> <code>style</code> <code>str</code> <p>The writing style of the agent. Automatically set for \"multi_branch\" and \"one_branch\" initialisation, but overrided if non-empty.</p> <code>'No style provided.'</code> <code>agent_description</code> <code>str</code> <p>The description of the agent. Automatically set for \"multi_branch\" and \"one_branch\" initialisation, but overrided if non-empty.</p> <code>'No description provided.'</code> <code>end_goal</code> <code>str</code> <p>The end goal of the agent. Automatically set for \"multi_branch\" and \"one_branch\" initialisation, but overrided if non-empty.</p> <code>'No end goal provided.'</code> <code>user_id</code> <code>str</code> <p>The id of the user, e.g. \"123-456\", unneeded outside of user management/hosting Elysia app</p> <code>None</code> <code>conversation_id</code> <code>str</code> <p>The id of the conversation, e.g. \"123-456\", unneeded outside of conversation management/hosting Elysia app</p> <code>None</code> <code>low_memory</code> <code>bool</code> <p>Whether to run the tree in low memory mode. If True, the tree will not load the (dspy) models within the tree. Set to False for normal operation.</p> <code>False</code> <code>use_elysia_collections</code> <code>bool</code> <p>Whether to use weaviate collections as processed by Elysia. If False, the tree will not use the processed collections.</p> <code>True</code> <code>settings</code> <code>Settings</code> <p>The settings for the tree, an object of elysia.Settings. This is automatically set to the environment settings if not provided.</p> <code>None</code> Source code in <code>elysia/tree/tree.py</code> <pre><code>def __init__(\n    self,\n    branch_initialisation: Literal[\n        \"default\", \"one_branch\", \"multi_branch\", \"empty\"\n    ] = \"default\",\n    style: str = \"No style provided.\",\n    agent_description: str = \"No description provided.\",\n    end_goal: str = \"No end goal provided.\",\n    user_id: str | None = None,\n    conversation_id: str | None = None,\n    low_memory: bool = False,\n    use_elysia_collections: bool = True,\n    settings: Settings | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        branch_initialisation (str): The initialisation method for the branches,\n            currently supports some pre-defined initialisations: \"multi_branch\", \"one_branch\".\n            Set to \"empty\" to start with no branches and to add them, and the tools, yourself.\n        style (str): The writing style of the agent. Automatically set for \"multi_branch\" and \"one_branch\" initialisation, but overrided if non-empty.\n        agent_description (str): The description of the agent. Automatically set for \"multi_branch\" and \"one_branch\" initialisation, but overrided if non-empty.\n        end_goal (str): The end goal of the agent. Automatically set for \"multi_branch\" and \"one_branch\" initialisation, but overrided if non-empty.\n        user_id (str): The id of the user, e.g. \"123-456\",\n            unneeded outside of user management/hosting Elysia app\n        conversation_id (str): The id of the conversation, e.g. \"123-456\",\n            unneeded outside of conversation management/hosting Elysia app\n        low_memory (bool): Whether to run the tree in low memory mode.\n            If True, the tree will not load the (dspy) models within the tree.\n            Set to False for normal operation.\n        use_elysia_collections (bool): Whether to use weaviate collections as processed by Elysia.\n            If False, the tree will not use the processed collections.\n        settings (Settings): The settings for the tree, an object of elysia.Settings.\n            This is automatically set to the environment settings if not provided.\n    \"\"\"\n    # Define base variables of the tree\n    if user_id is None:\n        self.user_id = str(uuid.uuid4())\n    else:\n        self.user_id = user_id\n\n    if conversation_id is None:\n        self.conversation_id = str(uuid.uuid4())\n    else:\n        self.conversation_id = conversation_id\n\n    if settings is None:\n        self.settings = environment_settings\n    else:\n        assert isinstance(\n            settings, Settings\n        ), \"settings must be an instance of Settings\"\n        self.settings = settings\n\n    self.use_elysia_collections = use_elysia_collections\n\n    # Initialise some tree variables\n    self.decision_nodes: dict[str, DecisionNode] = {}\n    self.decision_history = [[]]\n    self.tree_index = -1\n    self.suggestions = []\n    self.actions_called = {}\n    self.query_id_to_prompt = {}\n    self.prompt_to_query_id = {}\n    self.retrieved_objects = []\n    self.store_retrieved_objects = False\n    self.conversation_title = None\n    self.low_memory = low_memory\n    self._base_lm = None\n    self._complex_lm = None\n    self._config_modified = False\n    self.root = None\n\n    # Define the inputs to prompts\n    self.tree_data = TreeData(\n        environment=Environment(),\n        collection_data=CollectionData(\n            collection_names=[], logger=self.settings.logger\n        ),\n        atlas=Atlas(\n            style=style,\n            agent_description=agent_description,\n            end_goal=end_goal,\n        ),\n        recursion_limit=5,\n        settings=self.settings,\n    )\n\n    # initialise the timers\n    self.tracker = Tracker(\n        tracker_names=[\"decision_node\"],\n        logger=self.settings.logger,\n    )\n\n    # Set the initialisations\n    self.tools = {}\n    self.set_branch_initialisation(branch_initialisation)\n    self.tree_data.atlas.style = style\n    self.tree_data.atlas.agent_description = agent_description\n    self.tree_data.atlas.end_goal = end_goal\n\n    self.tools[\"forced_text_response\"] = ForcedTextResponse()\n\n    # some variables for storing feedback\n    self.action_information = []\n    self.history = {}\n    self.training_updates = []\n\n    # -- Get the root node and construct the tree\n    self._get_root()\n    self.tree = {}\n    self._construct_tree(self.root, self.tree)\n\n    # initialise the returner (for frontend)\n    self.returner = TreeReturner(\n        user_id=self.user_id,\n        conversation_id=self.conversation_id,\n    )\n\n    # Print the tree if required\n    self.settings.logger.debug(\n        \"Initialised tree with the following decision nodes:\"\n    )\n    for decision_node in self.decision_nodes.values():\n        self.settings.logger.debug(\n            f\"  - [magenta]{decision_node.id}[/magenta]: {list(decision_node.options.keys())}\"\n        )\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.add_branch","title":"<code>add_branch(branch_id, instruction, description='', root=False, from_branch_id='', from_tool_ids=[], status='')</code>","text":"<p>Add a branch to the tree.</p> <p>Parameters:</p> Name Type Description Default <code>branch_id</code> <code>str</code> <p>The id of the branch being added.</p> required <code>instruction</code> <code>str</code> <p>The general instruction for the branch, what is this branch containing? What kind of tools or actions are being decided on this branch? Only displayed to the decision maker when this branch is chosen.</p> required <code>description</code> <code>str</code> <p>A description of the branch, if it is to be chosen from a previous branch. How does the model know whether to choose this branch or not?</p> <code>''</code> <code>root</code> <code>bool</code> <p>Whether this is the root branch, i.e. the beginning of the tree.</p> <code>False</code> <code>from_branch_id</code> <code>str</code> <p>The id of the branch that this branch is stemming from.</p> <code>''</code> <code>from_tool_ids</code> <code>list[str]</code> <p>The ids of the tools that precede this branch being added (after the <code>from_branch_id</code> branch).</p> <code>[]</code> <code>status</code> <code>str</code> <p>The status message to be displayed when this branch is chosen.</p> <code>''</code> Source code in <code>elysia/tree/tree.py</code> <pre><code>def add_branch(\n    self,\n    branch_id: str,\n    instruction: str,\n    description: str = \"\",\n    root: bool = False,\n    from_branch_id: str = \"\",\n    from_tool_ids: list[str] = [],\n    status: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Add a branch to the tree.\n\n    args:\n        branch_id (str): The id of the branch being added.\n        instruction (str): The general instruction for the branch, what is this branch containing?\n            What kind of tools or actions are being decided on this branch?\n            Only displayed to the decision maker when this branch is chosen.\n        description (str): A description of the branch, if it is to be chosen from a previous branch.\n            How does the model know whether to choose this branch or not?\n        root (bool): Whether this is the root branch, i.e. the beginning of the tree.\n        from_branch_id (str): The id of the branch that this branch is stemming from.\n        from_tool_ids (list[str]): The ids of the tools that precede this branch being added (after the `from_branch_id` branch).\n        status (str): The status message to be displayed when this branch is chosen.\n    \"\"\"\n    if not root and description == \"\":\n        raise ValueError(\"Description is required for non-root branches.\")\n    if not root and from_branch_id == \"\":\n        raise ValueError(\n            \"`from_branch_id` is required for non-root branches. \"\n            \"Set `root=True` to create a root branch or choose where this branch stems from.\"\n        )\n    if root and description != \"\":\n        self.settings.logger.warning(f\"Description is not used for root branches. \")\n        description = \"\"\n\n    if root and from_branch_id != \"\":\n        self.settings.logger.warning(\n            \"`from_branch_id` is not used for root branches. \"\n            \"(As this is the root branch, it does not stem from any other branch.)\"\n            \"If you wish this to be stemming from a previous branch, set `root=False`.\"\n        )\n        from_branch_id = \"\"\n\n    if status == \"\":\n        status = f\"Running {branch_id}...\"\n\n    decision_node = DecisionNode(\n        id=branch_id,\n        instruction=instruction,\n        options={},\n        root=root,\n        logger=self.settings.logger,\n        use_elysia_collections=self.use_elysia_collections,\n    )\n    self.decision_nodes[branch_id] = decision_node\n\n    if not root:\n\n        if from_tool_ids == []:\n            self.decision_nodes[from_branch_id].add_option(\n                id=branch_id,\n                description=description,\n                inputs={},\n                action=None,\n                end=False,\n                status=status,\n                next=self.decision_nodes[branch_id],\n            )\n\n        else:\n\n            current_decision_node = self.decision_nodes[from_branch_id]\n            for from_tool_id in from_tool_ids:\n                if isinstance(current_decision_node, DecisionNode):\n                    if from_tool_id not in current_decision_node.options:\n                        raise ValueError(\n                            f\"Tool '{from_tool_id}' not found in branch '{from_branch_id}'. \"\n                            f\"Available options are: {list(current_decision_node.options.keys())}\"\n                        )\n                    current_decision_node = current_decision_node.options[\n                        from_tool_id\n                    ][\"next\"]\n\n            new_branch_id = from_branch_id\n            for from_tool_id in from_tool_ids:\n                new_branch_id += f\".{from_tool_id}\"\n\n            # only create a new decision node if one doesn't exist here\n            if new_branch_id not in self.decision_nodes:\n                decision_node = DecisionNode(\n                    id=new_branch_id,\n                    instruction=f\"Choose one of the actions based on their descriptions and the user prompt.\",\n                    options={},\n                    root=False,\n                    logger=self.settings.logger,\n                    use_elysia_collections=self.use_elysia_collections,\n                )\n                self.decision_nodes[new_branch_id] = decision_node\n\n                prev_branch_id = branch_id\n                for from_tool_id in from_tool_ids[:-1]:\n                    prev_branch_id += f\".{from_tool_id}\"\n\n                self.decision_nodes[prev_branch_id].options[from_tool_ids[-1]][\n                    \"next\"\n                ] = self.decision_nodes[new_branch_id]\n\n            # add the tool to the new decision node\n            self.decision_nodes[new_branch_id].add_option(\n                id=branch_id,\n                description=description,\n                inputs={},\n                action=None,\n                end=False,\n                status=status,\n                next=self.decision_nodes[branch_id],\n            )\n\n    if root and (self.root is not None):\n        # replace root branch with this one\n        self.decision_nodes[self.root] = decision_node\n        self.settings.logger.debug(\n            f\"Replacing root branch '{self.root}' with '{branch_id}'.\"\n        )\n        old_root = self.root\n        self.root = branch_id\n        self.remove_branch(old_root)\n\n    # reconstruct tree\n    self._get_root()\n    self.tree = {}\n    self._construct_tree(self.root, self.tree)\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.add_tool","title":"<code>add_tool(tool, branch_id=None, from_tool_ids=[], root=False, **kwargs)</code>","text":"<p>Add a Tool to a branch or on top of an existing tool. The tool needs to be an instance of the Tool class.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>Tool</code> <p>The tool to add</p> required <code>branch_id</code> <code>str</code> <p>The id of the branch to add the tool to If not specified, the tool will be added to the root branch</p> <code>None</code> <code>from_tool_ids</code> <code>list[str]</code> <p>The ids of the tools to add the new tool after If not specified, the tool will be added to the base of the branch</p> <code>[]</code> <code>root</code> <code>bool</code> <p>Whether the tool is the root tool If not specified, the tool will be added to the root branch</p> <code>False</code> <code>kwargs</code> <code>any</code> <p>Additional keyword arguments to pass to the initialisation of the tool</p> <code>{}</code> Example 1 <p>To add a tool, <code>Query</code>, to a branch called 'search', you can do this: <pre><code>tree.add_tool(Query, branch_id=\"search\")\n</code></pre> This will add the <code>Query</code> tool to the branch 'search'. If the branch 'search' doesn't exist, it will raise an error. To add a branch, use the <code>.add_branch()</code> method.</p> Example 2 <p>Assume your tree has a \"search\" branch with two tools: 'query' and 'aggregate'. You can add a tool, <code>CheckResult</code>, after the 'query' tool like this: <pre><code>tree.add_tool(CheckResult, branch_id=\"search\", from_tool_ids=[\"query\"])\n</code></pre> This will add the <code>CheckResult</code> tool to the \"search\" branch, after the 'query' tool. So the \"search\" branch will still only have two options: 'query' and 'aggregate'. But after 'query', there will be a new option for the <code>CheckResult</code> tool.</p> Example 3 <p>You can add a tool, <code>SendEmail</code>, after the <code>CheckResult</code> (from Example 2) tool like this: <pre><code>tree.add_tool(SendEmail, from_tool_ids=[\"query\", \"check_result\"], root=True)\n</code></pre> It will add an additional option to the root branch, after the 'query' and 'check_result' tools.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def add_tool(\n    self,\n    tool,\n    branch_id: str | None = None,\n    from_tool_ids: list[str] = [],\n    root: bool = False,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Add a Tool to a branch or on top of an existing tool.\n    The tool needs to be an instance of the Tool class.\n\n    Args:\n        tool (Tool): The tool to add\n        branch_id (str): The id of the branch to add the tool to\n            If not specified, the tool will be added to the root branch\n        from_tool_ids (list[str]): The ids of the tools to add the new tool after\n            If not specified, the tool will be added to the base of the branch\n        root (bool): Whether the tool is the root tool\n            If not specified, the tool will be added to the root branch\n        kwargs (any): Additional keyword arguments to pass to the initialisation of the tool\n\n    Example 1:\n        To add a tool, `Query`, to a branch called 'search', you can do this:\n        ```python\n        tree.add_tool(Query, branch_id=\"search\")\n        ```\n        This will add the `Query` tool to the branch 'search'.\n        If the branch 'search' doesn't exist, it will raise an error.\n        To add a branch, use the `.add_branch()` method.\n\n\n    Example 2:\n        Assume your tree has a \"search\" branch with two tools: 'query' and 'aggregate'.\n        You can add a tool, `CheckResult`, after the 'query' tool like this:\n        ```python\n        tree.add_tool(CheckResult, branch_id=\"search\", from_tool_ids=[\"query\"])\n        ```\n        This will add the `CheckResult` tool to the \"search\" branch, after the 'query' tool.\n        So the \"search\" branch will still only have two options: 'query' and 'aggregate'.\n        But after 'query', there will be a new option for the `CheckResult` tool.\n\n    Example 3:\n        You can add a tool, `SendEmail`, after the `CheckResult` (from Example 2) tool like this:\n        ```python\n        tree.add_tool(SendEmail, from_tool_ids=[\"query\", \"check_result\"], root=True)\n        ```\n        It will add an additional option to the root branch, after the 'query' and 'check_result' tools.\n    \"\"\"\n\n    if (\n        inspect.getfullargspec(tool.__init__).varkw is None\n        or inspect.getfullargspec(tool.__call__).varkw is None\n    ):\n        raise TypeError(\"tool __init__ and __call__ must accept **kwargs\")\n\n    if not inspect.isasyncgenfunction(tool.__call__):\n        raise TypeError(\n            \"__call__ must be an async generator function. \"\n            \"I.e. it must yield objects.\"\n        )\n\n    if isinstance(tool, Tool):\n        tool_instance = tool\n    else:\n        tool_instance = tool(\n            logger=self.settings.logger,\n            **kwargs,\n        )\n\n    if not isinstance(tool_instance, Tool):\n        raise TypeError(\"tool must be an instance of the Tool class\")\n\n    if \"__call__\" not in dir(tool_instance):\n        raise TypeError(\"tool must be callable (have a __call__ method)\")\n\n    if \"__init__\" not in dir(tool_instance):\n        raise TypeError(\"tool must have an __init__ method\")\n\n    if hasattr(tool_instance, \"is_tool_available\"):\n        if not inspect.iscoroutinefunction(tool_instance.is_tool_available):\n            raise TypeError(\n                \"is_tool_available must be an async function that returns a single boolean value\"\n            )\n\n    if hasattr(tool_instance, \"run_if_true\"):\n        if not inspect.iscoroutinefunction(tool_instance.run_if_true):\n            raise TypeError(\n                \"run_if_true must be an async function that returns a single boolean value\"\n            )\n\n    if root:\n        if branch_id is not None:\n            self.settings.logger.warning(\n                f\"In .add_tool(), `root` is True, so `branch_id` ('{branch_id}') will be ignored. \"\n                f\"Tool: '{tool_instance.name}' will be added to the root branch ('{self.root}').\"\n            )\n        branch_id = self.root\n\n    if branch_id is None:\n        branch_id = self.root\n\n    if branch_id not in self.decision_nodes:\n        raise ValueError(\n            f\"Branch '{branch_id}' not found. Use .add_branch() to add a branch before adding a tool. \"\n            f\"Or, set `root=True` to add the tool to the root branch ('{self.root}').\"\n        )\n\n    current_decision_node = self.decision_nodes[branch_id]\n    for from_tool_id in from_tool_ids:\n        if isinstance(current_decision_node, DecisionNode):\n            if from_tool_id not in current_decision_node.options:\n                raise ValueError(\n                    f\"Tool '{from_tool_id}' not found in branch '{branch_id}'. \"\n                    f\"Available options are: {list(current_decision_node.options.keys())}\"\n                )\n\n            current_decision_node = current_decision_node.options[from_tool_id][\n                \"next\"\n            ]\n\n    self.tools[tool_instance.name] = tool_instance\n\n    if from_tool_ids == []:\n        self.decision_nodes[branch_id].add_option(\n            id=tool_instance.name,\n            description=tool_instance.description,\n            inputs=tool_instance.inputs,\n            action=self.tools[tool_instance.name],\n            end=tool_instance.end,\n            status=tool_instance.status,\n        )\n    else:\n\n        new_branch_id = branch_id\n        for from_tool_id in from_tool_ids:\n            new_branch_id += f\".{from_tool_id}\"\n\n        # only create a new decision node if one doesn't exist here\n        if new_branch_id not in self.decision_nodes:\n            decision_node = DecisionNode(\n                id=new_branch_id,\n                instruction=f\"Choose one of the actions based on their descriptions and the user prompt.\",\n                options={},\n                root=False,\n                logger=self.settings.logger,\n                use_elysia_collections=self.use_elysia_collections,\n            )\n            self.decision_nodes[new_branch_id] = decision_node\n\n            prev_branch_id = branch_id\n            for from_tool_id in from_tool_ids[:-1]:\n                prev_branch_id += f\".{from_tool_id}\"\n\n            self.decision_nodes[prev_branch_id].options[from_tool_ids[-1]][\n                \"next\"\n            ] = self.decision_nodes[new_branch_id]\n\n        # add the tool to the new decision node\n        self.decision_nodes[new_branch_id].add_option(\n            id=tool_instance.name,\n            description=tool_instance.description,\n            inputs=tool_instance.inputs,\n            action=self.tools[tool_instance.name],\n            end=tool_instance.end,\n            status=tool_instance.status,\n        )\n\n    self.tracker.add_tracker(tool_instance.name)\n\n    # reconstruct tree\n    self._get_root()\n    self.tree = {}\n    self._construct_tree(self.root, self.tree)\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.async_run","title":"<code>async_run(user_prompt, collection_names=[], client_manager=None, training_route='', query_id=None, close_clients_after_completion=True, _first_run=True, **kwargs)</code>  <code>async</code>","text":"<p>Async version of .run() for running Elysia in an async environment. See .run() for full documentation.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>async def async_run(\n    self,\n    user_prompt: str,\n    collection_names: list[str] = [],\n    client_manager: ClientManager | None = None,\n    training_route: str = \"\",\n    query_id: str | None = None,\n    close_clients_after_completion: bool = True,\n    _first_run: bool = True,\n    **kwargs,\n) -&gt; AsyncGenerator[dict | None, None]:\n    \"\"\"\n    Async version of .run() for running Elysia in an async environment.\n    See .run() for full documentation.\n    \"\"\"\n\n    if client_manager is None:\n        client_manager = ClientManager(\n            wcd_url=self.settings.WCD_URL,\n            wcd_api_key=self.settings.WCD_API_KEY,\n            logger=self.settings.logger,\n            client_timeout=None,\n            **self.settings.API_KEYS,\n        )\n\n    # If training route is provided, split it into a list\n    if training_route != \"\":\n        route_list = training_route.split(\"/\")\n    else:\n        route_list = []\n\n    # Some initial steps if this is the first run (no recursion yet)\n    if _first_run:\n\n        self.settings.logger.debug(f\"Style: {self.tree_data.atlas.style}\")\n        self.settings.logger.debug(\n            f\"Agent description: {self.tree_data.atlas.agent_description}\"\n        )\n        self.settings.logger.debug(f\"End goal: {self.tree_data.atlas.end_goal}\")\n\n        if query_id is None:\n            query_id = str(uuid.uuid4())\n\n        self.returner.add_prompt(user_prompt, query_id)\n\n        # Reset the tree (clear temporary data specific to the last user prompt)\n        self.soft_reset()\n\n        check_base_lm_settings(self.settings)\n        check_complex_lm_settings(self.settings)\n\n        # Initialise some objects\n        self.set_start_time()\n        self.query_id_to_prompt[query_id] = user_prompt\n        self.prompt_to_query_id[user_prompt] = query_id\n        self.tree_data.set_property(\"user_prompt\", user_prompt)\n        self._update_conversation_history(\"user\", user_prompt)\n        self.user_prompt = user_prompt\n\n        # check and start clients if not already started\n        if client_manager.is_client:\n            await client_manager.start_clients()\n\n            # Initialise the collections\n            if self.use_elysia_collections:\n                if collection_names == []:\n                    async with client_manager.connect_to_async_client() as client:\n                        collection_names = await retrieve_all_collection_names(\n                            client\n                        )\n                await self.set_collection_names(\n                    collection_names,\n                    client_manager,\n                )\n\n        # If there are any empty branches, remove them (no tools attached to them)\n        self._remove_empty_branches()\n\n        if self.settings.LOGGING_LEVEL_INT &lt;= 20:\n            print(\n                Panel.fit(\n                    user_prompt,\n                    title=\"User prompt\",\n                    border_style=\"yellow\",\n                    padding=(1, 1),\n                )\n            )\n\n    # Start the tree at the root node\n    if self.root is not None:\n        current_decision_node: DecisionNode = self.decision_nodes[self.root]\n    else:\n        raise ValueError(\"No root node found!\")\n\n    # Loop through the tree until the end is reached\n    while True:\n\n        available_tools, unavailable_tools = await self._get_available_tools(\n            current_decision_node, client_manager\n        )\n\n        if len(available_tools) == 0:\n            self.settings.logger.error(\"No tools available to use!\")\n            raise ValueError(\n                \"No tools available to use! \"\n                \"Check the tool definitions and the `is_tool_available` methods.\"\n            )\n\n        init_options = deepcopy(self.tree[\"options\"])\n        successive_actions = self._get_successive_actions(\n            successive_actions={},\n            current_options=init_options,\n        )\n\n        # Evaluate any tools which have hardcoded rules that have been met\n        nodes_with_rules_met, rule_tool_inputs = await self._check_rules(\n            current_decision_node.id, client_manager\n        )\n\n        if len(nodes_with_rules_met) &gt; 0:\n            for rule in nodes_with_rules_met:\n                rule_decision = Decision(rule, {}, \"\", False, False)\n                with ElysiaKeyManager(self.settings):\n                    async for result in self.tools[rule](\n                        tree_data=self.tree_data,\n                        inputs=rule_tool_inputs[rule],\n                        base_lm=self.base_lm,\n                        complex_lm=self.complex_lm,\n                        client_manager=client_manager,\n                    ):\n                        action_result, _ = await self._evaluate_result(\n                            result, rule_decision\n                        )\n                        if action_result is not None:\n                            yield action_result\n\n        # If training route is provided, decide from the training route\n        if len(route_list) &gt; 0:\n            self.settings.logger.debug(f\"Route that will be used: {route_list}\")\n\n            (\n                self.current_decision,\n                training_route,\n            ) = current_decision_node.decide_from_route(route_list)\n\n            force_text_response = (\n                self.current_decision.function_name == \"text_response\"\n            )\n\n        # Under normal circumstances decide from the decision node\n        else:\n            self.tracker.start_tracking(\"decision_node\")\n            self.tree_data.set_current_task(\"elysia_decision_node\")\n            with ElysiaKeyManager(self.settings):\n                self.current_decision, results = await current_decision_node(\n                    tree_data=self.tree_data,\n                    base_lm=self.base_lm,\n                    complex_lm=self.complex_lm,\n                    available_tools=available_tools,\n                    unavailable_tools=unavailable_tools,\n                    successive_actions=successive_actions,\n                    client_manager=client_manager,\n                )\n\n            for result in results:\n                action_result, _ = await self._evaluate_result(\n                    result, self.current_decision\n                )\n                if action_result is not None:\n                    yield action_result\n\n            self.tracker.end_tracking(\n                \"decision_node\",\n                \"Decision Node\",\n                self.base_lm if not self.low_memory else None,\n                self.complex_lm if not self.low_memory else None,\n            )\n\n            # Force text response (later) if model chooses end actions\n            # but no response will be generated from the node, set flag now\n            force_text_response = (\n                not current_decision_node.options[\n                    self.current_decision.function_name\n                ][\"end\"]\n                and self.current_decision.end_actions\n            )\n\n        # Set default values for the function inputs for current call\n        self.current_decision.function_inputs = self._get_function_inputs(\n            self.current_decision.function_name,\n            self.current_decision.function_inputs,\n        )\n\n        # end criteria, task picked is \"text_response\" or model chooses to end conversation\n        completed = (\n            self.current_decision.function_name == \"text_response\"\n            or self.current_decision.end_actions\n            or self.current_decision.impossible\n            or self.tree_data.num_trees_completed &gt; self.tree_data.recursion_limit\n        )\n\n        # assign action function\n        action_fn: Tool | None = current_decision_node.options[\n            self.current_decision.function_name\n        ][\n            \"action\"\n        ]  # type: ignore\n\n        # update the decision history\n        self.decision_history[-1].append(self.current_decision.function_name)\n\n        # print the current node information\n        if self.settings.LOGGING_LEVEL_INT &lt;= 20:\n            print(\n                Panel.fit(\n                    f\"[bold]Node:[/bold] [magenta]{current_decision_node.id}[/magenta]\\n\"\n                    f\"[bold]Decision:[/bold] [green]{self.current_decision.function_name}[/green]\\n\"\n                    f\"[bold]Reasoning:[/bold] {self.current_decision.reasoning}\\n\",\n                    title=\"Current Decision\",\n                    border_style=\"magenta\",\n                    padding=(1, 1),\n                )\n            )\n\n        self.tree_data.update_tasks_completed(\n            prompt=self.user_prompt,\n            task=self.current_decision.function_name,\n            num_trees_completed=self.tree_data.num_trees_completed,\n            reasoning=self.current_decision.reasoning,\n            action=action_fn is not None,\n        )\n\n        # evaluate the action if this is not a branch\n        if action_fn is not None:\n            self.tracker.start_tracking(self.current_decision.function_name)\n            self.tree_data.set_current_task(self.current_decision.function_name)\n            successful_action = True\n            with ElysiaKeyManager(self.settings):\n                async for result in action_fn(\n                    tree_data=self.tree_data,\n                    inputs=self.current_decision.function_inputs,\n                    base_lm=self.base_lm,\n                    complex_lm=self.complex_lm,\n                    client_manager=client_manager,\n                    **kwargs,\n                ):\n                    action_result, error = await self._evaluate_result(\n                        result, self.current_decision\n                    )\n\n                    if action_result is not None:\n                        yield action_result\n\n                    successful_action = not error and successful_action\n\n            if not successful_action:\n                completed = (\n                    False\n                    or self.tree_data.num_trees_completed\n                    &gt; self.tree_data.recursion_limit\n                )\n\n            if successful_action:\n                self.tree_data.clear_error(self.current_decision.function_name)\n\n            self.tracker.end_tracking(\n                self.current_decision.function_name,\n                self.current_decision.function_name,\n                self.base_lm if not self.low_memory else None,\n                self.complex_lm if not self.low_memory else None,\n            )\n\n        yield (\n            await self._evaluate_result(\n                TreeUpdate(\n                    from_node=current_decision_node.id,\n                    to_node=self.current_decision.function_name,\n                    reasoning=(\n                        self.current_decision.reasoning\n                        if self.settings.BASE_USE_REASONING\n                        else \"\"\n                    ),\n                    reset_tree=current_decision_node.options[\n                        self.current_decision.function_name\n                    ][\"next\"]\n                    is None\n                    and (not completed),\n                ),\n                self.current_decision,\n            )\n        )[0]\n\n        # check if the current node is the end of the tree\n        if (\n            current_decision_node.options[self.current_decision.function_name][\n                \"next\"\n            ]\n            is None\n            or completed\n        ):\n            break\n        else:\n            current_decision_node = current_decision_node.options[\n                self.current_decision.function_name\n            ][\n                \"next\"\n            ]  # type: ignore\n\n    self.tree_data.num_trees_completed += 1\n\n    # end of all trees\n    if completed:\n\n        # firstly, if we reached the end of a tree at a node that shouldn't be the end, call text response tool here to respond\n        if (\n            not current_decision_node.options[self.current_decision.function_name][\n                \"end\"\n            ]\n            or force_text_response\n        ):\n            with ElysiaKeyManager(self.settings):\n                async for result in self.tools[\"forced_text_response\"](\n                    tree_data=self.tree_data,\n                    inputs={},\n                    base_lm=self.base_lm,\n                    complex_lm=self.complex_lm,\n                ):\n                    action_result, _ = await self._evaluate_result(\n                        result, self.current_decision\n                    )\n                    if action_result is not None:\n                        yield action_result\n\n        self.save_history(\n            query_id=self.prompt_to_query_id[user_prompt],\n            time_taken_seconds=time.time() - self.start_time,\n        )\n\n        yield await self.returner(\n            Completed(), query_id=self.prompt_to_query_id[user_prompt]\n        )\n\n        self.settings.logger.debug(\n            f\"[bold green]Model identified overall goal as completed![/bold green]\"\n        )\n        self.settings.logger.debug(\n            f\"Total time taken for decision tree: {time.time() - self.start_time:.2f} seconds\"\n        )\n        self.settings.logger.debug(\n            f\"Decision Node Avg. Time: {self.tracker.get_average_time('decision_node'):.2f} seconds\"\n        )\n        self.log_token_usage()\n\n        avg_times = []\n        for i, iteration in enumerate(self.decision_history):\n            if iteration != []:\n                avg_times = [\n                    (\n                        f\"  - {task} ([magenta]Avg. {self.tracker.get_average_time(task):.2f} seconds[/magenta])\\n\"\n                        if task in self.tracker.trackers\n                        else \"\"\n                    )\n                    for task in iteration\n                ]\n                self.settings.logger.debug(\n                    f\"Tasks completed (iteration {i+1}):\\n\" + \"\".join(avg_times)\n                )\n\n        if close_clients_after_completion and client_manager.is_client:\n            await client_manager.close_clients()\n\n    # otherwise, end of the tree for this iteration, and recursively call process() to restart the tree\n    else:\n        self.settings.logger.debug(\n            f\"Model did [bold red]not[/bold red] yet complete overall goal! \"\n        )\n        self.settings.logger.debug(\n            f\"Restarting tree (Recursion: {self.tree_data.num_trees_completed+1}/{self.tree_data.recursion_limit})...\"\n        )\n\n        # recursive call to restart the tree since the goal was not completed\n        self.decision_history.append([])\n        async for result in self.async_run(\n            user_prompt,\n            collection_names,\n            client_manager,\n            training_route=training_route,\n            query_id=query_id,\n            _first_run=False,\n        ):\n            yield result\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.configure","title":"<code>configure(**kwargs)</code>","text":"<p>Configure the tree with new settings. Wrapper for the settings.configure() method. Will not affect any settings preceding this (e.g. in TreeManager).</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def configure(self, **kwargs) -&gt; None:\n    \"\"\"\n    Configure the tree with new settings.\n    Wrapper for the settings.configure() method.\n    Will not affect any settings preceding this (e.g. in TreeManager).\n    \"\"\"\n    self.settings = deepcopy(self.settings)\n    self.settings.SETTINGS_ID = str(uuid.uuid4())\n    self._config_modified = True\n    self.tree_data.settings = self.settings\n    self.settings.configure(**kwargs)\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.create_conversation_title","title":"<code>create_conversation_title()</code>","text":"<p>Create a title for the tree using the base LM. Also assigns the <code>conversation_title</code> attribute to the tree.</p> <p>Returns:</p> Type Description <code>str</code> <p>The title for the tree.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def create_conversation_title(self) -&gt; str:\n    \"\"\"\n    Create a title for the tree using the base LM.\n    Also assigns the `conversation_title` attribute to the tree.\n\n    Returns:\n        (str): The title for the tree.\n    \"\"\"\n    return asyncio_run(self.create_conversation_title_async())\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.create_conversation_title_async","title":"<code>create_conversation_title_async()</code>  <code>async</code>","text":"<p>Create a title for the tree (async) using the base LM. Also assigns the <code>conversation_title</code> attribute to the tree.</p> <p>Returns:</p> Type Description <code>str</code> <p>The title for the tree.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>async def create_conversation_title_async(self) -&gt; str:\n    \"\"\"\n    Create a title for the tree (async) using the base LM.\n    Also assigns the `conversation_title` attribute to the tree.\n\n    Returns:\n        (str): The title for the tree.\n    \"\"\"\n    with ElysiaKeyManager(self.settings):\n        self.conversation_title = await create_conversation_title(\n            self.tree_data.conversation_history, self.base_lm\n        )\n    return self.conversation_title\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.detailed_memory_usage","title":"<code>detailed_memory_usage()</code>","text":"<p>Returns a detailed breakdown of memory usage for all major objects in the Tree class.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing memory sizes (in bytes) for each major component</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def detailed_memory_usage(self) -&gt; dict:\n    \"\"\"\n    Returns a detailed breakdown of memory usage for all major objects in the Tree class.\n\n    Returns:\n        dict: Dictionary containing memory sizes (in bytes) for each major component\n    \"\"\"\n    memory_usage = {}\n\n    # Core data structures\n    memory_usage[\"tree_data\"] = asizeof.asizeof(self.tree_data)\n    memory_usage[\"tree_data\"] = asizeof.asizeof(self.tree_data)\n    memory_usage[\"tree_data\"] = asizeof.asizeof(self.tree_data)\n    memory_usage[\"collection_data\"] = asizeof.asizeof(\n        self.tree_data.collection_data\n    )\n\n    # Decision nodes and tools\n    memory_usage[\"decision_nodes\"] = {\n        node_id: asizeof.asizeof(node)\n        for node_id, node in self.decision_nodes.items()\n    }\n    memory_usage[\"tools\"] = {\n        tool_name: asizeof.asizeof(tool) for tool_name, tool in self.tools.items()\n    }\n\n    # History and tracking\n    memory_usage[\"decision_history\"] = asizeof.asizeof(self.decision_history)\n    memory_usage[\"history\"] = asizeof.asizeof(self.history)\n    memory_usage[\"training_updates\"] = asizeof.asizeof(self.training_updates)\n    memory_usage[\"action_information\"] = asizeof.asizeof(self.action_information)\n\n    # Mapping dictionaries\n    memory_usage[\"query_mappings\"] = {\n        \"query_id_to_prompt\": asizeof.asizeof(self.query_id_to_prompt),\n        \"prompt_to_query_id\": asizeof.asizeof(self.prompt_to_query_id),\n    }\n\n    # Calculate total\n    memory_usage[\"total\"] = sum(\n        (v if isinstance(v, int) else sum(v.values()) if isinstance(v, dict) else 0)\n        for v in memory_usage.values()\n    )\n\n    return memory_usage\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.empty_init","title":"<code>empty_init()</code>","text":"<p>Initialize tree with empty configuration using modular tool loading.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def empty_init(self) -&gt; None:\n    \"\"\"Initialize tree with empty configuration using modular tool loading.\"\"\"\n    load_default_tools_for_mode(self, \"empty\", logger=self.settings.logger)\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.export_to_json","title":"<code>export_to_json()</code>","text":"<p>Export the tree to a JSON object, to be used for loading the tree via import_from_json().</p> <p>Returns:</p> Type Description <code>dict</code> <p>The JSON object.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def export_to_json(self) -&gt; dict:\n    \"\"\"\n    Export the tree to a JSON object, to be used for loading the tree via import_from_json().\n\n    Returns:\n        (dict): The JSON object.\n    \"\"\"\n    try:\n        return {\n            \"user_id\": self.user_id,\n            \"conversation_id\": self.conversation_id,\n            \"conversation_title\": self.conversation_title,\n            \"branch_initialisation\": self.branch_initialisation,\n            \"use_elysia_collections\": self.use_elysia_collections,\n            \"tree_index\": self.tree_index,\n            \"store_retrieved_objects\": self.store_retrieved_objects,\n            \"low_memory\": self.low_memory,\n            \"tree_data\": self.tree_data.to_json(remove_unserialisable=True),\n            \"settings\": self.settings.to_json(),\n            \"tool_names\": list(self.tools.keys()),\n            \"frontend_rebuild\": self.returner.store,\n        }\n    except Exception as e:\n        self.settings.logger.error(f\"Error exporting tree to JSON: {str(e)}\")\n        raise e\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.export_to_weaviate","title":"<code>export_to_weaviate(collection_name, client_manager=None)</code>  <code>async</code>","text":"<p>Export the tree to a Weaviate collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to export to.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use. If not provided, a new ClientManager will be created from environment variables.</p> <code>None</code> Source code in <code>elysia/tree/tree.py</code> <pre><code>async def export_to_weaviate(\n    self, collection_name: str, client_manager: ClientManager | None = None\n) -&gt; None:\n    \"\"\"\n    Export the tree to a Weaviate collection.\n\n    Args:\n        collection_name (str): The name of the collection to export to.\n        client_manager (ClientManager): The client manager to use.\n            If not provided, a new ClientManager will be created from environment variables.\n    \"\"\"\n    if client_manager is None:\n        client_manager = ClientManager()\n        close_after_use = True\n    else:\n        close_after_use = False\n\n    async with client_manager.connect_to_async_client() as client:\n\n        if not await client.collections.exists(collection_name):\n            await client.collections.create(\n                collection_name,\n                vectorizer_config=wc.Configure.Vectorizer.none(),\n                inverted_index_config=wc.Configure.inverted_index(\n                    index_timestamps=True\n                ),\n                properties=[\n                    wc.Property(\n                        name=\"user_id\",\n                        data_type=wc.DataType.TEXT,\n                    ),\n                    wc.Property(\n                        name=\"conversation_id\",\n                        data_type=wc.DataType.TEXT,\n                    ),\n                    wc.Property(\n                        name=\"tree\",\n                        data_type=wc.DataType.TEXT,\n                    ),\n                    wc.Property(\n                        name=\"title\",\n                        data_type=wc.DataType.TEXT,\n                    ),\n                ],\n            )\n\n        collection = client.collections.get(collection_name)\n\n        json_data_str = json.dumps(self.export_to_json())\n\n        uuid = generate_uuid5(self.conversation_id)\n\n        if await collection.data.exists(uuid):\n            await collection.data.update(\n                uuid=uuid,\n                properties={\n                    \"user_id\": self.user_id,\n                    \"conversation_id\": self.conversation_id,\n                    \"tree\": json_data_str,\n                    \"title\": self.conversation_title,\n                },\n            )\n            self.settings.logger.info(\n                f\"Successfully updated existing tree in collection '{collection_name}' with id '{self.conversation_id}'\"\n            )\n        else:\n            await collection.data.insert(\n                uuid=uuid,\n                properties={\n                    \"user_id\": self.user_id,\n                    \"conversation_id\": self.conversation_id,\n                    \"tree\": json_data_str,\n                    \"title\": self.conversation_title,\n                },\n            )\n            self.settings.logger.info(\n                f\"Successfully inserted new tree in collection '{collection_name}' with id '{self.conversation_id}'\"\n            )\n\n    if close_after_use:\n        await client_manager.close_clients()\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.get_follow_up_suggestions","title":"<code>get_follow_up_suggestions(context=None, num_suggestions=2)</code>","text":"<p>Get follow-up suggestions for the current user prompt via a base model LLM call (sync wrapper for get_follow_up_suggestions_async).</p> <p>E.g., if the user asks \"What was the most recent Github Issue?\",     and the results show a message from 'Jane Doe',     the follow-up suggestions might be \"What other issues did Jane Doe work on?\"</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str | None</code> <p>A description of the type of follow-up questions to suggest</p> <code>None</code> <code>num_suggestions</code> <code>int</code> <p>The number of follow-up suggestions to return (length of the list output)</p> <code>2</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of follow-up suggestions</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def get_follow_up_suggestions(\n    self,\n    context: str | None = None,\n    num_suggestions: int = 2,\n) -&gt; list[str]:\n    \"\"\"\n    Get follow-up suggestions for the current user prompt via a base model LLM call (sync wrapper for get_follow_up_suggestions_async).\n\n    E.g., if the user asks \"What was the most recent Github Issue?\",\n        and the results show a message from 'Jane Doe',\n        the follow-up suggestions might be \"What other issues did Jane Doe work on?\"\n\n    Args:\n        context (str | None): A description of the type of follow-up questions to suggest\n        num_suggestions (int): The number of follow-up suggestions to return (length of the list output)\n\n    Returns:\n        (list[str]): A list of follow-up suggestions\n    \"\"\"\n    return asyncio_run(\n        self.get_follow_up_suggestions_async(context, num_suggestions)\n    )\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.get_follow_up_suggestions_async","title":"<code>get_follow_up_suggestions_async(context=None, num_suggestions=2)</code>  <code>async</code>","text":"<p>Get follow-up suggestions for the current user prompt via a base model LLM call.</p> <p>E.g., if the user asks \"What was the most recent Github Issue?\",     and the results show a message from 'Jane Doe',     the follow-up suggestions might be \"What other issues did Jane Doe work on?\"</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>str | None</code> <p>A description of the type of follow-up questions to suggest</p> <code>None</code> <code>num_suggestions</code> <code>int</code> <p>The number of follow-up suggestions to return (length of the list output)</p> <code>2</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of follow-up suggestions</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>async def get_follow_up_suggestions_async(\n    self, context: str | None = None, num_suggestions: int = 2\n) -&gt; list[str]:\n    \"\"\"\n    Get follow-up suggestions for the current user prompt via a base model LLM call.\n\n    E.g., if the user asks \"What was the most recent Github Issue?\",\n        and the results show a message from 'Jane Doe',\n        the follow-up suggestions might be \"What other issues did Jane Doe work on?\"\n\n    Args:\n        context (str | None): A description of the type of follow-up questions to suggest\n        num_suggestions (int): The number of follow-up suggestions to return (length of the list output)\n\n    Returns:\n        (list[str]): A list of follow-up suggestions\n    \"\"\"\n    with ElysiaKeyManager(self.settings):\n        suggestions = await get_follow_up_suggestions(\n            self.tree_data,\n            self.suggestions,\n            self.base_lm,\n            context=context,\n            num_suggestions=num_suggestions,\n        )\n    if suggestions != []:\n        self.settings.logger.debug(f\"Follow-up suggestions: {suggestions}\")\n    else:\n        self.settings.logger.error(\"No follow-up suggestions found.\")\n\n    self.suggestions.extend(suggestions)\n    return suggestions\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.import_from_json","title":"<code>import_from_json(json_data)</code>  <code>classmethod</code>","text":"<p>Import a tree from a JSON object, outputted by the export_to_json() method.</p> <p>Parameters:</p> Name Type Description Default <code>json_data</code> <code>dict</code> <p>The JSON object to import the tree from.</p> required <p>Returns:</p> Type Description <code>Tree</code> <p>The new tree instance loaded from the JSON object.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>@classmethod\ndef import_from_json(cls, json_data: dict) -&gt; \"Tree\":\n    \"\"\"\n    Import a tree from a JSON object, outputted by the export_to_json() method.\n\n    Args:\n        json_data (dict): The JSON object to import the tree from.\n\n    Returns:\n        (Tree): The new tree instance loaded from the JSON object.\n    \"\"\"\n    settings = Settings.from_json(json_data[\"settings\"])\n    logger = settings.logger\n    tree = cls(\n        user_id=json_data[\"user_id\"],\n        conversation_id=json_data[\"conversation_id\"],\n        branch_initialisation=json_data[\"branch_initialisation\"],\n        style=json_data[\"tree_data\"][\"atlas\"][\"style\"],\n        agent_description=json_data[\"tree_data\"][\"atlas\"][\"agent_description\"],\n        end_goal=json_data[\"tree_data\"][\"atlas\"][\"end_goal\"],\n        low_memory=json_data[\"low_memory\"],\n        use_elysia_collections=json_data[\"use_elysia_collections\"],\n        settings=settings,\n    )\n\n    tree.returner.store = json_data[\"frontend_rebuild\"]\n    tree.tree_data = TreeData.from_json(json_data[\"tree_data\"])\n    tree.set_branch_initialisation(json_data[\"branch_initialisation\"])\n\n    # check tools\n    for tool_name in json_data[\"tool_names\"]:\n        if tool_name not in tree.tools:\n            logger.warning(\n                f\"In saved tree, custom tool '{tool_name}' found. \"\n                \"This will not be loaded in the new tree. \"\n                \"You will need to add it to the tree manually.\"\n            )\n\n    return tree\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.import_from_weaviate","title":"<code>import_from_weaviate(collection_name, conversation_id, client_manager=None)</code>  <code>async</code> <code>classmethod</code>","text":"<p>Import a tree from a Weaviate collection.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>The name of the collection to import from.</p> required <code>conversation_id</code> <code>str</code> <p>The id of the conversation to import.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use. If not provided, a new ClientManager will be created from environment variables.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tree</code> <p>The tree object.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>@classmethod\nasync def import_from_weaviate(\n    cls,\n    collection_name: str,\n    conversation_id: str,\n    client_manager: ClientManager | None = None,\n) -&gt; \"Tree\":\n    \"\"\"\n    Import a tree from a Weaviate collection.\n\n    Args:\n        collection_name (str): The name of the collection to import from.\n        conversation_id (str): The id of the conversation to import.\n        client_manager (ClientManager): The client manager to use.\n            If not provided, a new ClientManager will be created from environment variables.\n\n    Returns:\n        (Tree): The tree object.\n    \"\"\"\n\n    if client_manager is None:\n        client_manager = ClientManager()\n        close_after_use = True\n    else:\n        close_after_use = False\n\n    async with client_manager.connect_to_async_client() as client:\n\n        if not await client.collections.exists(collection_name):\n            raise ValueError(\n                f\"Collection '{collection_name}' does not exist in this Weaviate instance.\"\n            )\n\n        collection = client.collections.get(collection_name)\n        uuid = generate_uuid5(conversation_id)\n        # if not await collection.data.exists(uuid):\n        #     raise ValueError(\n        #         f\"No tree found for conversation id '{conversation_id}' in collection '{collection_name}'.\"\n        #     )\n\n        response = await collection.query.fetch_object_by_id(uuid)\n\n    if close_after_use:\n        await client_manager.close_clients()\n\n    if response is None:\n        raise ValueError(\n            f\"No tree found for conversation id '{conversation_id}' in collection '{collection_name}'.\"\n        )\n\n    json_data_str = response.properties[\"tree\"]\n    json_data = json.loads(json_data_str)  # type: ignore\n\n    return cls.import_from_json(json_data)\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.multi_branch_init","title":"<code>multi_branch_init()</code>","text":"<p>Initialize tree with multi-branch configuration using modular tool loading.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def multi_branch_init(self) -&gt; None:\n    \"\"\"Initialize tree with multi-branch configuration using modular tool loading.\"\"\"\n    load_default_tools_for_mode(self, \"multi_branch\", logger=self.settings.logger)\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.one_branch_init","title":"<code>one_branch_init()</code>","text":"<p>Initialize tree with one-branch configuration using modular tool loading.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def one_branch_init(self) -&gt; None:\n    \"\"\"Initialize tree with one-branch configuration using modular tool loading.\"\"\"\n    load_default_tools_for_mode(self, \"one_branch\", logger=self.settings.logger)\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.remove_branch","title":"<code>remove_branch(branch_id)</code>","text":"<p>Remove a branch from the tree.</p> <p>Parameters:</p> Name Type Description Default <code>branch_id</code> <code>str</code> <p>The id of the branch to remove</p> required Source code in <code>elysia/tree/tree.py</code> <pre><code>def remove_branch(self, branch_id: str) -&gt; None:\n    \"\"\"\n    Remove a branch from the tree.\n\n    Args:\n        branch_id (str): The id of the branch to remove\n    \"\"\"\n    # Validate branch exists\n    if branch_id not in self.decision_nodes:\n        self.settings.logger.warning(\n            f\"Branch {branch_id} not found, nothing to remove.\"\n        )\n        return\n\n    # Special handling for root node\n    if (\n        branch_id == self.root\n        and sum(1 for node in self.decision_nodes.values() if node.root) == 1\n    ):\n        self.settings.logger.error(\n            \"Cannot remove root branch if there is only one root branch.\"\n        )\n        raise ValueError(\n            \"Cannot remove the root branch when there is only one root branch. \"\n            \"Create a new root branch via .add_branch(..., root=True) first. \"\n            \"(You could be trying to replace a root branch with the same ID as the one you are trying to remove. \"\n            \"Try a different name for the new root branch.)\"\n        )\n\n    for decision_node_id in self.decision_nodes:\n        self.decision_nodes[decision_node_id].remove_option(branch_id)\n\n    if branch_id in self.decision_nodes:\n        del self.decision_nodes[branch_id]\n\n    # reconstruct tree\n    self._get_root()\n    self.tree = {}\n    self._construct_tree(self.root, self.tree)\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.remove_tool","title":"<code>remove_tool(tool_name, branch_id=None, from_tool_ids=[], root=False)</code>","text":"<p>Remove a Tool from a branch.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>The name of the tool to remove.</p> required <code>branch_id</code> <code>str</code> <p>The id of the branch to remove the tool from, if not specified, the tool will be removed from the root branch.</p> <code>None</code> <code>from_tool_ids</code> <code>list[str]</code> <p>The ids of the tools to which precedes the tool to remove.</p> <code>[]</code> <code>root</code> <code>bool</code> <p>Whether the branch the tool is in is the root branch.</p> <code>False</code> Source code in <code>elysia/tree/tree.py</code> <pre><code>def remove_tool(\n    self,\n    tool_name: str,\n    branch_id: str | None = None,\n    from_tool_ids: list[str] = [],\n    root: bool = False,\n) -&gt; None:\n    \"\"\"\n    Remove a Tool from a branch.\n\n    Args:\n        tool_name (str): The name of the tool to remove.\n        branch_id (str): The id of the branch to remove the tool from,\n            if not specified, the tool will be removed from the root branch.\n        from_tool_ids (list[str]): The ids of the tools to which precedes the tool to remove.\n        root (bool): Whether the branch the tool is in is the root branch.\n    \"\"\"\n    if root:\n        if branch_id is not None:\n            self.settings.logger.warning(\n                f\"In .add_tool(), `root` is True, so `branch_id` ('{branch_id}') will be ignored. \"\n                f\"Tool: '{tool_name}' will be removed from the root branch ('{self.root}').\"\n            )\n        branch_id = self.root\n\n    if branch_id is None:\n        branch_id = self.root\n\n    if branch_id not in self.decision_nodes:\n        raise ValueError(f\"Branch {branch_id} not found.\")\n\n    if (\n        tool_name not in self.decision_nodes[branch_id].options\n        and from_tool_ids == []\n    ):\n        raise ValueError(f\"Tool {tool_name} not found in branch {branch_id}.\")\n\n    current_decision_node = self.decision_nodes[branch_id]\n    for from_tool_id in from_tool_ids:\n        if isinstance(current_decision_node, DecisionNode):\n            if from_tool_id not in current_decision_node.options:\n                raise ValueError(\n                    f\"Tool '{from_tool_id}' not found in branch '{current_decision_node.id}'. \"\n                    f\"Available options are: {list(current_decision_node.options.keys())}\"\n                )\n            current_decision_node = current_decision_node.options[from_tool_id][\n                \"next\"\n            ]\n\n    if (\n        isinstance(current_decision_node, DecisionNode)\n        and tool_name not in current_decision_node.options\n    ):\n        raise ValueError(\n            f\"Tool '{tool_name}' not found in branch '{current_decision_node.id}'. \"\n            f\"Available options are: {list(current_decision_node.options.keys())}\"\n        )\n\n    if from_tool_ids == []:\n        self.decision_nodes[branch_id].remove_option(tool_name)\n    else:\n        tool_branch_id = branch_id\n        for from_tool_id in from_tool_ids:\n            tool_branch_id += f\".{from_tool_id}\"\n        tool_branch_id += f\".{tool_name}\"\n\n        prev_branch_id = branch_id\n        for from_tool_id in from_tool_ids:\n            prev_branch_id += f\".{from_tool_id}\"\n\n        self.decision_nodes[prev_branch_id].remove_option(tool_name)\n        if self.decision_nodes[prev_branch_id].options == {}:\n            del self.decision_nodes[prev_branch_id]\n            stem_branch_id = prev_branch_id[: prev_branch_id.rfind(\".\")]\n            for stem_branch_option in self.decision_nodes[\n                stem_branch_id\n            ].options.values():\n                if (\n                    stem_branch_option[\"next\"] is not None\n                    and isinstance(stem_branch_option[\"next\"], DecisionNode)\n                    and stem_branch_option[\"next\"].id == prev_branch_id\n                ):\n                    stem_branch_option[\"next\"] = None\n\n        if (\n            tool_branch_id in self.decision_nodes\n            and self.decision_nodes[tool_branch_id].options != {}\n        ):\n            self.settings.logger.warning(\n                f\"The following tools stem from '{tool_branch_id}', \"\n                f\"and have also been removed: {list(self.decision_nodes[tool_branch_id].options.keys())}\"\n            )\n\n        # find any decision nodes that stem from this\n        nodes_to_remove = []\n        for decision_node_id in self.decision_nodes:\n            if decision_node_id.startswith(tool_branch_id):\n                if decision_node_id != tool_branch_id:\n                    self.settings.logger.warning(\n                        f\"Decision node '{decision_node_id}' stems from '{tool_branch_id}'. \"\n                        f\"Removing tool '{tool_name}' has also removed '{decision_node_id}'.\"\n                    )\n                nodes_to_remove.append(decision_node_id)\n\n        for decision_node_id in nodes_to_remove:\n            del self.decision_nodes[decision_node_id]\n\n    del self.tools[tool_name]\n    self.tracker.remove_tracker(tool_name)\n\n    # reconstruct tree\n    self._get_root()\n    self.tree = {}\n    self._construct_tree(self.root, self.tree)\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.run","title":"<code>run(user_prompt, collection_names=[], client_manager=None, training_route='', query_id=None, close_clients_after_completion=True)</code>","text":"<p>Run the Elysia decision tree.</p> <p>Parameters:</p> Name Type Description Default <code>user_prompt</code> <code>str</code> <p>The input from the user.</p> required <code>collection_names</code> <code>list[str]</code> <p>The names of the collections to use. If not provided, Elysia will attempt to retrieve all collection names from the client.</p> <code>[]</code> <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use. If not provided, a new ClientManager will be created.</p> <code>None</code> <code>training_route</code> <code>str</code> <p>The route to use for training. Separate tools/branches you want to use with a \"/\". e.g. \"query/text_response\" will only use the \"query\" tool and the \"text_response\" tool, and end the tree there.</p> <code>''</code> <code>query_id</code> <code>str</code> <p>The id of the query. Only necessary if you are hosting Elysia on a server with multiple users. If not provided, a new query id will be generated.</p> <code>None</code> <code>close_clients_after_completion</code> <code>bool</code> <p>Whether to close the clients after the tree is completed. Leave as True for most use cases, but if you don't want to close the clients for the ClientManager, set to False. For example, if you are managing your own clients (e.g. in an app), you may want to set this to False.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The concatenation of all the responses from the tree.</p> <code>list[dict]</code> <p>The retrieved objects from the tree.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def run(\n    self,\n    user_prompt: str,\n    collection_names: list[str] = [],\n    client_manager: ClientManager | None = None,\n    training_route: str = \"\",\n    query_id: str | None = None,\n    close_clients_after_completion: bool = True,\n) -&gt; tuple[str, list[dict]]:\n    \"\"\"\n    Run the Elysia decision tree.\n\n    Args:\n        user_prompt (str): The input from the user.\n        collection_names (list[str]): The names of the collections to use.\n            If not provided, Elysia will attempt to retrieve all collection names from the client.\n        client_manager (ClientManager): The client manager to use.\n            If not provided, a new ClientManager will be created.\n        training_route (str): The route to use for training.\n            Separate tools/branches you want to use with a \"/\".\n            e.g. \"query/text_response\" will only use the \"query\" tool and the \"text_response\" tool, and end the tree there.\n        query_id (str): The id of the query.\n            Only necessary if you are hosting Elysia on a server with multiple users.\n            If not provided, a new query id will be generated.\n        close_clients_after_completion (bool): Whether to close the clients after the tree is completed.\n            Leave as True for most use cases, but if you don't want to close the clients for the ClientManager, set to False.\n            For example, if you are managing your own clients (e.g. in an app), you may want to set this to False.\n\n    Returns:\n        (str): The concatenation of all the responses from the tree.\n        (list[dict]): The retrieved objects from the tree.\n    \"\"\"\n\n    self.store_retrieved_objects = True\n\n    async def run_process():\n        async for result in self.async_run(\n            user_prompt,\n            collection_names,\n            client_manager,\n            training_route,\n            query_id,\n            close_clients_after_completion,\n        ):\n            pass\n        return self.retrieved_objects\n\n    async def run_with_live():\n        console = Console()\n\n        with console.status(\"[bold indigo]Thinking...\") as status:\n            async for result in self.async_run(\n                user_prompt,\n                collection_names,\n                client_manager,\n                training_route,\n                query_id,\n                close_clients_after_completion,\n            ):\n                if (\n                    result is not None\n                    and \"type\" in result\n                    and result[\"type\"] == \"status\"\n                    and isinstance(result[\"payload\"], dict)\n                    and \"text\" in result[\"payload\"]\n                ):\n                    payload: dict = result[\"payload\"]  # type: ignore\n                    status.update(f\"[bold indigo]{payload['text']}\")\n\n        return self.retrieved_objects\n\n    if self.settings.LOGGING_LEVEL_INT &lt;= 20:\n        yielded_results = asyncio_run(run_with_live())\n    else:\n        yielded_results = asyncio_run(run_process())\n\n    text = self.tree_data.conversation_history[-1][\"content\"]\n\n    return text, yielded_results\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.save_history","title":"<code>save_history(query_id, time_taken_seconds)</code>","text":"<p>What the tree did, results for saving feedback.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def save_history(self, query_id: str, time_taken_seconds: float) -&gt; None:\n    \"\"\"\n    What the tree did, results for saving feedback.\n    \"\"\"\n    training_update = deepcopy(\n        [update.to_json() for update in self.training_updates]\n    )\n\n    self.history[query_id] = {\n        \"num_trees_completed\": self.tree_data.num_trees_completed,\n        \"tree_data\": deepcopy(self.tree_data),\n        \"action_information\": deepcopy(self.action_information),\n        \"decision_history\": [\n            item for sublist in deepcopy(self.decision_history) for item in sublist\n        ],\n        \"base_lm_used\": self.settings.BASE_MODEL,\n        \"complex_lm_used\": self.settings.COMPLEX_MODEL,\n        \"time_taken_seconds\": time_taken_seconds,\n        \"training_updates\": training_update,\n        \"initialisation\": f\"{self.branch_initialisation}\",\n    }\n    # can reset training updates now\n    self.training_updates = []\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.smart_setup","title":"<code>smart_setup()</code>","text":"<p>Configures the <code>settings</code> object of the tree with the <code>Settings.smart_setup()</code> method.</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def smart_setup(self) -&gt; None:\n    \"\"\"\n    Configures the `settings` object of the tree with the `Settings.smart_setup()` method.\n    \"\"\"\n\n    self.settings = deepcopy(self.settings)\n    self.settings.SETTINGS_ID = str(uuid.uuid4())\n    self._config_modified = True\n    self.settings.smart_setup()\n</code></pre>"},{"location":"Reference/Tree/#elysia.tree.tree.Tree.view","title":"<code>view(indent=0, prefix='', max_width=80, tree_dict=None)</code>","text":"<p>Format a tree dictionary into a nice hierarchical text representation.</p> <p>Parameters:</p> Name Type Description Default <code>tree_dict</code> <code>dict | None</code> <p>The tree dictionary to format</p> <code>None</code> <code>indent</code> <code>int</code> <p>Current indentation level</p> <code>0</code> <code>prefix</code> <code>str</code> <p>Prefix for the current line (for tree structure visualization)</p> <code>''</code> <code>max_width</code> <code>int</code> <p>Maximum width for text wrapping</p> <code>80</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Formatted tree string</p> Source code in <code>elysia/tree/tree.py</code> <pre><code>def view(\n    self,\n    indent: int = 0,\n    prefix: str = \"\",\n    max_width: int = 80,\n    tree_dict: dict | None = None,\n):\n    \"\"\"\n    Format a tree dictionary into a nice hierarchical text representation.\n\n    Args:\n        tree_dict: The tree dictionary to format\n        indent: Current indentation level\n        prefix: Prefix for the current line (for tree structure visualization)\n        max_width: Maximum width for text wrapping\n\n    Returns:\n        str: Formatted tree string\n    \"\"\"\n    if tree_dict is None:\n        tree_dict = self.tree\n\n    result = []\n\n    name = tree_dict.get(\"name\", \"Unknown\")\n    node_id = tree_dict.get(\"id\", \"\")\n    description = tree_dict.get(\"description\", \"\")\n    is_branch = tree_dict.get(\"branch\", False)\n\n    indent_str = \"  \" * indent\n    node_line = (\n        f\"{indent_str}{prefix}\ud83d\udcc1 {name}\"\n        if is_branch\n        else f\"{indent_str}{prefix}\ud83d\udd27 {name}\"\n    )\n\n    result.append(node_line)\n\n    if description:\n        desc_indent = len(indent_str) + 4  # Extra space for description\n        available_width = max_width - desc_indent\n\n        wrapped_desc = textwrap.fill(\n            description,\n            width=available_width,\n            initial_indent=\"\",\n            subsequent_indent=\"\",\n        )\n\n        for i, line in enumerate(wrapped_desc.split(\"\\n\")):\n            if i == 0:\n                result.append(f\"{indent_str}    \ud83d\udcac {line}\")\n            else:\n                result.append(f\"{indent_str}       {line}\")\n\n        result.append(\"\")\n\n    options = tree_dict.get(\"options\", {})\n    if options:\n        option_items = list(options.items())\n        for i, (key, option) in enumerate(option_items):\n            is_last = i == len(option_items) - 1\n            child_prefix = \"\u2514\u2500\u2500 \" if is_last else \"\u251c\u2500\u2500 \"\n            child_result = self.view(\n                indent + 1, child_prefix, max_width, tree_dict=option\n            )\n            result.append(child_result)\n\n            if indent == 0 and not is_last:\n                result.append(\"\")\n\n    return \"\\n\".join(result)\n</code></pre>"},{"location":"Reference/Util/","title":"Util","text":""},{"location":"Reference/Util/#elysia.util.elysia_chain_of_thought.ElysiaChainOfThought","title":"<code>ElysiaChainOfThought</code>","text":"<p>               Bases: <code>Module</code></p> <p>A custom reasoning DSPy module that reasons step by step in order to predict the output of a task. It will automatically include the most relevant inputs: - The user's prompt - The conversation history - The atlas - Any errors (from calls of the same tool)</p> <p>And you can also include optional inputs (by setting their boolean flags on initialisation to <code>True</code>): - The environment - The collection schemas - The tasks completed</p> <p>You can also specify <code>collection_names</code> to only include certain collections in the collection schemas.</p> <p>It will optionally output (by setting the boolean flags on initialisation to <code>True</code>): - The reasoning (model step by step reasoning) - A message update (if <code>message_update</code> is <code>True</code>), a brief 'update' message to the user. - Whether the task is impossible (boolean)</p> <p>You can use this module by calling the <code>.aforward()</code> method, passing all your new inputs as keyword arguments. You do not need to include keyword arguments for the other inputs, like the <code>environment</code>.</p> <p>Example: <pre><code>my_module = ElysiaChainOfThought(\n    signature=...,\n    tree_data=...,\n    message_update=True,\n    environment=True,\n    collection_schemas=True,\n    tasks_completed=True,\n)\nmy_module.aforward(input1=..., input2=..., lm=...)\n</code></pre></p> Source code in <code>elysia/util/elysia_chain_of_thought.py</code> <pre><code>class ElysiaChainOfThought(Module):\n    \"\"\"\n    A custom reasoning DSPy module that reasons step by step in order to predict the output of a task.\n    It will automatically include the most relevant inputs:\n    - The user's prompt\n    - The conversation history\n    - The atlas\n    - Any errors (from calls of the same tool)\n\n    And you can also include optional inputs (by setting their boolean flags on initialisation to `True`):\n    - The environment\n    - The collection schemas\n    - The tasks completed\n\n    You can also specify `collection_names` to only include certain collections in the collection schemas.\n\n    It will optionally output (by setting the boolean flags on initialisation to `True`):\n    - The reasoning (model step by step reasoning)\n    - A message update (if `message_update` is `True`), a brief 'update' message to the user.\n    - Whether the task is impossible (boolean)\n\n    You can use this module by calling the `.aforward()` method, passing all your *new* inputs as keyword arguments.\n    You do not need to include keyword arguments for the other inputs, like the `environment`.\n\n    Example:\n    ```python\n    my_module = ElysiaChainOfThought(\n        signature=...,\n        tree_data=...,\n        message_update=True,\n        environment=True,\n        collection_schemas=True,\n        tasks_completed=True,\n    )\n    my_module.aforward(input1=..., input2=..., lm=...)\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        signature: Type[Signature],\n        tree_data: TreeData,\n        reasoning: bool = True,\n        impossible: bool = True,\n        message_update: bool = True,\n        environment: bool = False,\n        collection_schemas: bool = False,\n        tasks_completed: bool = False,\n        collection_names: list[str] = [],\n        **config,\n    ):\n        \"\"\"\n        Args:\n            signature (Type[dspy.Signature]): The signature of the module.\n            tree_data (TreeData): Required. The tree data from the Elysia decision tree.\n                Used to input the current state of the tree into the prompt.\n                If you are using this module as part of a tool, the `tree_data` is an input to the tool call.\n            reasoning (bool): Whether to include a reasoning input (chain of thought).\n            impossible (bool): Whether to include a boolean flag indicating whether the task is impossible.\n                This is useful for stopping the tree from continuing and returning to the base of the decision tree.\n                For example, the model judges a query impossible to execute, or the user has not provided enough information.\n            message_update (bool): Whether to include a message update input.\n                If True, the LLM output will include a brief 'update' message to the user.\n                This describes the current action the LLM is performing.\n                Designed to increase interactivity and provide the user with information before the final output.\n            environment (bool): Whether to include an environment input.\n                If True, the module will include the currently stored data from previous tasks and actions into the prompt.\n                This is useful so that the LLM knows what has already been done, and can avoid repeating actions.\n                Or to use information from the environment to perform the new action.\n            collection_schemas (bool): Whether to include a collection schema input.\n                If True, the module will include the preprocessed collection schemas in the prompt input.\n                This is useful so that the LLM knows the structure of the collections, if querying or similar.\n                Use this sparingly, as it will use a large amount of tokens.\n                You can specify `collection_names` to only include certain collections in this schema.\n            tasks_completed (bool): Whether to include a tasks completed input.\n                If True, the module will include the list of tasks completed input.\n                This is a nicely formatted list of the tasks that have been completed, with the reasoning for each task.\n                This is used so that the LLM has a 'stream of consciousness' of what has already been done,\n                as well as to stop it from repeating actions.\n                Other information is included in the `tasks_completed` field that format outputs from previous tasks.\n                This is useful for continuing a decision logic across tasks, or to reinforce key information.\n            collection_names (list[str]): A list of collection names to include in the prompt.\n                If provided, this will modify the collection schema input to only include the collections in this list.\n                This is useful if you only want to include certain collections in the prompt.\n                And to reduce token usage.\n            **config (Any): The DSPy configuration for the module.\n        \"\"\"\n\n        super().__init__()\n\n        signature = ensure_signature(signature)  # type: ignore\n\n        # Create a shallow copy of the tree_data\n        self.tree_data = copy(tree_data)\n\n        # Note which inputs are required\n        self.message_update = message_update\n        self.environment = environment\n        self.collection_schemas = collection_schemas\n        self.tasks_completed = tasks_completed\n        self.collection_names = collection_names\n        self.reasoning = reasoning\n        self.impossible = impossible\n\n        # == Inputs ==\n\n        # -- User Prompt --\n        user_prompt_desc = (\n            \"The user's original question/prompt that needs to be answered. \"\n            \"This, possibly combined with the conversation history, will be used to determine your current action.\"\n        )\n        user_prompt_prefix = \"${user_prompt}\"\n        user_prompt_field: str = dspy.InputField(\n            prefix=user_prompt_prefix, desc=user_prompt_desc\n        )\n\n        # -- Conversation History --\n        conversation_history_desc = (\n            \"Previous messages between user and assistant in chronological order: \"\n            \"[{'role': 'user'|'assistant', 'content': str}] \"\n            \"Use this to maintain conversation context and avoid repetition.\"\n        )\n        conversation_history_prefix = \"${conversation_history}\"\n        conversation_history_field: list[dict] = dspy.InputField(\n            prefix=conversation_history_prefix, desc=conversation_history_desc\n        )\n\n        # -- Atlas --\n        atlas_desc = (\n            \"Your guide to how you should proceed as an agent in this task. \"\n            \"This is pre-defined by the user.\"\n        )\n        atlas_prefix = \"${atlas}\"\n        atlas_field: Atlas = dspy.InputField(prefix=atlas_prefix, desc=atlas_desc)\n\n        # -- Errors --\n        errors_desc = (\n            \"Any errors that have occurred during the previous attempt at this action. \"\n            \"This is a list of dictionaries, containing details of the error. \"\n            \"Make an attempt at providing different output to avoid this error now. \"\n            \"If this error is repeated, or you judge it to be unsolvable, you can set `impossible` to True\"\n        )\n        errors_prefix = \"${previous_errors}\"\n        errors_field: list[dict] = dspy.InputField(\n            prefix=errors_prefix, desc=errors_desc\n        )\n\n        # -- Add to Signature --\n        extended_signature = signature.prepend(\n            name=\"user_prompt\", field=user_prompt_field, type_=str\n        )\n        extended_signature = extended_signature.append(\n            name=\"conversation_history\",\n            field=conversation_history_field,\n            type_=list[dict],\n        )\n        extended_signature = extended_signature.append(\n            name=\"atlas\", field=atlas_field, type_=Atlas\n        )\n        extended_signature = extended_signature.append(\n            name=\"previous_errors\", field=errors_field, type_=list[dict]\n        )\n\n        # == Optional Inputs / Outputs ==\n\n        # -- Environment Input --\n        if environment:\n            environment_desc = (\n                \"Information gathered from completed tasks. \"\n                \"Empty if no data has been retrieved yet. \"\n                \"Use to determine if more information is needed. \"\n                \"Additionally, use this as a reference to determine if you have already completed a task/what items are already available, to avoid repeating actions. \"\n                \"All items here are already shown to the user, so do not repeat information from these fields unless summarising, providing extra information or otherwise. \"\n                \"E.g., do not list out anything from here, only provide new content to the user.\"\n            )\n            environment_prefix = \"${environment}\"\n            environment_field: dict = dspy.InputField(\n                prefix=environment_prefix, desc=environment_desc\n            )\n            extended_signature = extended_signature.append(\n                name=\"environment\", field=environment_field, type_=dict\n            )\n\n        # -- Collection Schema Input --\n        if collection_schemas:\n            collection_schemas_desc = (\n                \"Metadata about available collections and their schemas: \"\n                \"This is a dictionary with the following fields: \"\n                \"{\\n\"\n                \"    name: collection name,\\n\"\n                \"    length: number of objects in the collection,\\n\"\n                \"    summary: summary of the collection,\\n\"\n                \"    fields: [\\n\"\n                \"        {\\n\"\n                \"            name: field_name,\\n\"\n                \"            groups: a dict with the value and count of each group.\\n\"\n                \"                a comprehensive list of all unique values that exist in the field.\\n\"\n                \"                if this is None, then no relevant groups were found.\\n\"\n                \"                these values are string, but the actual values in the collection are the 'type' of the field.\\n\"\n                \"            mean: mean of the field. if the field is text, this refers to the means length (in tokens) of the texts in this field. if the type is a list, this refers to the mean length of the lists,\\n\"\n                \"            range: minimum and maximum values of the length,\\n\"\n                \"            type: the data type of the field.\\n\"\n                \"        },\\n\"\n                \"        ...\\n\"\n                \"    ]\\n\"\n                \"}\\n\"\n            )\n            collection_schemas_prefix = \"${collection_schemas}\"\n            collection_schemas_field: dict = dspy.InputField(\n                prefix=collection_schemas_prefix, desc=collection_schemas_desc\n            )\n            extended_signature = extended_signature.append(\n                name=\"collection_schemas\", field=collection_schemas_field, type_=dict\n            )\n\n        # -- Tasks Completed Input --\n        if tasks_completed:\n            tasks_completed_desc = (\n                \"Which tasks have been completed in order. \"\n                \"These are numbered so that higher numbers are more recent. \"\n                \"Separated by prompts (so you should identify the prompt you are currently working on to see what tasks have been completed so far) \"\n                \"Also includes reasoning for each task, to continue a decision logic across tasks. \"\n                \"Use this to determine whether future searches, for this prompt are necessary, and what task(s) to choose. \"\n                \"It is IMPORTANT that you separate what actions have been completed for which prompt, so you do not think you have failed an attempt for a different prompt.\"\n            )\n            tasks_completed_prefix = \"${tasks_completed}\"\n            tasks_completed_field: str = dspy.InputField(\n                prefix=tasks_completed_prefix, desc=tasks_completed_desc\n            )\n            extended_signature = extended_signature.append(\n                name=\"tasks_completed\", field=tasks_completed_field, type_=str\n            )\n\n        # -- Impossible Field --\n        if impossible:\n            impossible_desc = (\n                \"Given the actions you have available, and the environment/information. \"\n                \"Is the task impossible to complete? \"\n                \"I.e., do you wish that you had a different task to perform/choose from and hence should return to the base of the decision tree?\"\n                \"Do not base this judgement on the entire prompt, as it is possible that other agents can perform other aspects of the request.\"\n                \"Do not judge impossibility based on if tasks have been completed, only on the current action and environment.\"\n            )\n            impossible_prefix = \"${impossible}\"\n            impossible_field: bool = dspy.OutputField(\n                prefix=impossible_prefix, desc=impossible_desc\n            )\n            extended_signature = extended_signature.prepend(\n                name=\"impossible\", field=impossible_field, type_=bool\n            )\n\n        # -- Message Update Output --\n        if message_update:\n            message_update_desc = (\n                \"Continue your current message to the user \"\n                \"(latest assistant field in conversation history) with ONE concise sentence that: \"\n                \"- Describes NEW technical details about your latest action \"\n                \"- Highlights specific parameters or logic you just applied \"\n                \"- Avoids repeating anything from conversation history \"\n                \"- Speaks directly to them (no 'the user'), gender neutral message \"\n                \"Just provide the new sentence update, not the full message from the conversation history. \"\n                \"Your response should be based on only the part of the user's request that you can work on. \"\n                \"It is possible other agents can perform other aspects of the request, so do not respond as if you cannot complete the entire request.\"\n            )\n\n            message_update_prefix = \"${message_update}\"\n            message_update_field: str = dspy.OutputField(\n                prefix=message_update_prefix, desc=message_update_desc\n            )\n            extended_signature = extended_signature.prepend(\n                name=\"message_update\", field=message_update_field, type_=str\n            )\n\n        # -- Reasoning Field --\n        if reasoning:\n            reasoning_desc = (\n                \"Reasoning: Repeat relevant parts of the any context within your environment, \"\n                \"Evaluate all relevant information from the inputs, including any previous errors if applicable, \"\n                \"use this to think step by step in order to answer the query.\"\n                \"Limit your reasoning to maximum 150 words. Only exceed this if the task is very complex.\"\n            )\n            reasoning_prefix = \"${reasoning}\"\n            reasoning_field: str = dspy.OutputField(\n                prefix=reasoning_prefix, desc=reasoning_desc\n            )\n            extended_signature = extended_signature.prepend(\n                name=\"reasoning\", field=reasoning_field, type_=str\n            )\n\n        # -- Predict --\n        self.predict = dspy.Predict(extended_signature, **config)\n        self.predict.signature.instructions += elysia_meta_prompt  # type: ignore\n\n    def _add_tree_data_inputs(self, kwargs: dict):\n\n        # Add the tree data inputs to the kwargs\n        kwargs[\"user_prompt\"] = self.tree_data.user_prompt\n        kwargs[\"conversation_history\"] = self.tree_data.conversation_history\n        kwargs[\"atlas\"] = self.tree_data.atlas\n        kwargs[\"previous_errors\"] = self.tree_data.get_errors()\n\n        # Add the optional inputs to the kwargs\n        if self.environment:\n            kwargs[\"environment\"] = self.tree_data.environment.environment\n\n        if self.collection_schemas:\n            if self.collection_names != []:\n                kwargs[\"collection_schemas\"] = (\n                    self.tree_data.output_collection_metadata(\n                        collection_names=self.collection_names, with_mappings=False\n                    )\n                )\n            else:\n                kwargs[\"collection_schemas\"] = (\n                    self.tree_data.output_collection_metadata(with_mappings=False)\n                )\n\n        if self.tasks_completed:\n            kwargs[\"tasks_completed\"] = self.tree_data.tasks_completed_string()\n\n        return kwargs\n\n    def forward(self, **kwargs):\n        kwargs = self._add_tree_data_inputs(kwargs)\n        return self.predict(**kwargs)\n\n    async def aforward(self, **kwargs):\n        kwargs = self._add_tree_data_inputs(kwargs)\n        return await self.predict.acall(**kwargs)\n\n    async def aforward_with_feedback_examples(\n        self,\n        feedback_model: str,\n        client_manager: ClientManager,\n        base_lm: dspy.LM,\n        complex_lm: dspy.LM,\n        num_base_lm_examples: int = 3,\n        return_example_uuids: bool = False,\n        **kwargs,\n    ) -&gt; tuple[dspy.Prediction, list[str]] | dspy.Prediction:\n        \"\"\"\n        Use the forward pass of the module with feedback examples.\n        This will first retrieve examples from the feedback collection, and use those as few-shot examples to run the module.\n        It retrieves based from vectorising and searching on the user's prompt, finding similar prompts from the feedback collection.\n        This is an EXPERIMENTAL feature, and may not work as expected.\n\n        If the number of examples is less than `num_base_lm_examples`, the module will use the complex LM.\n        Otherwise, it will use the base LM. This is so that the less accurate, but faster base LM can be used when guidance is available.\n        However, when there are insufficient examples, the complex LM will be used.\n\n        Args:\n            feedback_model (str): The label of the feedback data to use as examples.\n                E.g., \"decision\" is the default name given to examples for the LM in the decision tree.\n                This is used to retrieve the examples from the feedback collection.\n            client_manager (ClientManager): The client manager to use.\n            base_lm (dspy.LM): The base LM to (conditionally) use.\n            complex_lm (dspy.LM): The complex LM to (conditionally) use.\n            num_base_lm_examples (int): The threshold number of examples to use the base LM.\n                When there are fewer examples than this, the complex LM will be used.\n            **kwargs (Any): The keyword arguments to pass to the forward pass.\n                Important: All additional inputs to the DSPy module should be passed here as keyword arguments.\n                Also: Do not include `lm` in the kwargs, as this will be set automatically.\n\n        Returns:\n            (dspy.Prediction): The prediction from the forward pass.\n        \"\"\"\n\n        examples, uuids = await retrieve_feedback(\n            client_manager, self.tree_data.user_prompt, feedback_model, n=10\n        )\n        if len(examples) &gt; 0:\n            optimizer = dspy.LabeledFewShot(k=10)\n            optimized_module = optimizer.compile(self, trainset=examples)\n        else:\n            if return_example_uuids:\n                return (\n                    await self.aforward(lm=complex_lm, **kwargs),\n                    uuids,\n                )\n            else:\n                return await self.aforward(lm=complex_lm, **kwargs)\n\n        # Select the LM to use based on the number of examples\n        if len(examples) &lt; num_base_lm_examples:\n            if return_example_uuids:\n                return (\n                    await optimized_module.aforward(lm=complex_lm, **kwargs),\n                    uuids,\n                )\n            else:\n                return await optimized_module.aforward(lm=complex_lm, **kwargs)\n        else:\n            if return_example_uuids:\n                return (\n                    await optimized_module.aforward(lm=base_lm, **kwargs),\n                    uuids,\n                )\n            else:\n                return await optimized_module.aforward(lm=base_lm, **kwargs)\n</code></pre>"},{"location":"Reference/Util/#elysia.util.elysia_chain_of_thought.ElysiaChainOfThought.__init__","title":"<code>__init__(signature, tree_data, reasoning=True, impossible=True, message_update=True, environment=False, collection_schemas=False, tasks_completed=False, collection_names=[], **config)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>signature</code> <code>Type[Signature]</code> <p>The signature of the module.</p> required <code>tree_data</code> <code>TreeData</code> <p>Required. The tree data from the Elysia decision tree. Used to input the current state of the tree into the prompt. If you are using this module as part of a tool, the <code>tree_data</code> is an input to the tool call.</p> required <code>reasoning</code> <code>bool</code> <p>Whether to include a reasoning input (chain of thought).</p> <code>True</code> <code>impossible</code> <code>bool</code> <p>Whether to include a boolean flag indicating whether the task is impossible. This is useful for stopping the tree from continuing and returning to the base of the decision tree. For example, the model judges a query impossible to execute, or the user has not provided enough information.</p> <code>True</code> <code>message_update</code> <code>bool</code> <p>Whether to include a message update input. If True, the LLM output will include a brief 'update' message to the user. This describes the current action the LLM is performing. Designed to increase interactivity and provide the user with information before the final output.</p> <code>True</code> <code>environment</code> <code>bool</code> <p>Whether to include an environment input. If True, the module will include the currently stored data from previous tasks and actions into the prompt. This is useful so that the LLM knows what has already been done, and can avoid repeating actions. Or to use information from the environment to perform the new action.</p> <code>False</code> <code>collection_schemas</code> <code>bool</code> <p>Whether to include a collection schema input. If True, the module will include the preprocessed collection schemas in the prompt input. This is useful so that the LLM knows the structure of the collections, if querying or similar. Use this sparingly, as it will use a large amount of tokens. You can specify <code>collection_names</code> to only include certain collections in this schema.</p> <code>False</code> <code>tasks_completed</code> <code>bool</code> <p>Whether to include a tasks completed input. If True, the module will include the list of tasks completed input. This is a nicely formatted list of the tasks that have been completed, with the reasoning for each task. This is used so that the LLM has a 'stream of consciousness' of what has already been done, as well as to stop it from repeating actions. Other information is included in the <code>tasks_completed</code> field that format outputs from previous tasks. This is useful for continuing a decision logic across tasks, or to reinforce key information.</p> <code>False</code> <code>collection_names</code> <code>list[str]</code> <p>A list of collection names to include in the prompt. If provided, this will modify the collection schema input to only include the collections in this list. This is useful if you only want to include certain collections in the prompt. And to reduce token usage.</p> <code>[]</code> <code>**config</code> <code>Any</code> <p>The DSPy configuration for the module.</p> <code>{}</code> Source code in <code>elysia/util/elysia_chain_of_thought.py</code> <pre><code>def __init__(\n    self,\n    signature: Type[Signature],\n    tree_data: TreeData,\n    reasoning: bool = True,\n    impossible: bool = True,\n    message_update: bool = True,\n    environment: bool = False,\n    collection_schemas: bool = False,\n    tasks_completed: bool = False,\n    collection_names: list[str] = [],\n    **config,\n):\n    \"\"\"\n    Args:\n        signature (Type[dspy.Signature]): The signature of the module.\n        tree_data (TreeData): Required. The tree data from the Elysia decision tree.\n            Used to input the current state of the tree into the prompt.\n            If you are using this module as part of a tool, the `tree_data` is an input to the tool call.\n        reasoning (bool): Whether to include a reasoning input (chain of thought).\n        impossible (bool): Whether to include a boolean flag indicating whether the task is impossible.\n            This is useful for stopping the tree from continuing and returning to the base of the decision tree.\n            For example, the model judges a query impossible to execute, or the user has not provided enough information.\n        message_update (bool): Whether to include a message update input.\n            If True, the LLM output will include a brief 'update' message to the user.\n            This describes the current action the LLM is performing.\n            Designed to increase interactivity and provide the user with information before the final output.\n        environment (bool): Whether to include an environment input.\n            If True, the module will include the currently stored data from previous tasks and actions into the prompt.\n            This is useful so that the LLM knows what has already been done, and can avoid repeating actions.\n            Or to use information from the environment to perform the new action.\n        collection_schemas (bool): Whether to include a collection schema input.\n            If True, the module will include the preprocessed collection schemas in the prompt input.\n            This is useful so that the LLM knows the structure of the collections, if querying or similar.\n            Use this sparingly, as it will use a large amount of tokens.\n            You can specify `collection_names` to only include certain collections in this schema.\n        tasks_completed (bool): Whether to include a tasks completed input.\n            If True, the module will include the list of tasks completed input.\n            This is a nicely formatted list of the tasks that have been completed, with the reasoning for each task.\n            This is used so that the LLM has a 'stream of consciousness' of what has already been done,\n            as well as to stop it from repeating actions.\n            Other information is included in the `tasks_completed` field that format outputs from previous tasks.\n            This is useful for continuing a decision logic across tasks, or to reinforce key information.\n        collection_names (list[str]): A list of collection names to include in the prompt.\n            If provided, this will modify the collection schema input to only include the collections in this list.\n            This is useful if you only want to include certain collections in the prompt.\n            And to reduce token usage.\n        **config (Any): The DSPy configuration for the module.\n    \"\"\"\n\n    super().__init__()\n\n    signature = ensure_signature(signature)  # type: ignore\n\n    # Create a shallow copy of the tree_data\n    self.tree_data = copy(tree_data)\n\n    # Note which inputs are required\n    self.message_update = message_update\n    self.environment = environment\n    self.collection_schemas = collection_schemas\n    self.tasks_completed = tasks_completed\n    self.collection_names = collection_names\n    self.reasoning = reasoning\n    self.impossible = impossible\n\n    # == Inputs ==\n\n    # -- User Prompt --\n    user_prompt_desc = (\n        \"The user's original question/prompt that needs to be answered. \"\n        \"This, possibly combined with the conversation history, will be used to determine your current action.\"\n    )\n    user_prompt_prefix = \"${user_prompt}\"\n    user_prompt_field: str = dspy.InputField(\n        prefix=user_prompt_prefix, desc=user_prompt_desc\n    )\n\n    # -- Conversation History --\n    conversation_history_desc = (\n        \"Previous messages between user and assistant in chronological order: \"\n        \"[{'role': 'user'|'assistant', 'content': str}] \"\n        \"Use this to maintain conversation context and avoid repetition.\"\n    )\n    conversation_history_prefix = \"${conversation_history}\"\n    conversation_history_field: list[dict] = dspy.InputField(\n        prefix=conversation_history_prefix, desc=conversation_history_desc\n    )\n\n    # -- Atlas --\n    atlas_desc = (\n        \"Your guide to how you should proceed as an agent in this task. \"\n        \"This is pre-defined by the user.\"\n    )\n    atlas_prefix = \"${atlas}\"\n    atlas_field: Atlas = dspy.InputField(prefix=atlas_prefix, desc=atlas_desc)\n\n    # -- Errors --\n    errors_desc = (\n        \"Any errors that have occurred during the previous attempt at this action. \"\n        \"This is a list of dictionaries, containing details of the error. \"\n        \"Make an attempt at providing different output to avoid this error now. \"\n        \"If this error is repeated, or you judge it to be unsolvable, you can set `impossible` to True\"\n    )\n    errors_prefix = \"${previous_errors}\"\n    errors_field: list[dict] = dspy.InputField(\n        prefix=errors_prefix, desc=errors_desc\n    )\n\n    # -- Add to Signature --\n    extended_signature = signature.prepend(\n        name=\"user_prompt\", field=user_prompt_field, type_=str\n    )\n    extended_signature = extended_signature.append(\n        name=\"conversation_history\",\n        field=conversation_history_field,\n        type_=list[dict],\n    )\n    extended_signature = extended_signature.append(\n        name=\"atlas\", field=atlas_field, type_=Atlas\n    )\n    extended_signature = extended_signature.append(\n        name=\"previous_errors\", field=errors_field, type_=list[dict]\n    )\n\n    # == Optional Inputs / Outputs ==\n\n    # -- Environment Input --\n    if environment:\n        environment_desc = (\n            \"Information gathered from completed tasks. \"\n            \"Empty if no data has been retrieved yet. \"\n            \"Use to determine if more information is needed. \"\n            \"Additionally, use this as a reference to determine if you have already completed a task/what items are already available, to avoid repeating actions. \"\n            \"All items here are already shown to the user, so do not repeat information from these fields unless summarising, providing extra information or otherwise. \"\n            \"E.g., do not list out anything from here, only provide new content to the user.\"\n        )\n        environment_prefix = \"${environment}\"\n        environment_field: dict = dspy.InputField(\n            prefix=environment_prefix, desc=environment_desc\n        )\n        extended_signature = extended_signature.append(\n            name=\"environment\", field=environment_field, type_=dict\n        )\n\n    # -- Collection Schema Input --\n    if collection_schemas:\n        collection_schemas_desc = (\n            \"Metadata about available collections and their schemas: \"\n            \"This is a dictionary with the following fields: \"\n            \"{\\n\"\n            \"    name: collection name,\\n\"\n            \"    length: number of objects in the collection,\\n\"\n            \"    summary: summary of the collection,\\n\"\n            \"    fields: [\\n\"\n            \"        {\\n\"\n            \"            name: field_name,\\n\"\n            \"            groups: a dict with the value and count of each group.\\n\"\n            \"                a comprehensive list of all unique values that exist in the field.\\n\"\n            \"                if this is None, then no relevant groups were found.\\n\"\n            \"                these values are string, but the actual values in the collection are the 'type' of the field.\\n\"\n            \"            mean: mean of the field. if the field is text, this refers to the means length (in tokens) of the texts in this field. if the type is a list, this refers to the mean length of the lists,\\n\"\n            \"            range: minimum and maximum values of the length,\\n\"\n            \"            type: the data type of the field.\\n\"\n            \"        },\\n\"\n            \"        ...\\n\"\n            \"    ]\\n\"\n            \"}\\n\"\n        )\n        collection_schemas_prefix = \"${collection_schemas}\"\n        collection_schemas_field: dict = dspy.InputField(\n            prefix=collection_schemas_prefix, desc=collection_schemas_desc\n        )\n        extended_signature = extended_signature.append(\n            name=\"collection_schemas\", field=collection_schemas_field, type_=dict\n        )\n\n    # -- Tasks Completed Input --\n    if tasks_completed:\n        tasks_completed_desc = (\n            \"Which tasks have been completed in order. \"\n            \"These are numbered so that higher numbers are more recent. \"\n            \"Separated by prompts (so you should identify the prompt you are currently working on to see what tasks have been completed so far) \"\n            \"Also includes reasoning for each task, to continue a decision logic across tasks. \"\n            \"Use this to determine whether future searches, for this prompt are necessary, and what task(s) to choose. \"\n            \"It is IMPORTANT that you separate what actions have been completed for which prompt, so you do not think you have failed an attempt for a different prompt.\"\n        )\n        tasks_completed_prefix = \"${tasks_completed}\"\n        tasks_completed_field: str = dspy.InputField(\n            prefix=tasks_completed_prefix, desc=tasks_completed_desc\n        )\n        extended_signature = extended_signature.append(\n            name=\"tasks_completed\", field=tasks_completed_field, type_=str\n        )\n\n    # -- Impossible Field --\n    if impossible:\n        impossible_desc = (\n            \"Given the actions you have available, and the environment/information. \"\n            \"Is the task impossible to complete? \"\n            \"I.e., do you wish that you had a different task to perform/choose from and hence should return to the base of the decision tree?\"\n            \"Do not base this judgement on the entire prompt, as it is possible that other agents can perform other aspects of the request.\"\n            \"Do not judge impossibility based on if tasks have been completed, only on the current action and environment.\"\n        )\n        impossible_prefix = \"${impossible}\"\n        impossible_field: bool = dspy.OutputField(\n            prefix=impossible_prefix, desc=impossible_desc\n        )\n        extended_signature = extended_signature.prepend(\n            name=\"impossible\", field=impossible_field, type_=bool\n        )\n\n    # -- Message Update Output --\n    if message_update:\n        message_update_desc = (\n            \"Continue your current message to the user \"\n            \"(latest assistant field in conversation history) with ONE concise sentence that: \"\n            \"- Describes NEW technical details about your latest action \"\n            \"- Highlights specific parameters or logic you just applied \"\n            \"- Avoids repeating anything from conversation history \"\n            \"- Speaks directly to them (no 'the user'), gender neutral message \"\n            \"Just provide the new sentence update, not the full message from the conversation history. \"\n            \"Your response should be based on only the part of the user's request that you can work on. \"\n            \"It is possible other agents can perform other aspects of the request, so do not respond as if you cannot complete the entire request.\"\n        )\n\n        message_update_prefix = \"${message_update}\"\n        message_update_field: str = dspy.OutputField(\n            prefix=message_update_prefix, desc=message_update_desc\n        )\n        extended_signature = extended_signature.prepend(\n            name=\"message_update\", field=message_update_field, type_=str\n        )\n\n    # -- Reasoning Field --\n    if reasoning:\n        reasoning_desc = (\n            \"Reasoning: Repeat relevant parts of the any context within your environment, \"\n            \"Evaluate all relevant information from the inputs, including any previous errors if applicable, \"\n            \"use this to think step by step in order to answer the query.\"\n            \"Limit your reasoning to maximum 150 words. Only exceed this if the task is very complex.\"\n        )\n        reasoning_prefix = \"${reasoning}\"\n        reasoning_field: str = dspy.OutputField(\n            prefix=reasoning_prefix, desc=reasoning_desc\n        )\n        extended_signature = extended_signature.prepend(\n            name=\"reasoning\", field=reasoning_field, type_=str\n        )\n\n    # -- Predict --\n    self.predict = dspy.Predict(extended_signature, **config)\n    self.predict.signature.instructions += elysia_meta_prompt  # type: ignore\n</code></pre>"},{"location":"Reference/Util/#elysia.util.elysia_chain_of_thought.ElysiaChainOfThought.aforward_with_feedback_examples","title":"<code>aforward_with_feedback_examples(feedback_model, client_manager, base_lm, complex_lm, num_base_lm_examples=3, return_example_uuids=False, **kwargs)</code>  <code>async</code>","text":"<p>Use the forward pass of the module with feedback examples. This will first retrieve examples from the feedback collection, and use those as few-shot examples to run the module. It retrieves based from vectorising and searching on the user's prompt, finding similar prompts from the feedback collection. This is an EXPERIMENTAL feature, and may not work as expected.</p> <p>If the number of examples is less than <code>num_base_lm_examples</code>, the module will use the complex LM. Otherwise, it will use the base LM. This is so that the less accurate, but faster base LM can be used when guidance is available. However, when there are insufficient examples, the complex LM will be used.</p> <p>Parameters:</p> Name Type Description Default <code>feedback_model</code> <code>str</code> <p>The label of the feedback data to use as examples. E.g., \"decision\" is the default name given to examples for the LM in the decision tree. This is used to retrieve the examples from the feedback collection.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager to use.</p> required <code>base_lm</code> <code>LM</code> <p>The base LM to (conditionally) use.</p> required <code>complex_lm</code> <code>LM</code> <p>The complex LM to (conditionally) use.</p> required <code>num_base_lm_examples</code> <code>int</code> <p>The threshold number of examples to use the base LM. When there are fewer examples than this, the complex LM will be used.</p> <code>3</code> <code>**kwargs</code> <code>Any</code> <p>The keyword arguments to pass to the forward pass. Important: All additional inputs to the DSPy module should be passed here as keyword arguments. Also: Do not include <code>lm</code> in the kwargs, as this will be set automatically.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Prediction</code> <p>The prediction from the forward pass.</p> Source code in <code>elysia/util/elysia_chain_of_thought.py</code> <pre><code>async def aforward_with_feedback_examples(\n    self,\n    feedback_model: str,\n    client_manager: ClientManager,\n    base_lm: dspy.LM,\n    complex_lm: dspy.LM,\n    num_base_lm_examples: int = 3,\n    return_example_uuids: bool = False,\n    **kwargs,\n) -&gt; tuple[dspy.Prediction, list[str]] | dspy.Prediction:\n    \"\"\"\n    Use the forward pass of the module with feedback examples.\n    This will first retrieve examples from the feedback collection, and use those as few-shot examples to run the module.\n    It retrieves based from vectorising and searching on the user's prompt, finding similar prompts from the feedback collection.\n    This is an EXPERIMENTAL feature, and may not work as expected.\n\n    If the number of examples is less than `num_base_lm_examples`, the module will use the complex LM.\n    Otherwise, it will use the base LM. This is so that the less accurate, but faster base LM can be used when guidance is available.\n    However, when there are insufficient examples, the complex LM will be used.\n\n    Args:\n        feedback_model (str): The label of the feedback data to use as examples.\n            E.g., \"decision\" is the default name given to examples for the LM in the decision tree.\n            This is used to retrieve the examples from the feedback collection.\n        client_manager (ClientManager): The client manager to use.\n        base_lm (dspy.LM): The base LM to (conditionally) use.\n        complex_lm (dspy.LM): The complex LM to (conditionally) use.\n        num_base_lm_examples (int): The threshold number of examples to use the base LM.\n            When there are fewer examples than this, the complex LM will be used.\n        **kwargs (Any): The keyword arguments to pass to the forward pass.\n            Important: All additional inputs to the DSPy module should be passed here as keyword arguments.\n            Also: Do not include `lm` in the kwargs, as this will be set automatically.\n\n    Returns:\n        (dspy.Prediction): The prediction from the forward pass.\n    \"\"\"\n\n    examples, uuids = await retrieve_feedback(\n        client_manager, self.tree_data.user_prompt, feedback_model, n=10\n    )\n    if len(examples) &gt; 0:\n        optimizer = dspy.LabeledFewShot(k=10)\n        optimized_module = optimizer.compile(self, trainset=examples)\n    else:\n        if return_example_uuids:\n            return (\n                await self.aforward(lm=complex_lm, **kwargs),\n                uuids,\n            )\n        else:\n            return await self.aforward(lm=complex_lm, **kwargs)\n\n    # Select the LM to use based on the number of examples\n    if len(examples) &lt; num_base_lm_examples:\n        if return_example_uuids:\n            return (\n                await optimized_module.aforward(lm=complex_lm, **kwargs),\n                uuids,\n            )\n        else:\n            return await optimized_module.aforward(lm=complex_lm, **kwargs)\n    else:\n        if return_example_uuids:\n            return (\n                await optimized_module.aforward(lm=base_lm, **kwargs),\n                uuids,\n            )\n        else:\n            return await optimized_module.aforward(lm=base_lm, **kwargs)\n</code></pre>"}]}