(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[3360],{73560:function(e,t,a){Promise.resolve().then(a.bind(a,40041))},40041:function(e,t,a){"use strict";a.r(t),a.d(t,{default:function(){return g}});var n=a(57437),i=a(2265),o=a(91312);let s={id:"12345",query:"What is Elysia?",messages:[{type:"User",id:"12345",query_id:"12345",conversation_id:"12345",payload:{type:"text",metadata:{},code:{language:"",title:"",text:""},objects:["What is Elysia?"]}},{type:"text",id:"12345",query_id:"12345",conversation_id:"12345",payload:{type:"response",metadata:null,objects:[{text:"Elysia is an agentic retrieval augmented generation (RAG) service within Weaviate. I'm here to help you query Weaviate's collections, retrieve relevant information, and answer your questions. This includes various ways to query, such as filtering, sorting, querying multiple collections, and providing summaries and textual responses. I'll dynamically display retrieved objects from the collections. I work via a tree-based approach, using your question to generate a tree of potential queries. Each branch leads to an agent performing a specific task. These agents communicate via prompts."}]}}],finished:!0,query_start:new Date,query_end:new Date(new Date().getTime()+1e3),NER:{text:"What is Elysia?",noun_spans:[],entity_spans:[[8,14]]},feedback:2,index:0},r={id:"12345",query:"What is Verba?",messages:[{type:"User",id:"36241ec2-3a1c-497b-b697-5631e2694480",query_id:"70971527-63f2-404f-8aca-11a7c41cab1f",conversation_id:"dad2e092-61ba-45d9-9307-35947c3469c5",payload:{type:"text",metadata:{},code:{language:"",title:"",text:""},objects:["What is Verba?"]}},{type:"text",conversation_id:"dad2e092-61ba-45d9-9307-35947c3469c5",query_id:"70971527-63f2-404f-8aca-11a7c41cab1f",id:"tex-3a759a2a-35d3-4678-819d-964da7bc903a",payload:{type:"response",metadata:{},objects:[{text:'I will query the available datasets to find information related to "Verba."'}]}},{type:"text",conversation_id:"dad2e092-61ba-45d9-9307-35947c3469c5",query_id:"70971527-63f2-404f-8aca-11a7c41cab1f",id:"tex-1b66e997-a70b-4e9c-b002-5e6a473c767c",payload:{type:"response",metadata:{},objects:[{text:"I'll search the example_verba_github_issues collection using a semantic search to provide a summary of what Verba is."}]}},{type:"text",conversation_id:"dad2e092-61ba-45d9-9307-35947c3469c5",query_id:"70971527-63f2-404f-8aca-11a7c41cab1f",id:"tex-86583209-1953-4393-afc8-dcf4fe9a8c73",payload:{type:"response",metadata:{},objects:[{text:"I'm now generating summaries for each issue, focusing on the title, author, and number of comments for easy readability."}]}},{type:"result",conversation_id:"dad2e092-61ba-45d9-9307-35947c3469c5",query_id:"70971527-63f2-404f-8aca-11a7c41cab1f",id:"res-bc03c61c-1339-4a2a-9212-314d67c5328f",payload:{type:"document",objects:[{collection_name:"example_verba_github_issues",content:"NaN",date:"2024-05-17T15:20:52Z",title:"Fix https://github.com/weaviate/Verba/issues/162",author:"hholtmann",category:[],uuid:"60c0be46eb9358318ebcb189c5452215",summary:"Issue #163 (closed) by hholtmann: Fix https://github.com/weaviate/Verba/issues/162",chunk_spans:[]},{collection_name:"example_verba_github_issues",content:"NaN",date:"2023-10-20T06:54:17Z",title:"Remove Unnececary imports",author:"agi-dude",category:[],uuid:"a72ff03d1015507b87b43207e3006121",summary:"Issue #27 (closed) by agi-dude: Remove Unnececary imports (4 comments)",chunk_spans:[]},{collection_name:"example_verba_github_issues",content:"NaN",date:"2024-09-10T21:14:41Z",title:"this is v2",author:"RyanLisse",category:[],uuid:"4d7c71af5dc95ee5a2d1b75a7d5447c6",summary:"Issue #281 (closed) by RyanLisse: this is v2 (1 comment)",chunk_spans:[]},{collection_name:"example_verba_github_issues",content:"NaN",date:"2024-07-01T09:47:05Z",title:"Add more SentenceTransformers (PR #152)",author:"thomashacker",category:[],uuid:"c8e85dc0b00d54fd96dd3253f139b256",summary:"Issue #234 (closed) by thomashacker: Add more SentenceTransformers (PR #152)",chunk_spans:[]},{collection_name:"example_verba_github_issues",content:"NaN",date:"2024-05-20T01:52:41Z",title:"N",author:"Jannice89",category:[],uuid:"3301494c93625e2ea39234579274fe1f",summary:"Issue #176 (closed) by Jannice89: N",chunk_spans:[]}],metadata:{collection_name:"example_verba_github_issues",return_type:"document",output_type:"summary",query_text:"Verba",query_type:"semantic",code:{language:"python",title:"Query",text:"collection.query.near_text(query='Verba', limit=5)"},chunked:!1},code:{language:"python",title:"Query",text:"collection.query.near_text(query='Verba', limit=5)"}}},{type:"text",conversation_id:"dad2e092-61ba-45d9-9307-35947c3469c5",query_id:"70971527-63f2-404f-8aca-11a7c41cab1f",id:"tex-07cdee1c-1fd2-4153-b375-633b2c26adfc",payload:{type:"summary",metadata:{},objects:[{text:"Verba is a collaborative software project on GitHub related to Weaviate, focusing on various enhancements and bug fixes. Recent closed issues include efforts to fix specific problems, such as addressing a previous issue, removing unnecessary imports, and adding more SentenceTransformers. The project reflects active maintenance and contributions from multiple developers, indicating its ongoing evolution and improvement.",title:"Overview of Verba"}]}}],finished:!0,query_start:new Date,query_end:new Date(new Date().getTime()+1e3),NER:{text:"What is Verba?",noun_spans:[],entity_spans:[[8,13]]},feedback:1,index:0},l={id:"12345",query:"Let's start this chat!",messages:[{type:"User",id:"12345",query_id:"12345",conversation_id:"12345",payload:{type:"text",metadata:{},code:{language:"",title:"",text:""},objects:["Let's start this chat!"]}}],finished:!1,query_start:new Date,query_end:new Date(new Date().getTime()+1e3),NER:{text:"Let's start this chat!",noun_spans:[[17,21]],entity_spans:[]},feedback:0,index:0};var d=a(99376);let c={id:"12345",query:"Who is Mark Robson?",messages:[{type:"User",id:"d7fcab26-48f9-40ac-a8ba-c987710856f6",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",payload:{type:"text",metadata:{},code:{language:"",title:"",text:""},objects:["Who is Mark Robson?"]}},{type:"text",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",id:"tex-689a30c8-7a61-42e7-bcb8-538a3ba25cb9",payload:{type:"response",metadata:{},objects:[{text:"I'll see what I can find about Mark Robson for you. One moment please. I am now searching the available knowledge bases for information about Mark Robson to answer your question. "}]}},{type:"text",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",id:"tex-97f7afe4-4ddd-4678-88ee-4d6fb799f619",payload:{type:"response",metadata:{},objects:[{text:"I am now executing a search query across all available datasets to find information about Mark Robson."}]}},{type:"text",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",id:"tex-afcd6e57-963b-4685-a186-fe3fd3cdc14a",payload:{type:"response",metadata:{},objects:[{text:"I'll search the ML_Wikipedia collection using semantic search to find any relevant information about Mark Robson."}]}},{type:"result",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",id:"res-1c8ad4be-52a4-4fe0-a2f3-e8106eebdcbb",payload:{type:"document",objects:[{content:"Tony Robinson is a researcher in the application of recurrent neural networks to speech recognition, being one of the first to discover the practical capabilities of deep neural networks and its application to speech recognition. == Education and Early Career == Robinson studied natural sciences at Cambridge University between 1981 and 1984, where he specialized in physics. He went on to complete an MPhil in computer speech and language processing in 1985 and continued with a PhD in the same area in 1989, both at Cambridge. He first published on the topic of speech recognition during his PhD and has published over a hundred widely cited research papers on automatic speech recognition (ASR) in the years since. == Entrepreneurial Career == In 1995, Robinson formed SoftSound Ltd, a speech technology company which was acquired by Autonomy with a view to using the technology to make unstructured video and voice data easily searchable. Robinson helped build the fastest large vocabulary speech recognition system available at the time, and operating in more languages than any other model, based on recurrent neural networks. From 2008 to 2010, Robinson was the Director of the Advanced Speech Group at SpinVox, a provider of speech-to-text conversion services for carrier markets, including wireless, VoIP and cable. Their Automatic Speech Recognition (ASR) system was, for a time, being used more than one million times per day and SpinVox was subsequently acquired by global speech technology company Nuance. Robinson was also founder of Speechmatics, which launched its cloud-based speech recognition services in 2012. Speechmatics subsequently announced a new technology in accelerated new language modeling late in 2017. Robinson continues to publish papers in speech recognition technology, especially in the area of statistical language modelling. == References ==",date:"2025-01-31T16:24:35.644570Z",title:"Tony Robinson (speech recognition)",author:"",collection_name:"ML_Wikipedia",category:["All BLP articles lacking sources","All articles with a promotional tone","Alumni of the University of Cambridge","Articles with a promotional tone from May 2018","Articles with multiple maintenance issues","Articles with short description","BLP articles lacking sources from May 2018","British businesspeople","British computer scientists","Living people","Natural language processing researchers","Short description matches Wikidata","Year of birth missing (living people)"],summary:"",chunk_spans:[[0,943],[719,1630]]},{content:"Frank Rosenblatt (July 11, 1928 – July 11, 1971) was an American psychologist notable in the field of artificial intelligence. He is sometimes called the father of deep learning for his pioneering work on artificial neural networks. == Life and career == Rosenblatt was born into a Jewish family in New Rochelle, New York as the son of Dr. Frank and Katherine Rosenblatt. After graduating from The Bronx High School of Science in 1946, he attended Cornell University, where he obtained his A.B. in 1950 and his Ph.D. in 1956. For his PhD thesis he built a custom-made computer, the Electronic Profile Analyzing Computer (EPAC), to perform multidimensional analysis for psychometrics. He used it between 1951 and 1953 to analyze psychometric data collected for his PhD thesis. The data were collected from a paid, 600 item survey of more than 200 Cornell undergraduates. The total computational cost was 2.5 million arithmetic operations, necessitating the use of an IBM CPC as well. It was said that 15 minutes of data processing took just 2 seconds.: 32 He subsequently moved to Cornell Aeronautical Laboratory in Buffalo, New York, where he was successively a research psychologist, senior psychologist, and head of the cognitive systems section. It was there that he also conducted the early work on perceptrons, which culminated in the development and hardware construction in 1960 of the Mark I Perceptron, essentially the first computer that could learn new skills by trial and error, using a type of neural network that simulates human thought processes. Rosenblatt's research interests were exceptionally broad. In 1959 he went to Cornell's Ithaca campus as director of the Cognitive Systems Research Program and lecturer in the Psychology Department. In 1966 he joined the Section of Neurobiology and Behavior within the newly formed Division of Biological Sciences, as associate professor. Also in 1966, he became fascinated with the transfer of learned behavior from trained to naive rats by the injection of brain extracts, a subject on which he would publish extensively in later years. In 1970 he became field representative for the Graduate Field of Neurobiology and Behavior, and in 1971 he shared the acting chairmanship of the Section of Neurobiology and Behavior. Frank Rosenblatt died in July 1971 on his 43rd birthday, in a boating accident in Chesapeake Bay. He was eulogized on the floor of the House of Representatives, among others by former Senator Eugene McCarthy. == Academic interests == === Perceptron === Rosenblatt is best known for the Perceptron, an electronic device which was constructed in accordance with biological principles and showed an ability to learn. Rosenblatt's perceptrons were initially simulated on an IBM 704 computer at Cornell Aeronautical Laboratory in 1957. When a triangle was held before the perceptron's eye, it would pick up the image and convey it along a random succession of lines to the response units, where the image was registered. He developed and extended this approach in numerous papers and a book called Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms, published by Spartan Books in 1962. He received international recognition for the Perceptron. The New York Times billed it as a revolution, with the headline \"New Navy Device Learns By Doing\", and The New Yorker similarly admired the technological advancement. Rosenblatt proved four main theorems. The first theorem states that elementary perceptrons can solve any classification problem if there are no discrepancies in the training set (and sufficiently many independent A-elements). The fourth theorem states convergence of learning algorithm if this realisation of elementary perceptron can solve the problem. Research on comparable devices was also being conducted in other places such as SRI, and many researchers had big expectations on what they could do. The initial excitement became somewhat reduced, however, when in 1969 Marvin Minsky and Seymour Papert published the book \"Perceptrons\". Minsky and Papert considered elementary perceptrons with restrictions on the neural inputs: a bounded number of connections or a relatively small diameter of A-units receptive fields. They proved that under these constraints, an elementary perceptron cannot solve some problems, such as the connectivity of input images or the parity of pixels in them. Thus, Rosenblatt proved omnipotence of the unrestricted elementary perceptrons, whereas Minsky and Papert demonstrated that abilities of perceptrons with restrictions are limited. These results are not contradictory, but the Minsky and Papert book was widely (and wrongly) cited as the proof of strong limitations of perceptrons. (For detailed elementary discussion of the first Rosenblatt's theorem and its relation to Minsky and Papert work we refer to a recent note.) After research on neural networks returned to the mainstream in the 1980s, new researchers started to study Rosenblatt's work again. This new wave of study on neural networks is interpreted by some researchers as being a contradiction of hypotheses presented in the book Perceptrons, and a confirmation of Rosenblatt's expectations. The Mark I Perceptron, which is generally recognized as a forerunner to artificial intelligence, currently resides in the Smithsonian Institution in Washington D.C. The Mark I was able to learn, recognize letters, and solve quite complex problems. Principles of Neurodynamics (1962) The neuron model employed is a direct descendant of that originally proposed by McCulloch and Pitts. The basic philosophical approach has been heavily influenced by the theories of Hebb and Hayek and the experimental findings of Lashley. The probabilistic approach is shared with theorists such as Ashby, Uttley, Minsky, MacKay, and von Neumann. Rosenblatt's book Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms, published by Spartan Books in 1962, summarized his work on perceptrons at the time. The book was previously issued as an unclassified report No. 1196-G-8, on 1961 March 15, through the Defense Technical Information Center. The book is divided into four parts. The first gives an historical review of alternative approaches to brain modeling, the physiological and psychological considerations, and the basic definitions and concepts of the perceptron approach. The second covers three-layer series-coupled perceptrons: the mathematical underpinnings, performance results in psychological experiments, and a variety of perceptron variations. The third covers multi-layer and cross-coupled perceptrons, and the fourth back-coupled perceptrons and problems for future study. Rosenblatt used the book to teach an interdisciplinary course entitled \"Theory of Brain Mechanisms\" that drew students from Cornell's Engineering and Liberal Arts colleges. === Rat brain experiments === Around the late 1960s, inspired by James V. McConnell's experiments with memory transfer in planarians, Rosenblatt began experiments within the Cornell Department of Entomology on the transfer of learned behavior via rat brain extracts. Rats were taught discrimination tasks such as Y-maze and two-lever Skinner box. Their brains were then extracted, and the extracts and their antibodies were injected into untrained rats that were subsequently tested in the discrimination tasks to determine whether or not there was behavior transfer from the trained to the untrained rats. Rosenblatt spent his last several years on this problem and showed convincingly that the initial reports of larger effects were wrong and that any memory transfer was at most very small. == Other interests == === Astronomy === Rosenblatt also had a serious research interest in astronomy and proposed a new technique to detect the presence of stellar satellites. He built an observatory on a hilltop behind his house in Brooktondale about 6 miles east of Ithaca. When construction on the observatory was completed, Rosenblatt began an intensive study on SETI (Search for Extraterrestrial Intelligence). He also studied photometry and developed a technique for \"detecting low-level laser signals against a relatively intense background of non-coherent light\". === Politics === Rosenblatt was very active in liberal politics. He worked in the Eugene McCarthy primary campaigns for president in New Hampshire and California in 1968 and in a series of Vietnam protest activities in Washington. == IEEE Frank Rosenblatt Award == The Institute of Electrical and Electronics Engineers (IEEE), the world's largest professional association dedicated to advancing technological innovation and excellence for the benefit of humanity, presents annually a IEEE Frank Rosenblatt Award. == See also == History of artificial intelligence \xa7 Perceptrons AI Winter == References == Mason, Harding; Harding Mason, D. Stewart,Brendan Gill (November 29, 1958). \"Rival\". The New Yorker.{{cite web}}: CS1 maint: multiple names: authors list (link) An interview with Frank Rosenblatt, and Marshall C. Yovits of the Office of Naval Research.",date:"2025-01-31T15:37:19.738240Z",title:"Frank Rosenblatt",author:"",collection_name:"ML_Wikipedia",category:["1928 births","1971 deaths","20th-century American psychologists","American artificial intelligence researchers","Articles with hCards","Articles with short description","Boating accident deaths","CS1 maint: multiple names: authors list","Cornell University alumni","Cornell University faculty","Jewish American scientists","No local image but image on Wikidata","Scientists from New Rochelle, New York","Short description is different from Wikidata","The Bronx High School of Science alumni"],summary:"",chunk_spans:[[6020,6707],[7951,8759]]},{content:'Peter Norvig (born December 14, 1956) is an American computer scientist and Distinguished Education Fellow at the Stanford Institute for Human-Centered AI. He previously served as a director of research and search quality at Google. Norvig is the co-author with Stuart J. Russell of the most popular textbook in the field of AI: Artificial Intelligence: A Modern Approach used in more than 1,500 universities in 135 countries. == Early life and education == Norvig grew up in an academic family. His father was Danish and came to the United States after World War II to study math at the University of Minnesota. Norvig received a Bachelor of Science in applied mathematics from Brown University and a Ph.D. in computer science from the University of California, Berkeley. == Career and research == Norvig is a councilor of the Association for the Advancement of Artificial Intelligence and co-author, with Stuart J. Russell, of Artificial Intelligence: A Modern Approach, now the leading college text in the field. He was head of the Computational Sciences Division (now the Intelligent Systems Division) at NASA Ames Research Center, where he oversaw a staff of 200 scientists performing NASA\'s research and development in autonomy and robotics, automated software engineering and data analysis, neuroengineering, collaborative systems research, and simulation-based decision-making. Before that he was chief scientist at Junglee, where he helped develop one of the first Internet comparison-shopping services; chief designer at Harlequin Inc.; and senior scientist at Sun Microsystems Laboratories. Norvig has served as an assistant professor at the University of Southern California and as a research faculty member at Berkeley. He has over fifty publications in various areas of computer science, concentrating on artificial intelligence, natural language processing, information retrieval and software engineering, including the books Artificial Intelligence: A Modern Approach, Paradigms of AI Programming: Case Studies in Common Lisp, Verbmobil: A Translation System for Face-to-Face Dialog, and Intelligent Help Systems for UNIX. Norvig is one of the creators of JScheme. Norvig is listed under "Academic Faculty & Advisors" for the Singularity University. In 2011, Norvig worked with Sebastian Thrun to develop a popular online course in Artificial Intelligence that had more than 160,000 students enrolled. He also teaches an online course via the Udacity platform. == Selected publications and presentations == By 2022, Artificial Intelligence: A Modern Approach, which Norvig first co-authored with Stuart J. Russell in 1995, was the leading textbook in the field used by over 1400 schools globally. In 2001, Norvig published a short article titled Teach Yourself Programming in Ten Years, arguing against the fashionable introductory programming textbooks that purported to teach programming in days or weeks. The article was widely shared and discussed, and has attracted contributed translations to over 20 languages. Norvig is also known for his 2003 Gettysburg Powerpoint Presentation, a satire about bad presentation practices using Abraham Lincoln\'s famous Gettysburg Address. His 2009 IEEE Intelligent Systems article, "The Unreasonable Effectiveness of Data" co-authored with Alon Y. Halevy and Fernando Pereira, described how the best approach to highly complex natural language understanding problems is to harness large quantities of data, not to depend on "tidy", simple formulas. They said that by generating "large amounts of unlabeled, noisy data, new algorithms can be used to build high-quality models from the data. This has informed the development of foundation models. "But invariably, simple models and a lot of data trump more elaborate models based on less data.": 9 "Choose a representation that can use unsupervised learning on unlabeled data, which is so much more plentiful than labeled data.": 12 The title refers to the physicist Eugene Wigner\'s 1960 journal article, "The Unreasonable Effectiveness of Mathematics in the Natural Sciences". In a 23 September 2010 lecture presented as part of the Vancouver-based University of British Columbia\'s Department of Computer Science\'s Distinguished Lecture Series, Norvig, who was then the Director of Research at Google, described how large quantities of data deepen our understanding of phenomena. In his June 2012 Ted Talk, described the fall of 2011 hybrid class on artificial intelligence attended by 100,000 online students around the globe that he co-taught with Sebastian Thrun at Stanford University. === Awards and honors === Norvig was elected an AAAI Fellow in 2001 and a fellow of the Association for Computing Machinery in 2006. == References == == External links == The Prospects for AI, featuring Neil Jacobstein, Patrick Lincoln, Peter Norvig, and Bruno Olshausen An experiment by Norvig on Scientific opinion on climate change',date:"2025-01-31T15:44:38.845110Z",title:"Peter Norvig",author:"",collection_name:"ML_Wikipedia",category:["1956 births","2006 fellows of the Association for Computing Machinery","American artificial intelligence researchers","American computer scientists","American people of Danish descent","Articles with hCards","Articles with short description","Biography with signature","Brown University alumni","CS1 Danish-language sources (da)","Fellows of the Association for the Advancement of Artificial Intelligence","Google employees","Lisp (programming language) people","Living people","Natural language processing researchers","Python (programming language) people","Short description is different from Wikidata","University of California, Berkeley alumni","Use dmy dates from September 2022"],summary:"",chunk_spans:[[496,1601]]}],metadata:{collection_name:"ML_Wikipedia",return_type:"document",output_type:"original",query_text:"Mark Robson",query_type:"semantic",code:{language:"python",title:"Query",text:'collection.query.near_text(query="Mark Robson", limit=5)'},chunked:!0},code:{language:"python",title:"Query",text:'collection.query.near_text(query="Mark Robson", limit=5)'}}},{type:"text",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",id:"tex-92872768-3144-4b57-a440-eb3ee6d5a412",payload:{type:"response",metadata:{},objects:[{text:"I'm now expanding my search to include other collections to find more relevant information about Mark Robson."}]}},{type:"text",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",id:"tex-b992fd03-041a-429e-b98c-23b2ba10f3a3",payload:{type:"response",metadata:{},objects:[{text:"I am now systematically searching each of the remaining collections for information about Mark Robson."}]}},{type:"text",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",id:"tex-43e3d40d-68f2-487e-bd4c-f489f4bf8797",payload:{type:"response",metadata:{},objects:[{text:"I'm expanding the search to include the weaviate_blogs and weaviate_documentation collections to find more relevant information about Mark Robson."}]}},{type:"result",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",id:"res-cffc0a2a-1b8a-4b5d-a375-1ca05498ed26",payload:{type:"document",objects:[{title:"Blog Solution-to-tl-drs",date:"",content:'![The solution to TL;DRs - Weaviate\'s summarizer module](./img/hero.png)\n\n(*Note: You can skip to the TL;DR version [below](/blog/solution-to-tl-drs#bonus-tldr-version-edited)* \uD83D\uDE09)\n\nHow often do you find yourself facing a wall of text in an email, a report, or a paper, and letting out a sigh? Nobody enjoys hacking their way through boring, dense prose. Especially if it\'s just to see if the information is even relevant.\n\nIn this day and age, this is a more common problem than ever. For a while now, the bottleneck in knowledge work has been our rate of information discovery and consumption. So how do we solve this problem?\n\nYou probably already know that Weaviate, as a vector database, can help with information cataloging and discovery. But did you know that Weaviate can also summarize information during retrieval?\n\nOur summarizer module ([`sum-transformers`](/developers/weaviate/modules/sum-transformers)) can be added to a Weaviate instance to do exactly that.\n\nAnd as a bonus, we will also show you how to use our new generative module (`generative-openai`) to do the same thing as well.\n\nBy the end, you will see how you can use Weaviate to reduce the amount of TL;DRs ([too long\\; did not read](https://www.merriam-webster.com/dictionary/tldr)) in your life and in the lives of those around you.\n\n## `sum-transformers` in action\n\nSince we’re talking about reducing TL;DRs - let’s cut to the chase. The `sum-transformers` module does one thing - summarize a piece of text into shorter text. For example, it will produce a pithy, to-the-point summary like this:\n\n> *The Sydney Opera House is a multi-venue performing arts centre in Sydney, Australia. The building was designed by Danish architect J\xf8rn Utzon and opened in 1973. It is one of the most popular visitor attractions in Australia, visited by more than eight million people annually.*\n\nFrom an original text that has about 7x the length!\n\n\n  See original text\n\n*The Sydney Opera House is a multi-venue performing arts centre in Sydney. Located on the foreshore of Sydney Harbour, it is widely regarded as one of the world\'s most famous and distinctive buildings and a masterpiece of 20th-century architecture. Designed by Danish architect J\xf8rn Utzon, but completed by an Australian architectural team headed by Peter Hall, the building was formally opened by Queen Elizabeth II on 20 October 1973 after a gestation beginning with Utzon\'s 1957 selection as winner of an international design competition. The Government of New South Wales, led by the premier, Joseph Cahill, authorised work to begin in 1958 with Utzon directing construction. The government\'s decision to build Utzon\'s design is often overshadowed by circumstances that followed, including cost and scheduling overruns as well as the architect\'s ultimate resignation. The building and its surrounds occupy the whole of Bennelong Point on Sydney Harbour, between Sydney Cove and Farm Cove, adjacent to the Sydney central business district and the Royal Botanic Gardens, and near to the Sydney Harbour Bridge.*\n\n*The building comprises multiple performance venues, which together host well over 1,500 performances annually, attended by more than 1.2 million people. Performances are presented by numerous performing artists, including three resident companies: Opera Australia, the Sydney Theatre Company and the Sydney Symphony Orchestra. As one of the most popular visitor attractions in Australia, the site is visited by more than eight million people annually, and approximately 350,000 visitors take a guided tour of the building each year. The building is managed by the Sydney Opera House Trust, an agency of the New South Wales State Government.*\n\n*On 28 June 2007, the Sydney Opera House became a UNESCO World Heritage Site, having been listed on the (now defunct) Register of the National Estate since 1980, the National Trust of Australia register since 1983, the City of Sydney Heritage Inventory since 2000, the New South Wales State Heritage Register since 2003, and the Australian National Heritage List since 2005. The Opera House was also a finalist in the New7Wonders of the World campaign list.*\n\n\n\nHere are other examples, where the module produced summaries of biographical, mythical, and technical information:\n\n\n  \n    Lewis Hamilton (80% reduction)\n  \n\nSir Lewis Carl Davidson Hamilton (born 7 January 1985) is a British racing driver. In Formula One, Hamilton has won a joint-record seven World Drivers\' Championship titles (tied with Michael Schumacher), and holds the records for the most wins (103), pole positions (103) and podium finishes (191) Hamilton joined the McLaren young driver programme in 1998 at the age of 13, becoming the youngest racing driver ever to be contracted by a Formula One team. After six years with McLaren, Hamilton signed with Mercedes in 2013.\n\n#### Original text\n\nSir Lewis Carl Davidson Hamilton   (born 7 January 1985) is a British racing driver currently competing in Formula One, driving for Mercedes-AMG Petronas Formula One Team. In Formula One, Hamilton has won a joint-record seven World Drivers\' Championship titles (tied with Michael Schumacher), and holds the records for the most wins (103), pole positions (103), and podium finishes (191), among others.\n\nBorn and raised in Stevenage, Hertfordshire, Hamilton joined the McLaren young driver programme in 1998 at the age of 13, becoming the youngest racing driver ever to be contracted by a Formula One team. This led to a Formula One drive with McLaren for six years from 2007 to 2012, making Hamilton the first black driver to race in the series. In his inaugural season, Hamilton set numerous records as he finished runner-up to Kimi R\xe4ikk\xf6nen by one point. The following season, he won his maiden title in dramatic fashion—making a crucial overtake at the last corner on the last lap of the last race of the season—to become the then-youngest Formula One World Champion in history.  After six years with McLaren, Hamilton signed with Mercedes in 2013.\n\nChanges to the regulations for 2014 mandating the use of turbo-hybrid engines saw the start of a highly successful period for Hamilton, during which he won six further drivers\' titles. Consecutive titles came in 2014 and 2015 during an intense rivalry with teammate Nico Rosberg. Following Rosberg\'s retirement in 2016, Ferrari\'s Sebastian Vettel became Hamilton\'s closest rival in two championship battles, in which Hamilton twice overturned mid-season point deficits to claim consecutive titles again in 2017 and 2018. His third and fourth consecutive titles followed in 2019 and 2020 to equal Schumacher\'s record of seven drivers\' titles. Hamilton achieved his 100th pole position and race win during the 2021 season.\n\nHamilton has been credited with furthering Formula One\'s global following by appealing to a broader audience outside the sport, in part due to his high-profile lifestyle, environmental and social activism, and exploits in music and fashion. He has also become a prominent advocate in support of activism to combat racism and push for increased diversity in motorsport. Hamilton was the highest-paid Formula One driver from 2013 to 2021, and was ranked as one of the world\'s highest-paid athletes by Forbes of twenty-tens decade and 2021. He was also listed in the 2020 issue of Time as one of the 100 most influential people globally, and was knighted in the 2021 New Year Honours. Hamilton was granted honorary Brazilian citizenship in 2022.\n\n\n\n\n\n  \n    The Loch Ness Monster (52% reduction)\n  \n\nThe Loch Ness Monster is said to be a large, long-necked creature. Popular belief in the creature has varied since it was brought to worldwide attention in 1933. Evidence of its existence is disputed, with a number of disputed photographs and sonar readings. The pseudoscience and subculture of cryptozoology has placed particular emphasis on the creature.\n\n#### Original text\n\nThe Loch Ness Monster (Scottish Gaelic: Uilebheist Loch Nis), affectionately known as Nessie, is a creature in Scottish folklore that is said to inhabit Loch Ness in the Scottish Highlands. It is often described as large, long-necked, and with one or more humps protruding from the water. Popular interest and belief in the creature has varied since it was brought to worldwide attention in 1933. Evidence of its existence is anecdotal, with a number of disputed photographs and sonar readings.\n\nThe scientific community explains alleged sightings of the Loch Ness Monster as hoaxes, wishful thinking, and the misidentification of mundane objects. The pseudoscience and subculture of cryptozoology has placed particular emphasis on the creature.\n\n\n\n\n\n  \n    Bitmap Indexes (79% reduction)\n  \n\nA bitmap index is a special kind of database index that uses bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional B-tree indexes for columns whose data is frequently updated.\n\n#### Original text\n\nBitmap indexes have traditionally been considered to work well for low-cardinality columns, which have a modest number of distinct values, either absolutely, or relative to the number of records that contain the data. The extreme case of low cardinality is Boolean data (e.g., does a resident in a city have internet access?), which has two values, True and False. Bitmap indexes use bit arrays (commonly called bitmaps) and answer queries by performing bitwise logical operations on these bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional B-tree indexes for columns whose data is frequently updated: consequently, they are more often employed in read-only systems that are specialized for fast query - e.g., data warehouses, and generally unsuitable for online transaction processing applications.\n\nSome researchers argue that bitmap indexes are also useful for moderate or even high-cardinality data (e.g., unique-valued data) which is accessed in a read-only manner, and queries access multiple bitmap-indexed columns using the AND, OR or XOR operators extensively. Bitmap indexes are also useful in data warehousing applications for joining a large fact table to smaller dimension tables such as those arranged in a star schema.\n\n\n\nIf you examine these summaries, you will notice that these sentences are not lifted verbatim from the original text. Instead, what is produced is an *abstractive* summary, which is newly produced based on the original text.\n\nThe summarization module achieves this at query time by passing on the text that is retrieved from Weaviate to a language model that is trained specifically for summarization.\n\nThis means that you can set up your Weaviate instance to not only retrieve the most relevant query results for you but take the next step and add an overview of each retrieved object.\n\n![Summarized documents are easier to understand](./img/weaviate-summarize.png#gh-light-mode-only)\n![Summarized documents are easier to understand](./img/weaviate-summarize-dark.png#gh-dark-mode-only)\n\nSo, in a parallel fashion to the generative module, you can get back the information stored in Weaviate and *then some*.\n\nInstead of this:\n\n```json\n{\n  "data": {\n    "Get": {\n      "Article": [\n        {\n          "title": "Sydney Opera House",\n          "url": "https://en.wikipedia.org/wiki/Sydney_Opera_House",\n          "wiki_summary": ...\n        },\n        ...\n      ]\n    }\n  }\n}\n```\n\nYou can get back:\n\n```json\n{\n  "data": {\n    "Get": {\n      "Article": [\n        {\n          "_additional": {\n            "summary": [\n              {\n                "property": "wiki_summary",\n                "result": ""\n              }\n            ]\n          },\n          "title": "Sydney Opera House",\n          "url": "https://en.wikipedia.org/wiki/Sydney_Opera_House",\n          "wiki_summary": ...\n        },\n        ...\n      ]\n    }\n  }\n}\n```\n\nWhere the `` is actually not something that is stored in Weaviate!\n\nHere is how:\n\n## Using `sum-transformers`\n\nThe required steps to make use of this module are as follows:\n\n1. Enable the module in your Docker Compose file (e.g. `docker-compose.yml`).\n1. Add this to your query:\n\n    ```graphql\n    _additional { summary ( properties: [""]) { property result } }\n    ```\n\n1. Parse the results!\n\nThat\'s all there is to it. Let\'s take a look at each step in more detail.\n\n### Configuration (`docker-compose.yml`) file\n\nUse the `sum-transformers` module in addition to another vectorizer module, such as `text2vec-transformers`, or an inference-API-based model such as `text2vec-openai/cohere/huggingface`.\n\nAccordingly, the relevant lines are:\n\n```yaml\n---\nservices:\n  weaviate:\n    ...\n    environment:\n      ...\n      ENABLE_MODULES: \'text2vec-contextionary,sum-transformers\'\n  ...\n  sum-transformers:\n    image: cr.weaviate.io/semitechnologies/sum-transformers:facebook-bart-large-cnn-1.0.0\n    # image: cr.weaviate.io/semitechnologies/sum-transformers:google-pegasus-xsum-1.2.0  # Alternative model\n    environment:\n      ENABLE_CUDA: \'0\'\n...\n```\n\nNotice that the `ENABLE_MODULES` variable includes `sum-transformers`, and there is a `sum-transformers` section that specifies the Docker image to be used, specifies the summarizer model, and whether to use CUDA (GPU) acceleration.\n\n\n  Example of an entire configuration yaml\n\n```yaml\n---\nservices:\n  weaviate:\n    command:\n    - --host\n    - 0.0.0.0\n    - --port\n    - \'8080\'\n    - --scheme\n    - http\n    image: cr.weaviate.io/semitechnologies/weaviate:1.17.4\n    ports:\n    - 8080:8080\n    restart: on-failure:0\n    environment:\n      CONTEXTIONARY_URL: contextionary:9999\n      SUM_INFERENCE_API: \'http://sum-transformers:8080\'\n      QUERY_DEFAULTS_LIMIT: 25\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: \'true\'\n      PERSISTENCE_DATA_PATH: \'/var/lib/weaviate\'\n      DEFAULT_VECTORIZER_MODULE: \'text2vec-contextionary\'\n      ENABLE_MODULES: \'text2vec-contextionary,sum-transformers\'\n      CLUSTER_HOSTNAME: \'node1\'\n  contextionary:\n    environment:\n      OCCURRENCE_WEIGHT_LINEAR_FACTOR: 0.75\n      EXTENSIONS_STORAGE_MODE: weaviate\n      EXTENSIONS_STORAGE_ORIGIN: http://weaviate:8080\n      NEIGHBOR_OCCURRENCE_IGNORE_PERCENTILE: 5\n      ENABLE_COMPOUND_SPLITTING: \'false\'\n    image: cr.weaviate.io/semitechnologies/contextionary:en0.16.0-v1.0.2\n    ports:\n    - 9999:9999\n  sum-transformers:\n    image: cr.weaviate.io/semitechnologies/sum-transformers:facebook-bart-large-cnn-1.0.0\n    # image: cr.weaviate.io/semitechnologies/sum-transformers:google-pegasus-xsum-1.2.0  # Alternative model\n    environment:\n      ENABLE_CUDA: \'0\'\n...\n```\n\n\n\nWith these variables specified, an instance of Weaviate can be spun up with the summarizer module enabled. Next, it\'s a matter of making queries:\n\nAt the time of writing, Weaviate Cloud (WCD) instances do not support the `sum-transformers` module.\n\nHowever, you can perform summarizations with the `generative-openai` module, by providing a specific prompt. Take a look below \uD83D\uDE09.\n\n### Results with summaries\n\nSummaries are available at query time. In other words, they are not pre-determined but generated from the results retrieved by the `sum-transformers` model.\n\nThis can be triggered as an `additional` property, with a GraphQL query syntax like so:\n\n```graphql\n{\n  Get {\n     {\n      _additional {\n        summary (\n          properties: [""],\n        ) {\n          property\n          result\n        }\n      }\n    }\n  }\n}\n```\n\nWhere the `_additional { summary ... }` instructs Weaviate to carry out the summarization.\n\nIn this syntax, `properties` specifies the fields to be summarized, where each summary will include a `property` that echoes the field name, and `result` is the summarized text output.\n\nThus, it will produce a result like:\n\n```json\n{\n  "data": {\n    "Get": {\n      "Article": [\n        {\n          "_additional": {\n            "summary": [\n              {\n                "property": "wiki_summary",\n                "result": "The Loch Ness Monster is said to be a large, long-necked creature. Popular belief in the creature has varied since it was brought to worldwide attention in 1933. Evidence of its existence is disputed, with a number of disputed photographs and sonar readings. The pseudoscience and subculture of cryptozoology has placed particular emphasis on the creature."\n              }\n            ]\n          },\n          "title": "Loch Ness Monster"\n        },\n      ]\n    }\n  }\n}\n```\n\nNote that in this example, `Article` was the class name, `wiki_summary` was the input text and the query included the `title` field also.\n\n\n  See the full GraphQL query\n\n```graphql\n{\n  Get {\n    Article (\n      limit: 1\n      where: {\n        path: ["title"],\n        operator: Like,\n        valueText: "loch ness"\n      }\n      ) {\n      title\n      _additional {\n        summary (\n          properties: ["wiki_summary"],\n        ) {\n          property\n          result\n        }\n      }\n    }\n  }\n}\n```\n\n\n\nGrabbing these results is relatively straightforward. But as with many tasks that involve language models, there are a few tips and tricks to help you get the most out of this wonderful tool.\n\n### Alternative: `generative-openai`\n\nYou might have heard of our new [`generative-openai` module](/developers/weaviate/model-providers/openai/generative) already. It can take your results, and provide a prompt to a generative model to get back a response.\n\nSo, this module can also be used to generate summaries by prompting it directly to do so. Below is one such example:\n\n```graphql\n{\n  Get {\n    Article(\n      nearText: {\n        concepts: ["Bitmap Index"]\n      }\n      limit: 1\n    ) {\n      title\n      wiki_summary\n      _additional {\n        generate(\n          singleResult: {\n            prompt: """\n              Describe the following as a short summary: {wiki_summary}\n            """\n          }\n        ) {\n          singleResult\n          error\n        }\n      }\n    }\n  }\n}\n```\n\nThis produced:\n\n> A bitmap index is a special kind of database index that uses bitmaps to efficiently store and query low-cardinality data. It is often used in read-only systems such as data warehouses, and is less efficient than traditional B-tree indexes for frequently updated data. Some researchers argue that bitmap indexes can also be used for moderate or high-cardinality data, and are useful for joining large fact tables to smaller dimension tables in data warehousing applications.\n\nFrom the original text below:\n\n\n  See original text\n\n*A bitmap index is a special kind of database index that uses bitmaps.*\n\n*Bitmap indexes have traditionally been considered to work well for low-cardinality columns, which have a modest number of distinct values, either absolutely, or relative to the number of records that contain the data. The extreme case of low cardinality is Boolean data (e.g., does a resident in a city have internet access?), which has two values, True and False. Bitmap indexes use bit arrays (commonly called bitmaps) and answer queries by performing bitwise logical operations on these bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional B-tree indexes for columns whose data is frequently updated: consequently, they are more often employed in read-only systems that are specialized for fast query - e.g., data warehouses, and generally unsuitable for online transaction processing applications.*\n\n*Some researchers argue that bitmap indexes are also useful for moderate or even high-cardinality data (e.g., unique-valued data) which is accessed in a read-only manner, and queries access multiple bitmap-indexed columns using the AND, OR or XOR operators extensively. Bitmap indexes are also useful in data warehousing applications for joining a large fact table to smaller dimension tables such as those arranged in a star schema.*\n\n\n\nWhile this is not a custom-trained summarization model, this may be a great alternative solution where `sum-transformers` module cannot be used, or where you may wish to leverage flexible, large language models that are only available through APIs.\n\n## Best practice notes\n\n### GPU usage\n\nThe `sum-transformers` module is configured to spin up a Docker container on your system to carry out the inference (i.e. summarization) task. While inference tasks are far less resource intensive than training, they are still non-trivial.\n\nIt will run much faster on systems that support GPU acceleration with CUDA. CPUs may be fine for occasional or private, evaluation use, but for any kind of deployment, we highly recommend utilizing GPU acceleration.\n\n### Model choice\n\nCurrently, the `sum-transformers` module uses the `bart-large-cnn` model under the hood by default, with an option for the `pegasus-xsum` model. Both of these are well-known, high-performance models trained by Facebook and Google respectively.\n\nIn addition to these two models, however, you can use any model from the Hugging Face Hub (or your own) by following [this guide](/developers/weaviate/modules/sum-transformers#use-another-summarization-module-from-hugging-face).\n\nEven when looking only at language models that are trained for summarization tasks, there is still a wide range of choices in terms of sheer numbers, which vary in the target domain (e.g. medical, legal, scientific, etc.) and size (number of parameters, i.e. speed). If you have specific needs, we recommend investigating other models.\n\n### Avoid too long an input\n\nAll transformer models have a maximum input length size. For example, `bart-large-cnn` has a maximum limit of 1024 tokens, where each token is part of a word (i.e. a few characters).\n\nInputting too long a text into the summarizer module will cause it to throw an error. Accordingly, if you envisage generating summaries using `sum-transformers`, we recommend considering chunking your data at import time to make sure that the content of each field is shorter than the maximum length.\n\n### But also... not too short\n\nThe [Goldilocks Principle](https://en.wikipedia.org/wiki/Goldilocks_principle) also applies here, where too short an input may cause the summarizer model to misbehave (i.e. [hallucinate](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence))).\n\nWhen fed insufficient input, the model may "pad" the output with tokens that have very little grounding in the truth. Generally speaking, an input length that is shorter than a typical "summary" output length by the model is inadvisable.\n\n## Do more with less\n\nAll in all, summarizing your data with Weaviate can be another valuable tool in your toolkit to reduce TL;DRs in our lives. By combining the power of vector search with summarization capabilities, Weaviate offers a practical, easy solution to the problem of information overload.\n\nSo instead of slashing your way through thickets of prose to look for the information you need, work smarter! Whether it\'s with `generate-openai` or `sum-transformers` - let Weaviate lift you up to give you a bird\'s eye view of the whole landscape.\n\n## Bonus: TL;DR version (edited):\n\nWeaviate can also summarize information during retrieval through the use of its summarizer module, `sum-transformers`.\n\nThis module can shorten a piece of text into a pithy, to-the-point summary by passing the text retrieved from Weaviate to a language model trained specifically for summarization.\n\nBy using Weaviate to summarize your data, you can reduce the amount of too long; did not read (TL;DR) content in your life and reduce the problem of information overload. The `sum-transformers` module uses the `bart-large-cnn` model by default, with an option for the `pegasus-xsum` model, but any model from Hugging Face Hub can be used.\n\n\n',author:"",category:["Blog"],summary:"",chunk_spans:[[5466,6198]]},{title:"Blog Dspy-optimizers",date:"",content:'\nLanguage models have revolutionized the way we interact with and utilize artificial intelligence. These powerful tools can be prompted to perform a wide array of tasks, from creative writing to answering complex questions, and even assisting with decision making. However, the quality and effectiveness of the generated output is heavily dependent on the particular language in the prompt to the language model.\n\nCrafting the perfect prompt, also known as Prompt Engineering, is generally more of an art than a science. AI developers typically have unique styles of prompting large language models (LLMs) through trial-and-error experiences. With the inception of DSPy, prompting is returning to its roots in Machine Learning, allowing us to better control generative AI systems.\n\nTo achieve this, DSPy introduces two new concepts: a programming model and a collection of optimizers or compilers. The DSPy programming model lets you define each component that makes a language model request, such as an initial task description and input and output fields. You can then connect these components to build complex pipelines. For example, if you’d like to convert questions into blog posts, the simplest pipeline consists of two components: 1. Question to Blog Outline, and 2. Outline to Paragraphs. The `Question2BlogOutline` takes in two inputs, question and contexts, and outputs a comma-separated list of topics. The `Topic2Paragraph` takes the topic and contexts as input, and outputs the paragraph. We then piece these components (or signatures) together in the program (see this [notebook](https://github.com/weaviate/recipes/blob/main/integrations/llm-frameworks/dspy/2.Writing-Blog-Posts-with-DSPy.ipynb) for the full example). This lets you interface this program with the DSPy compilers.\n\nCompilers enter the picture to give your LLMs “the prompts they deserve.” This blog post will help you gain a deep understanding of the current state of Prompt Engineering and how DSPy’s BootstrapFewShot, COPRO, and MIPRO optimizers can help you.\n\nYou can follow along with the demo notebook [here](https://github.com/weaviate/recipes/blob/main/integrations/llm-frameworks/dspy/5.Advanced-Optimizers.ipynb)! The notebook uses tools from Weaviate, Cohere, OpenAI, Arize, and Weights & Biases.\n\n## Examples and Instructions\nThe role of optimization in machine learning systems has been dramatically transformed with the introduction of LLMs. LLMs no longer **require** gradient descent training to increase their performance for a specific task. Instead, we can tweak the language of the prompt which is presented to the LLM. There are 2 main aspects of the prompt to tweak, instructions and examples.\n\nInstructions refer to the task description typically placed at the beginning of the prompt. Different ways of phrasing the task will result in better or worse performance for a given task and/or language model. For example, the prompt "Write a paragraph to answer this question" will perform differently than the prompt: "Analyze the given question, identify its key components, and compose a well-structured, coherent paragraph that thoroughly addresses the main points.”\n\nFurther, LLM performance may be improved by providing examples of inputs and outputs. Adding these examples to the prompt is known as few-shot learning. Few-shot learning has even had successes described in the original release of GPT-3, before the explosion of instruction tuning with Reinforcement Learning from Human Feedback (RLHF).\n\nBoth of these strategies are fairly intuitive. A detailed instruction for the task you would like completed will certainly help communicate it, and what better way to demonstrate the task then with a few examples?\n\nThis article will explain how DSPy tunes instructions and creates high-quality examples to help you get higher quality responses from LLMs.\n\n\n  RAG Prompt Example\n\n```text\nAssess the context and answer the question.\n\n---\n\nFollow the following format.\n\nContexts: May contain helpful information\nQuestion:\nAnswer:\n\n---\n\nContexts: Weaviate 1.24 is here!\nHere are the release highlights!\n\n- Named vectors: A single object can have multiple vectors. Create vectors for properties, use different vectorization models, and apply different metrics to fine tune interactions with your data.\n- HNSW and binary quantization (BQ): HNSW indexes and BQ combine for serious compression and blazing speed.\n- Simplified Docker configuration: A new Docker image that needs no configuration.\n- Backend improvements: Numerous improvements to make updates, imports, and deletions faster.\n- Python client update: General availability, support for all the latest features.\n\nQuestion: Does Weaviate support Binary Quantization?\nAnswer:\n```\n\n\n## Setting up our Pipeline for Optimization\nThis blog post will illustrate how the BootstrapFewShot, COPRO, and MIPRO optimizers work through the lens of a RAG example. BootstrapFewShot optimizes **Examples**, COPRO optimizes **Instructions**, and MIPRO optimizes both **Instructions and Examples**.\n\nThere are 4 steps before we can optimize our program:\n\n1. Connect DSPy to the LLMs we want to use, as well as external tools such as vector databases like Weaviate, and Arize Phoenix for logging and tracing.\n\n2. Define our LLM Pipeline using DSPy’s Signatures and Modules.\n\n3. Wrap examples of your dataset in dspy.Example objects.\n\n4. Define your metric to judge responses.\n\n### Connect DSPy to our LLMs and Tools\nWe’ll begin with configuring the language model, connecting to our Weaviate cluster, and launching Phoenix:\n\n```python\nimport dspy\nfrom dspy.retrieve.weaviate_rm import WeaviateRM\nimport weaviate\nimport phoenix as px\n\ncommand_r = dspy.Cohere(model="command-r", max_tokens=4000, api_key=cohere_api_key)\ncommand_r_plus = dspy.Cohere(model="command-r-plus", max_tokens=4000, api_key=cohere_api_key)\ngpt4 = dspy.OpenAI(model="gpt-4", max_tokens=4000)\n\nweaviate_client = weaviate.connect_to_local()\nretriever_model = WeaviateRM("WeaviateBlogChunk", weaviate_client=weaviate_client)\ndspy.settings.configure(lm=command_r, rm=retriever_model)\n\n\nphoenix_sessin = px.launch_app()\n# ... more setup omitted in blog post, please see the full notebook.\n```\n\n### Define our LLM Pipeline (RAG)\nAs a primer to the DSPy programming model if you are unfamiliar, we have a 1-layer DSPy pipeline consisting of a retrieval and a Signature to implement `GenerateAnswer`. This LLM pipeline is commonly referred to as Retrieval-Augmented Generation (RAG). The RAG pipeline is shown below:\n\n```python\nclass GenerateAnswer(dspy.Signature):\n    """Assess the context and answer the question."""\n    \n    context = dspy.InputField()\n    question = dspy.InputField()\n    answer = dspy.OutputField()\n\nclass RAG(dspy.Module):\n    def __init__(self, passages_per_hop=3, max_hops=2):\n        super().__init__()\n        \n        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n    \n    def forward(self, question):\n        context = self.retrieve(question).passages\n        with dspy.context(lm=command_r):\n            pred = self.generate_answer(context=context, question=question).answer\n        return dspy.Prediction(context=context, answer=pred, question=question)\n```\n\n### Wrap examples of your dataset in `dspy.Example` objects\nNow that we have our DSPy program, we can work on optimizing the language model that answers the question. To optimize your program, you will need a training dataset, and a performance metric as the input. For this demo, we’re using a dataset consisting of Weaviate questions and answers. In total, we have 50 `query`, and `gold_answer` pairs.\n\n```python\nimport json\n\nfile_path = \'./WeaviateBlogRAG-0-0-0.json\'\nwith open(file_path, \'r\') as file:\n    dataset = json.load(file)\n\ngold_answers = []\nqueries = []\n\nfor row in dataset:\n    gold_answers.append(row["gold_answer"])\n    queries.append(row["query"])\n    \ndata = []\n\nfor i in range(len(gold_answers)):\n    data.append(dspy.Example(gold_answer=gold_answers[i], question=queries[i]).with_inputs("question"))\n\ntrainset, devset, testset = data[:25], data[25:35], data[35:]\n```\n\n### Define your metric to judge responses\nAdditionally, we will need the LLM metric that assesses the generated answer, as well as its alignment to the gold answer in the uncompiled RAG program. The LLM metric is:\n\n```python\nclass TypedEvaluator(dspy.Signature):\n    """Evaluate the quality of a system\'s answer to a question according to a given criterion."""\n    \n    criterion: str = dspy.InputField(desc="The evaluation criterion.")\n    question: str = dspy.InputField(desc="The question asked to the system.")\n    ground_truth_answer: str = dspy.InputField(desc="An expert written Ground Truth Answer to the question.")\n    predicted_answer: str = dspy.InputField(desc="The system\'s answer to the question.")\n    rating: float = dspy.OutputField(desc="A float rating between 1 and 5")\n\n\ndef MetricWrapper(gold, pred, trace=None):\n		alignment_criterion = "How aligned is the predicted_answer with the ground_truth?"\n		return dspy.TypedPredictor(TypedEvaluator)(criterion=alignment_criterion,\n            question=gold.question,\n			ground_truth_answer=gold.answer,\n			predicted_answer=pred.answer\n        )\n```\n\n## BootstrapFewShot\nCollecting a dataset with examples of the task you want your LLM pipeline to perform can be very time consuming to annotate. If you want to add Chain-of-Thought reasoning to each LLM call, well now you have to manually write the rationales that lead to the desired outputs **for each example in your dataset.** This tutorial is only concerned with 1-layer DSPy pipelines, but imagine you have 5 or more intermediate calls in the LLM pipeline. This means you would have to write 5 or more examples for each of these components (and their respective rationales if using Chain-of-Thought).\n\nDSPy to the rescue with the `BootstrapFewShot` compiler. `BootstrapFewShot` generates traces through the program and passes the outputs into a metric to see if the example is worth keeping as an input-output example. This metric can be either boolean valued (true or false) or numeric valued paired with a `metric_threshold` to check if the example is worth keeping. In fact, the training dataset used in this demo was created using this compiler.\n\nThe following code snippet illustrates how to pass in our RAG program and our LLM_Metric into the `BootstrapFewShot` compiler. We are further looping through values of `max_bootstrapped_demos`. If set to 1, `BootstrapFewShot` will produce 1 input-output example to use in the prompt. Similarly, if set to 3, `BootstrapFewShot` will use 3 input-output examples to use in the prompt. This loop can help you get a quick sense if adding more examples will improve performance.\n\n```python\nfrom dspy.teleprompt import BootstrapFewShot\n\nfor i in range(1, 4, 1):\n    teleprompter = BootstrapFewShot(metric=MetricWrapper, max_bootstrapped_demos=i, max_rounds=1)\n    compiled_RAG = teleprompter.compile(RAG(), trainset=trainset)\n    compiled_RAG_score = evaluate(compiled_RAG, metric=MetricWrapper)\n    print(f"\\n\\033[91mCompiled RAG Score at Demos = {i}: {compiled_RAG_score}\\n")\n```\n\nIn [PR #849](https://github.com/stanfordnlp/dspy/pull/849), we introduced Weights & Biases logging to `BootstrapFewShot`! This can be used for monitoring runs to BootstrapFewShot in order to see the `metric_val` returned for each bootstrapped example. To motivate the use case, you may have a rating on a scale of 1 to 5 for answers and you only want to use examples that achieve a 5 in your prompt. Further, you can visualize how each teacher model is doing with bootstrapping, as shown below. Please see this [notebook](https://github.com/weaviate/recipes/blob/main/integrations/operations/weights_and_biases/wandb_logging_RAG_dspy_cohere.ipynb) for more details!\n\n![W&B Dashboard](img/RAG-optimization-dashboard.png)\n\n### Adding Random Search or Optuna to BootstrapFewShot\n`BootstrapFewShot` will stop searching for examples once it finds the `K` desired number of examples for the prompt; however, these may not be the optimal `K`.\n\nOne solution to this is to use DSPy’s `BootstrapFewShotWithRandomSearch` or `BootstrapFewShotWithOptuna`. Instead of generating the desired K and stopping, these optimizers will generate `num_candidate` examples and use either random search or bayesian optimization, respectively, to find the optimal set of `K` examples to use in the prompt. \n\nRandom search and Bayesian optimization are two common techniques for hyperparameter tuning. Random search is used to explore the space of possible example combinations by randomly selecting `K` examples from the generated `num_candidate` examples and evaluating their performance, ultimately finding the optimal set of examples for the prompt. `BootstrapFewShotWithOptuna` employs Bayesian optimization, which builds a probabilistic model of the relationship between the selected examples and the prompt\'s performance, guiding the search for the optimal set of `K` examples by balancing exploration and exploitation based on the model\'s predictions.\n\nWe began by testing `max_bootstrapped_demos=2`, `num_candidate_programs=2` and then `max_bootstrapped_demos=1`, `num_candidate_programs=5`. The `max_bootstrapped_demos` parameter controls for the maximum number of examples used in the prompt (these two extensions of `BootstrapFewShot` will also check the metric of less demos, for example if `max_bootstrapped_demos=2`, it will check the performance of just using 1 example as well). The `num_candidate_programs` parameter will control how many configurations are added to the search space.\n\n```python\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\nteleprompter = BootstrapFewShotWithRandomSearch(metric=MetricWrapper,\n			    max_bootstrapped_demos=2,\n				num_candidate_programs=2)\n\ncompiled_RAG = teleprompmter.compile(RAG(), trainset=trainset)\n```\n\nMIPRO will also use this mechanism in tandem with instruction optimization, but first we will begin by covering COPRO instruction optimization and then bring these concepts together (and introduce 1 more new concept) to understand MIPRO.\n\n### Instruction Optimization\nIn `BootstrapFewShot`, we use an LLM to generate examples of our program, for instruction optimization we similarly use an LLM to write better instructions. DSPy has two optimizers to achieve this: COPRO and MIPRO. Both of which use the `BasicGenerateInstruction` Signature as a core building block:\n\n```python\nclass BasicGenerateInstruction(Signature):\n    """You are an instruction optimizer for large language models. I will give you a ``signature`` of fields (inputs and outputs) in English. Your task is to propose an instruction that will lead a good language model to perform the task well. Don\'t be afraid to be creative."""\n\n    basic_instruction = dspy.InputField(desc="The initial instructions before optimization")\n    proposed_instruction = dspy.OutputField(desc="The improved instructions for the language model")\n    proposed_prefix_for_output_field = dspy.OutputField(\n        desc="The string at the end of the prompt, which will help the model start solving the task",\n    )\n```\n\nCOPRO begins by generating `breadth` instruction candidates with `BasicGenerateInstruction`. These candidates are then evaluated by running each candidate instruction through the trainset with the provided metric. These results are then formatted into `GenerateInstructionGivenAttempts` to generate new instructions. This continues for `depth` rounds to produce the optimized instruction.\n\n```python\nclass GenerateInstructionGivenAttempts(dspy.Signature):\n    """You are an instruction optimizer for large language models. I will give some task instructions I\'ve tried, along with their corresponding validation scores. The instructions are arranged in increasing order based on their scores, where higher scores indicate better quality.\n\n    Your task is to propose a new instruction that will lead a good language model to perform the task even better. Don\'t be afraid to be creative."""\n\n    attempted_instructions = dspy.InputField(format=dsp.passages2text)\n    proposed_instruction = dspy.OutputField(desc="The improved instructions for the language model")\n    proposed_prefix_for_output_field = dspy.OutputField(\n        desc="The string at the end of the prompt, which will help the model start solving the task",\n    )\n```\n\nHere are the results we found in the first round of running COPRO (Note: COPRO runs for `depth` rounds). The first instruction is, “Assess the context and answer the question”, COPRO then generates 4 additional phrasings of the task. COPRO then takes each instruction and its corresponding metric value as input to propose a new instruction.\n\n| Instruction | Metric Value |\n|-------------|--------------|\n| Assess the context and answer the question. | 4.5 |\n| Carefully read the provided context, ensuring you understand the nuances and main points. Then, answer the question posed, drawing on the relevant information from the context, and provide a clear, concise, and accurate response that directly addresses the question. If the question is open-ended or subjective, offer a well-reasoned answer supported by the information given. | 4.17 |\n| Read the provided context carefully to understand the nuances and details. Then, thoughtfully answer the question posed, ensuring your response is directly relevant to the context given. Make sure your answer is clear, concise, and logically derived from the information provided. | 4.5 |\n| Read the provided context carefully to fully understand the details and nuances. Then, answer the question based on your understanding of the context in a clear, concise, and accurate manner. Ensure your response is well-reasoned and directly addresses the question, using relevant information from the context. | 4.5 |\n| Read the provided context carefully, ensuring you fully understand the details and nuances. Then, answer the question accurately and concisely, providing any necessary explanation to support your answer. Make sure your response is directly relevant to the question asked and is based on the information from the context. If the context does not contain enough information to answer the question definitively, state this explicitly and give the best possible answer based on the available information, noting any assumptions you make. | 4.0 |\n\nThese are the 5 outputs from our LLM paraphrasing the initial instructions to write a new and better instruction. The next step of COPRO takes the 5 outputs, their resulting performance according to the metric, and writes a new instruction.\n\nOutput of `GenerateInstructionGivenAttempts`: “Delve into the provided context with attention to detail to grasp the subtleties and core elements. Subsequently, craft a response to the question that is both pertinent and substantiated by the context. Your answer should be articulate, succinct, and precise, effectively capturing the essence of the question. In instances where the information at hand is insufficient to provide a definitive response, acknowledge this limitation, and present the most plausible answer, highlighting any presuppositions. Strive to ensure that your reasoning is transparent and that your answer adds value to the discussion by being insightful.” This instruction achieves an improved score of 4.83!\n\nAnother interesting work on prompting is [“The Unreasonable Effectiveness of Eccentric Automatic Prompts”](https://arxiv.org/abs/2402.10949) from Battle and Gollapudi. Their paper demonstrates that the highest scoring, automatically generated prompts can be remarkably different from what a human would devise. For example, the optimized prompt for Llama2-70B that expresses an affinity for Star Trek led to improved mathematical reasoning performance. This suggests that DSPy compilers should be open to exploring a wide range of prompt variations, even those that may seem counterintuitive to humans.\n\n## Example and Instruction Optimization\n[MIPRO](https://twitter.com/kristahopsalong/status/1766166198079889737) combines both example **optimization** and **instruction tuning**. By generating one or multiple examples for a candidate instruction, we get a better sense of how well it performs. Further, MIPRO introduces a dataset summarizer to bring in more information about the particular task and dataset you are optimizing for.\n\nMIPRO begins by calling `_observe_data` to generate observations about the dataset. MIPRO uses the `DatasetDescriptor` Signature to generate initial observations based on the first `view_data_batch_size examples`. Then, MIPRO iteratively updates the observations using the `DatasetDescriptorWithPriorObservations` Signature for the remaining examples in the dataset, processed in batches of size `view-data_batch_size`. The final observations are then summarized with the `ObservationSummarizer`. This is very similar to the research around [summarization chains](https://python.langchain.com/docs/modules/chains/).\n\nMIPRO then generates `num_candidates` sets of input-output examples for each predictor in the program using `BootstrapFewShot`, these are stored in the `demo_candidates` dictionary. MIPRO then uses `BasicGenerateInstructionsWithExamplesAndDataObservations` to generate new task descriptions. Dataset observations and bootstrapped examples can also be toggled on or off with the `view_data` and `view_examples` flags. If `view_data` is turned off, MIPRO will use `BasicGenerateInstructionsWithExamples`. If `view_examples` is turned off, MIPRO will use `BasicGenerateInstructionsWithDataObservations`. If both are turned off, MIPRO will just use the `BasicGenerateInstruction` Signature to generate new instructions. MIPRO will then find the optimal combination of instruction and the few-shot example candidate set with the Optuna Bayesian Optimization library. This is guided by the metric provided to MIPRO.\n\nHere is the result of the dataset summarizer on the Weaviate RAG dataset:\n```text\nThe dataset is tailored for an audience with technical expertise in Weaviate and related technologies, focusing on providing practical, step-by-step troubleshooting and optimization guidance. It is designed to train AI for technical support, offering concise, actionable instructions and contextual explanations, which could also serve educational purposes for users seeking to enhance their understanding of these technologies.\n```\n\nAfter running MIPRO, here is the final RAG prompt:\n```text\nCarefully read the provided context, which may contain technical information, links, and code snippets related to advanced technology and machine learning topics. Your task is to interpret this information and provide a clear, concise, and accurate answer to the question posed. Use domain-specific language appropriate for an audience knowledgeable in computational linguistics or AI technologies. Ensure that your answer is context-rich and directly addresses the question, citing any specific resources or guides mentioned in the context when relevant.\n```\n\nMIPRO is able to jointly leverage information about how well different prompts have performed, as well as additional information about the dataset it is being optimized for, such as questions derived from Weaviate’s blog collection.\n\n## Compiling Weaviate’s `generative-search` modules with DSPy\nThis article has demonstrated a few tools DSPy gives you to produce an optimized prompt for RAG! Now that you have this prompt, you can plug it into Weaviate’s `generative-search` modules. Weaviate’s `generative-search` module, [released in March 2023](https://www.youtube.com/watch?v=ro3ln4A9N8w), is our solution to help developers build Stateful AI.\n\nAfter we compile a DSPy program, we save the instructions with:\n```python\nCOPRO_compiled_RAG.save("My-RAG-Prompt.json")\n```\n\nWe then load it into Weaviate by reading the JSON file,\n```python\ndef GenerateAnswer_instruction_from_dspy_json(file_path):\n    with open(file_path, "r") as file:\n        data = json.load(file)\n    return data["generate_answer"]["signature_instructions"]\n\ntask_description = GenerateAnswer_instruction_from_dspy_json("RAG-with-Command-R-Example.json")\n```\n\nWe can then query in Weaviate using the v4 Python client with:\n```\nweaviate_blogs = weaviate_client.collections.get("WeaviateBlogChunk")\n\nquery = "What is Product Quantization? Why is it helpful for Vector Databases?"\nextended_task_description = task_description + "\\nQuery: {query}\\nAnswer:"\n\nresponse = weaviate_blogs.generate.near_text(\n    query = query,\n    limit = 3,\n    grouped_task=task_description\n)\n\nprint(response.generated)\n```\n\n## Beyond Prompting: Gradient Descent and DSPy\nIn this blog post, we covered how we can optimize the prompt that we feed to the language model. However, there is one additional way that we can boost our RAG performance – fine tuning with gradient descent. DSPy offers a level of abstraction around both methods of optimization. [Here](https://twitter.com/lateinteraction/status/1777082868357070972) are more thoughts on this from Omar Khattab.\n\n## Conclusion \nIn conclusion, this article presents the DSPy optimizers and advice on when to use which depending on whether you need instruction tuning, few-shot examples, gradient descent or combinations of the three. Which optimization strategy you need generally depends on the size of the LLM, for example, GPT-4 and Claude Opus are much larger models than Cohere’s Command R, which is similarly much larger than Mistral or Llama2 7B. This article provides a deep dive into how each of the optimizers work, BootstrapFewShot, COPRO, and MIPRO. Hopefully this has helped your understanding of DSPy’s optimizers!\n\n\nimport WhatNext from \'/_includes/what-next.mdx\'\n\n',author:"",category:["Blog"],summary:"",chunk_spans:[[20218,20891]]},{title:"Blog Hurricane-blog-gfls",date:"",content:'\n![HERO image](./img/hero.png)\n\nBack in May 2023, we published an initial vision for [Generative Feedback Loops](/blog/generative-feedback-loops-with-llms), which we have defined to describe saving the outputs of generative AI systems back into databases for future use. We have since extended our experiments from AirBnB listings to [Amazon products](https://medium.com/@iamleonie/recreating-amazons-new-generative-ai-feature-product-review-summaries-50640e40872a), even running every component of a Generative Feedback Loop within [Snowpark Container Services](/blog/generative-feedback-loop-with-weaviate-and-spcs#implementing-a-generative-feedback-loop-with-weaviate). Both of these applications illustrate the ability of generative AI to enhance our data. With the emergence of DSPy, we are now entering a new era of control with generative AI systems and are able to create even more compelling examples of Generative Feedback Loops.\n\nWhen we initially published our predictions for Generative Feedback Loops, we presented Ben Thompson’s [Aggregation Theory](https://stratechery.com/2015/aggregation-theory/) to describe some of the impacts of this technology. Aggregation Theory describes how “companies that win the user experience can generate a virtuous cycle where their ownership of consumers/users attracts suppliers which improves the user experience”. We concluded that “LLMs present the opportunity to expedite the virtuous cycle Aggregation Theory describes by commoditizing the human effort traditionally required to deliver great user experiences”.\n\nAt Weaviate, we are committed to delivering a great user experience to our developers. One of our main efforts to support this has been to author blog posts that clarify key concepts and help developers build with Weaviate. This article will walk you through a new addition to our [`weaviate-tutorials`](https://github.com/weaviate-tutorials) collection, [`Hurricane`](https://github.com/weaviate-tutorials/Hurricane). Inspired by Stanford’s [STORM](https://arxiv.org/abs/2402.14207), Hurricane leverages technologies from Arize, DSPy, OpenAI, Weaviate, and You to author new Weaviate blog posts and feed these generated blogs back into Weaviate. Hurricane lightens the human effort required to deliver a great user experience for blog reading, tailored to a Weaviate developer’s particular interests and questions.\n\nThis article contains two key sections:\n1. [Hurricane Code Explained](#part-1-hurricane-code-explained)\n2. [Optimizing Hurricane](#part-2-hurricane-optimization)\n\n## Part 1: Hurricane Code Explained\n\nInspired by Erika’s initial experiments [writing blog posts with DSPy](https://www.youtube.com/watch?v=ickqCzFxWj0) and Stanford’s STORM, we began to understand that we could build completely new types of generative AI applications with DSPy. As demonstrated in massively popular AI products such as ChatGPT, Perplexity, Le Chat, and Weaviate’s [Verba](https://github.com/weaviate/Verba), putting User Interface design at the heart of development dramatically facilitates adoption of new technologies. The Hurricane front-end user story is illustrated below:\n\n\n  \n    \n    Your browser does not support the video tag.\n  \n  A walkthrough of the Hurricane user story\n\n\n- A user enters the question they want to write a blog post about.\n- Hurricane acknowledges the request and streams its progress while writing.\n- Hurricane returns a blog post to the user and updates the counter of AI-generated blog posts in Weaviate.\n\nAs illuminated by [Arize Phoenix](https://docs.arize.com/phoenix), running Hurricane with GPT-3.5-Turbo takes about 45 seconds to convert a question into a blog post. This presents a significant challenge for user experience design. We thus believe that intermediate progress streaming or, “thoughts” if you prefer that abstraction, will help keep users engaged during long running processes. There are several user experience features we have seen released with ChatGPT that we believe should be celebrated and integrated into future system design: `Stop Generating`, `Edit Past Messages`, and `Conversation History`. These interactions help humans achieve what they want with AI systems. Generative Feedback Loops offer a major opportunity to enhance these user experiences, and we are just scratching the surface with the design of Hurricane.\n\nWe will now switch gears into the backend of Hurricane and how we orchestrated processing with our DSPy program. Beginning with the program and signature design, the DSPy Programming Model provides a brilliant developer experience for defining prompts and their respective input and output fields. DSPy further interfaces these programs with next generation optimizers, which we will discuss later in the article. The DSPy Programming Model consists of Signatures that define the task instructions and input and output fields, as well as Modules that implement Signatures and how they exchange messages between each other. Hurricane is composed of 5 signatures: Question2BlogOutline, Topic2Pargraph, BoldPrediction, WeaviateRelevance, and TitleAndTakeaways. Signatures also greatly facilitate the organization of codebases with prompts, illustrated [here in `hurricane_signatures.py`](https://github.com/weaviate-tutorials/Hurricane/blob/main/hurricane_signatures.py).\n\n```python\nclass Question2BlogOutline(dspy.Signature):\n    """Your task is to write a Weaviate blog post that will help answer the given question.\\nPlease use the contexts from a web search and published Weaviate blog posts to evaluate the structure of the blog post."""\n    \n    question = dspy.InputField()\n    blog_context = dspy.InputField()\n    web_context = dspy.InputField()\n    blog_outline = dspy.OutputField(desc="A list of topics the blog will cover. IMPORTANT!! This must follow a comma separated list of values!")\n    introduction_paragraph = dspy.OutputField(desc="An introduction overview of the blog post that previews the topics and presents the question the blog post seeks to answer.")\n```\n\n```python\nclass Topic2Paragraph(dspy.Signature):\n    """Please write a paragraph that explains a topic based on contexts from a web search and blog posts from an authoritative source. You are also given the original question that inspired research into this topic, please try to connect your review of the topic to the original question."""\n    \n    topic = dspy.InputField(desc="A topic to write a paragraph about based on the information in the contexts.")\n    original_question = dspy.InputField(desc="The original question that inspired research into this topic.")\n    web_contexts = dspy.InputField(desc="contains relevant information about the topic from a web search.")\n    blog_contexts = dspy.InputField(desc="contains relevant information about the topic from the published Weaviate blogs.")\n    paragraph = dspy.OutputField()\n```\n\n```python\nclass BoldPrediction(dspy.Signature):\n    """Please review this blog post and propose a bold prediction about it\'s content."""\n    \n    blog = dspy.InputField()\n    bold_prediction = dspy.OutputField(desc="A bold prediction about it\'s content.")\n```\n\n```python\nclass WeaviateRelevance(dspy.Signature):\n    """Please review this blog post and describe why its content and the claims it\'s making are relevant for the development of the Weaviate Vector Database. You are given additional contexts describing what Weaviate is and some aspects of its technology."""\n\n    blog_contexts = dspy.InputField(desc="Content describing what Weaviate is")\n    blog_post = dspy.InputField()\n    weaviate_relevance = dspy.OutputField()\n```\n\n```python\nclass TitleAndTakeaways(dspy.Signature):\n    """Write a title and key takeaways for a blog post given the blog post and the original question it sought to answer as input and a bold prediction the author discovered after conducting initial research on the topic."""\n    \n    blog = dspy.InputField()\n    original_question = dspy.InputField()\n    title = dspy.OutputField()\n    key_takeaways = dspy.OutputField(desc="Key takeaways from the article. IMPORTANT!! This must follow a comma separated list of values!")\n```\n\nWe can then connect how these Signatures process inputs by defining the DSPy program, `Hurricane`.  Similar to previous waves of automation, the latest advances in Artificial Intelligence allow humans to abstract our thinking into the high-level structure of what we are trying to achieve and leave the painstaking details to automated systems. Rather than focus on particular details of the content of a blog post, we instead think about how the structure of a blog post writing task should be. In Hurricane, we decompose the blog writing task into (1) researching and writing an outline, (2) researching and writing further about each topic in the outline, (3) forming a bold prediction about the future based on the writing so far, (4) writing about why this topic is relevant to Weaviate, and finally, (5) writing a title and key takeaways from the blog post. The following image visualizes how the components connect together and which parts of the computation can be parallelized.\n\n\n\nThe code for connecting these components in DSPy looks like this:\n\n```python\nimport dspy\nfrom hurricane_signatures import Question2BlogOutline, Topic2Paragraph, BoldPrediction, WeaviateRelevance, TitleAndTakeaways\nfrom utils import format_weaviate_and_you_contexts\nfrom utils import format_blog_draft, format_blog_post, BlogPost\n\nclass Hurricane(dspy.Module):\n    def __init__(self, you_rm):\n        # 5 LLM Layers (Question2BlogOutline, Topic2Paragraph, BoldPrediction, WeaviateRelevance, TitleAndTakeaways)\n        # 2 Retrieval Engines (Weaviate and You)\n        # Weaviate is configured with dspy.settings.configure(rm=weaviate_rm)\n        \n        self.question_to_blog_outline = dspy.ChainOfThought(Question2BlogOutline)\n        self.topic_to_paragraph = dspy.ChainOfThought(Topic2Paragraph)\n        self.bold_prediction = dspy.ChainOfThought(BoldPrediction)\n        self.weaviate_relevance = dspy.ChainOfThought(WeaviateRelevance)\n        self.title_and_key_takeaways = dspy.ChainOfThought(TitleAndTakeaways)\n        self.you_rm = you_rm\n\n    def forward(self, question):\n        blog_container = BlogPost()\n        blog_contexts = dspy.Retrieve(k=5)(question).passages\n        web_contexts = self.you_rm(question)\n        blog_contexts, web_contexts = format_weaviate_and_you_contexts(blog_contexts, web_contexts)\n        question_to_blog_outline_outputs = self.question_to_blog_outline(question=question, blog_context=blog_contexts, web_context=web_contexts)\n        blog_container.outline = question_to_blog_outline_outputs.blog_outline\n        parsed_blog_outline = blog_container.outline.split(",")\n        blog_container.introduction_paragraph = question_to_blog_outline_outputs.introduction_paragraph\n        for topic in parsed_blog_outline:\n            blog_contexts = dspy.Retrieve(k=5)(topic).passages\n            web_contexts = self.you_rm(topic)\n            blog_contexts, web_contexts = format_weaviate_and_you_contexts(blog_contexts, web_contexts)\n            blog_container.evidence_paragraphs.append(self.topic_to_paragraph(topic=topic, original_question=question, web_contexts=web_contexts, blog_contexts=blog_contexts).paragraph)\n        blog = format_blog_draft(blog_container)\n        blog_container.bold_prediction = self.bold_prediction(blog=blog).bold_prediction\n        blog_contexts = dspy.Retrieve(k=8)("What technology does Weaviate build?").passages\n        blog_contexts = "".join(blog_contexts)\n        blog_container.weaviate_relevance = self.weaviate_relevance(blog_contexts=blog_contexts, blog_post=blog).weaviate_relevance\n        title_and_takeaways = self.title_and_key_takeaways(blog=blog, original_question=question)\n        blog_container.title = title_and_takeaways.title\n        blog_container.takeaways = title_and_takeaways.key_takeaways\n        \n        final_blog = format_blog_post(blog_container)\n        return dspy.Prediction(blog=final_blog)\n```\n\nWe use this program structure for compilation, but at inference time we separate each component into its own API. This can be done by loading the program state into each component as follows:\n\n```python\ncompiled_hurricane = Hurricane(you_rm)\ncompiled_hurricane.load("compiled_hurricane.json")\nquestion_to_blog_outline = compiled_hurricane.question_to_blog_outline\ntopic_to_paragraph = compiled_hurricane.topic_to_paragraph\nbold_prediction = compiled_hurricane.bold_prediction\nweaviate_relevance = compiled_hurricane.weaviate_relevance\ntitle_and_key_takeaways = compiled_hurricane.title_and_key_takeaways\n```\n\nBefore transitioning back into the web application code, we would like to highlight [Self-Discover](https://arxiv.org/pdf/2402.03620.pdf) for LLM program design. It remains an open question of how much **human** program design will be needed. For example, prompting Claude 3 with the following prompt results in a very similar program architecture to what we have shown above. In a similar spirit to [Gorilla LLMs](https://weaviate.io/blog/weaviate-gorilla-part-1), we can also give the LLM background information about DSPy python syntax, and it will generate an executable program.\n\n\n> Can you please help me break a task into subtasks? \n> The task is to take a question and write a blog post that answers it, provides more background information about the \n> topic, looks for nuances about the topic such as unanswered questions or bold predictions, and connects the relevance \n> of the topic to the Weaviate Vector Database. Please format each sub-task as a `Signature` that contains a task \n> description and input and output fields. Please describe how each sub-task processes outputs from the previous sub-task\n> and where sub-tasks can be parallelized in the `Module`.\n\n### Stateful AI\n\nReturning to our web application, one of the most interesting questions surrounding DSPy and LLM programming is: How to serve DSPy programs in Production? As widespread adoption of Retrieval-Augmented Generation was beginning, Weaviate Co-Founder Bob van Luijt explained Weaviate’s mission to help developers build **Stateful AI** [[1](https://twitter.com/bobvanluijt/status/1695078287046005082), [2](https://www.youtube.com/watch?v=TXMUXeml9JY)]. In client-server software design, state management involves maintaining and synchronizing the application’s data across the server and clients. The server typically handles central state, such as application or session data, as well as resources that are shared among multiple clients, such as caches, message queues, or file storage. Clients interact with the central state by sending requests to the server, which then processes the requests, updates the central state if necessary, and sends responses back to the clients.\n\nTo facilitate state management in LLM applications, Weaviate has developed [`generative-search`](https://weaviate.io/developers/weaviate/search/generative) featuring integrations with Cohere, Google, Mistral, OpenAI, Bedrock, and Anyscale. Generative Search couples retrieval and inference management into a single API orchestrated on the server. We are currently researching further ways to improve this product and help developers manage state in their LLM applications. Hurricane currently uses a Generative Feedback Loop consisting of a sequence of client-server calls, persisting state by inserting data into Weaviate and accessing it with a session id. A single LLM call, where we send a prompt and receive text is **stateless**, but when we extend the LLM call with data from retrieval or a previous call to an LLM, such as the handoff from `Question2BlogOutline` to `Topic2Paragraph`, the call is now **stateful**. \n\nStateful processing is orchestrated in Hurricane as follows:\n\n1. When the submit button is clicked, we send a request to the server and create a new instance of a `Blog` object in Weaviate, returning the UUID to serve as a session id and provide key-value lookups for the intermediate state of Hurricane\'s processing.\n\n```javascript\nConst handleClick = async () => {\n	Try {\n		setThoughts(prevThoughts => ["Received question: " + String(inputValue)]);\n		const createSessionResponse = await axios.post(\'http://localhost:8000/create-session\');\n        const sessionId = String(createSessionResponse.data.session_id); \n	} catch (error) {\n		console.error(‘Error’:, error);\n	}\n}\n```\n\n2. We then process the question with 5 APIs: `/question-to-blog`, `/topic-to-paragraph`, `/bold-prediction`, `/weaviate-relevance`, and `/finish-blog`. Each endpoint uses Weaviate to save and load the intermediate state by providing the `sessionId`, for example:\n\n```python\n@app.post("/weaviate-relevance")\nasync def weaviate_relevance_generator(request: SessionRequest):\n    sessionId = request.sessionId\n    weaviate_gfl_client = weaviate.connect_to_local()\n    weaviate_blog_collection = weaviate_gfl_client.collections.get("Blog")\n    # load state into a BlogPost to format_blog_draft\n    blog_container = BlogPost()\n    saved_blog_properties = weaviate_blog_collection.query.fetch_object_by_id(sessionId)\n    # dump state into a Pydantic model\n    for key in saved_blog_properties.properties:\n        setattr(blog_container, key, saved_blog_properties.properties[key])\n    blog = format_blog_draft(blog_container)\n    blog_contexts = dspy.Retrieve(k=5)("What technology does Weaviate build?").passages\n    blog_contexts = "".join(blog_contexts)\n    weaviate_relevance_response = weaviate_relevance(blog_contexts=blog_contexts, blog_post=blog).weaviate_relevance\n    # Save State\n    weaviate_blog_collection.data.update(\n        uuid=sessionId,\n        properties={\n            "weaviate_relevance": weaviate_relevance_response\n        }\n    )\n    thoughts = "Researched why this topic is relevant to Weaviate."\n    return {"thoughts": thoughts}\n```\n\nNote, we also use a Pydantic model for blog posts which we use to format the intermediate state into a blog draft for certain inputs as well as for returning the final result to a JavaScript component. Pydantic is a library in Python that provides access to strictly typed objects such as a `struct` in Golang.\n\n```python\nfrom pydantic import BaseModel\n\nclass BlogPost(BaseModel):\n    question: Optional[str] = None\n    title: Optional[str] = None\n    introduction_paragraph: Optional[str] = None\n    outline: Optional[str] = None\n    evidence_paragraphs: Optional[List[str]] = []\n    bold_prediction: Optional[str] = None\n    weaviate_relevance: Optional[str] = None\n    takeaways: Optional[List[str]] = []\n\n```\n\n3. We load the final state from Weaviate, pass it into the Pydantic model and send this to the front-end to render with a `Blogpost.js` template:\n\n```javascript\n// BlogPost.js\nconst BlogPost = ({ blogPost }) => {\n  if (!blogPost) {\n    return null;\n  }\n\n  return (\n    \n      {blogPost.title && {blogPost.title}}\n      {blogPost.introduction_paragraph && {blogPost.introduction_paragraph}}\n      {blogPost.evidence_paragraphs && blogPost.evidence_paragraphs.length > 0 && (\n        \n          {blogPost.evidence_paragraphs.map((paragraph, index) => (\n            {paragraph}\n          ))}\n        \n      )}\n      {blogPost.bold_prediction && (\n        \n          Bold Prediction\n          {blogPost.bold_prediction}\n        \n      )}\n      {blogPost.weaviate_relevance && (\n        \n          Relevance to Weaviate\n          {blogPost.weaviate_relevance}\n        \n      )}\n      {blogPost.takeaways && blogPost.takeaways.length > 0 && (\n        \n          Takeaways from {blogPost.title}:\n          \n            {blogPost.takeaways.map((takeaway, index) => (\n              {takeaway}\n            ))}\n          \n        \n      )}\n    \n  );\n};\n\nexport default BlogPost;\n```\n\nWe believe there are many more opportunities to microservice architecture design with DSPy programs. For example, as a next step we can parallelize the 5 calls to `Topic2Paragraph` because they do not depend on each other. We would then wait to synchronize the evidence paragraphs into a blog draft, and then move on to parallelizing `BoldPrediction` and `WeaviateRelevance`. There are many interesting works in this domain, we highly recommend interested readers to check out [ALTO](https://arxiv.org/pdf/2403.04311.pdf), from Santhanam et al. Shown below, ALTO further pushes the cutting edge of LLM program design by integrating streamed structured output parsing to further speed up parallel processing. This topic was also heavily discussed in our [Weaviate podcast with Jason Liu](https://www.youtube.com/watch?v=higlHgYDc5E).\n\n\n\nAn example from [`ALTO`](https://arxiv.org/pdf/2403.04311.pdf), image taken from Santhanam et al. 2024.\n\n\nThere are many other cases where we may want to split up the components of a DSPy program into multiple APIs. For example, during optimization, we could have an 8-layer program we wish to optimize packaged in one container, and then a 3-layer metric program or 7-layer MIPRO optimizer in other containers. This improves throughput and resource allocation at scale. As we have shown with Weaviate’s Generative Feedback Loops, this also helps tremendously with saving intermediate state for future use, or in case the program crashes and needs to pick up where it left off. Imagining very complex systems, we may also reuse parts of the program with conditional routing. Further, we may imagine a future of systems that consist jointly of LLMs behind closed APIs, and self-hosted models. This could facilitate processing parts of the program containing sensitive information on-prem, as well as cost efficiency when serving smaller fine-tuned models for a particular task. We encourage interested readers to check out the Weaviate module system to gain a better sense of what to expect in the upcoming few months from the Weaviate development team. We are super excited about [new decoupling improvements to our `generative-search` APIs](https://github.com/weaviate/weaviate/issues/3364) that will in the meantime facilitate easy model switching, but in the long run make Weaviate more modular for all sorts of LLM processing from RAG to Generative Feedback Loops and [Compound AI Systems](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/).\n\n## Part 2: Hurricane Optimization\n\nHopefully you are already impressed with the DSPy programming model and are excited to use this framework to write task descriptions, define the inputs and outputs to LLM calls, and orchestrate the system logic in the `forward` pass of the program. However, the new opportunities with DSPy does not stop there, in fact, we are only scratching the surface! On the backend of DSPy, we have a completely new paradigm of optimizers, also referred to as teleprompters or compilers. The DSPy optimizers use novel algorithms to find the optimal task description, input-output examples for either few-shot in-context learning, or fine-tuning with gradient descent, and with the emergence of Self-Discover, [perhaps the structure of the program itself](https://twitter.com/ecardenas300/status/1769396057002082410).\n\nDSPy has recently unveiled two new optimizers, COPRO and [MIPRO](https://twitter.com/kristahopsalong/status/1766166198079889737). One of the opportunities that drew us the most to this project was to better understand how these new optimizers work with multi-layer programs. In this article, we used a pipeline of COPRO instruction optimization to BootstrapFewShot example sourcing.\n\nCOPRO is a super fascinating optimizer for task instructions, reminiscent of [Google DeepMind’s Promptbreeder](https://arxiv.org/pdf/2309.16797.pdf). COPRO begins by proposing alternate ways of phrasing a task. For example, we begin by prompting an LLM with `Your task is to write an optimized description of a task`, and it may convert “please rerank these documents according to their relevance to a query” into “optimize and reorganize a collection of documents by arranging them in order of their significance and applicability in response to a specific search query”.\n\nCOPRO then evaluates the performance of each of these rewritten descriptions and uses a fascinating crossover step common to evolutionary optimization algorithms to write new candidates. COPRO takes as input the `breadth` hyperparameter of descriptions and their respective performances and uses this information to propose further re-written descriptions. This continues for the `depth` hyperparameter number of iterations. Applying COPRO to Hurricane results in the following transformations of the initial to optimized prompts.\n\n| Signature          | Initial Prompt                                                                                                                                                                                                                                                    | COPRO Optimized Prompt |\n|--------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Blog2Outline       | Your task is to write a Weaviate blog post that will help answer the given question. Please use the contexts from a web search and published Weaviate blog posts to evaluate the structure of the blog post. | Start by conducting thorough research on the question provided to gather relevant information from reputable sources. Analyze the structure and language used in previous Weaviate blog posts to maintain consistency. Consider including visuals like images or graphs to enhance the content. |                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| Topic2Paragraph    | Please write a paragraph that explains a topic based on contexts from a web search and blog posts from an authoritative source. You are also given the original question that inspired research into this topic, please try to connect your review of the topic to the original question. | Research, merge insights, analyze various case studies, and summarize information to elucidate the specific context surrounding the subject matter of interest. Make sure to reference reputable sources for your conclusions and attempt to related your comprehensive analysis back to the initial inquiry that sparked your investigation. |\n| BoldPrediction     | Please review this blog post and propose a bold prediction about its content. | Analyzing the article, make relevant connections to current trends and propose an innovative forecast about its subject matter. |                                                                               |\n| WeaviateRelevance  | Please review this blog post and describe why it’s content and the claims it’s making are relevant for the development of the Weaviate Vector Database. You are given additional contexts describing what Weaviate is and some aspects of its technology.    | Please review this blog post and describe why it’s content and the claims it’s making are relevant for the development of the Weaviate Vector Database. You are given additional contexts describing what Weaviate is and some aspects of its technology. |\n| TitleAndTakeaways  | Write a title and key takeaways for a blog post given the blog post and the original question it sought to answer as input and a bold prediction the author discovered after conducting initial research on the topic.               | Provide a concise summary of the blog post based on the original question and main findings, followed by highlighting a bold prediction based on initial research. |\n\n\nWe then stack these new signatures with the `BootstrapFewShot` compiler that runs traces through the program with the new signatures, providing input-output examples to better control the system. In our experiments, we bootstrap 2 examples for each of the 5 components of Hurricane.\n\nGraduating from COPRO, the DSPy team has recently unveiled their latest and greatest optimizer, [MIPRO](https://twitter.com/kristahopsalong/status/1766166198079889737). MIPRO, short for “Multi-prompt Instruction Proposal Optimizer”, extends automated prompt optimization by adding a dataset summarizer and bayesian optimization of proposed instructions and examples. We found massive value from Arize Phoenix in understanding the compilation process. Optimizers like COPRO and MIPRO make several calls to LLMs to optimize LLMs, and Phoenix tracing is super insightful for keeping track of what’s going on!\n\nHopefully that gives you a good initial sense of how these optimizers work. A deeper dive is a bit out of scope for this blog post. If interested in discussing these things further, we highly recommend joining the [DSPy discord](https://discord.gg/zqcyX9TN4Q)!\n\nOnce we have a compiled program, the following short code snippet illustrates how to save and load a compiled DSPy program:\n\n```python\ncompiled_hurricane.save(“compiled_hurricane.json”)\nhurricane = Hurricane()\nhurricane.load(“complied_hurricane.json”)\n```\n\n### DSPyOps\n\nDevOps is one of the most wide ranging roles in software engineering teams that requires a range of expertise from production log monitoring to continuous integration and deployment of new software, and many more. Because AI-Native applications have caused a massive shift in software design, MLOps has emerged to describe DevOps challenges unique to Machine Learning systems. MLOps encompasses many areas of engineering and research from experiment tracking and hyperparameter tuning to continual training, model versioning, data drift detection, and more. With the rise of LLMs, MLOps has been somewhat replaced with LLMOps, with nearly all of the same concepts making the transition, but with a lot of the emphasis on continual training replaced with input-output logging and retrieval monitoring.\n\nWe believe DSPy is game changing enough to warrant the next adoption of the Ops nomenclature, DSPyOps. The key evolution from LLMOps being that we return the emphasis of continual training and model versioning from MLOps. However, similarly to LLMOps, DSPyOps also shares a massive emphasis on logging and call monitoring.\n\nOne of the most powerful tools we have seen for LLM- and now, DSPy- Ops has been Arize AI, further offering an open-source product called `Phoenix`. We have found Phoenix to be a tremendous help for building with DSPy. Phoenix logs calls to LLMs as JSONs, letting you inspect inputs and outputs. Phoenix further wraps traces in their program structure, letting you inspect each intermediate input and output of the Hurricane program in calls to `Hurricane.forward()`. Phoenix logs the latency of each call, further summarizing the results into P50 and P99 latencies!\n\n\n  \n    \n    Your browser does not support the video tag.\n  \n  DSPy program logging with Arize Phoenix\n\n\nAs discussed earlier, this could be enormously insightful for finding where the bottlenecks are in DSPy programs consisting of multiple LLMs, programs, and layers within each program!\n\nTracking the progress of Generative Feedback Loops is very similar to a lot of the work Weaviate has done on monitoring imports to Weaviate or moving data between clusters. Our integrations and partnerships with Prometheus metrics, Grafana dashboards, and Datadog, let us understand how much memory Weaviate is using, the time to import individual objects or batches of objects, as well as query latencies. As Generative Feedback Loops populate the data inside of Weaviate, we will need to employ the same tools for monitoring and observability. To learn more about Weaviate and Datadog, sign up for this [webinar](https://www.datadoghq.com/partner/weaviate-q1/?utm_source=partner&utm_medium=organic-email&utm_term=p&utm_campaign=webinar-202403weaviate) with Erika Cardenas!\n\nFinally, DSPyOps is indeed different from most of the recent discussion around LLMOps because we add an optimization component. The questions then become: How to monitor performance and re-run optimization? Which optimization to continually run? We believe the [MIPRO](https://twitter.com/kristahopsalong/status/1766166198079889737) optimizer’s integration of dataset analysis will be massive for combating potential dataset or distribution shifts, aiding in the continual learning problem. Further, another outstanding question is: How to update a DSPy program in production? As shown earlier, we can update a DSPy program by loading in compiled parameters sent to the system as a JSON file. This should work great for systems that call black-box LLM APIs, but updating self-hosted fine-tuned LLMs will require deeper insight.\n\n## Concluding thoughts\n\nHopefully this example of writing blog posts with generative AI has further inspired the possibilities of Generative Feedback Loops! This describes feeding the data produced from generative AI systems back into databases for future use. Generative Feedback Loops can enhance the blog post experience in several ways from generating new posts as we have explored, to proposing edits of existing posts, or writing meta analyses of what the posts have covered, and more.\n\nWe believe the next step in Generative Feedback Loops for blog post writing will be to equip Hurricane with intrinsic motivation. Currently Hurricane will only write a blog post if a human spins up the app and enters a question or topic. However, Hurricane could be prompted with the context of existing blog posts and questions such as “What questions or topics does this blog post further inspire your interest in?”. The blog posts we have currently written average about 15 500 token chunks per blog post. Imagine we write 100 synthetic blog posts, we now have 1,500 chunks in our Vector Database. By connecting this data with additional processing such as meta-analyses, perhaps writing code or additional documentation will result in an explosion in the dataset size of most generative AI applications.\n\nHowever, Hurricane is not quite ready to realize this potential. The cost of generating blog posts with high capacity models such as GPT-4 is currently too high. We thus believe efficiency optimizations such as employing the DSPy compilers to tune Mistral 7B, or fine-tune Gemma 2B, will be extremely valuable and take us to a future where everyone has millions, and maybe even eventually billions, of data objects for their applications.\n\nHurricane is just the beginning. As we continue to develop and refine these techniques, we envision a future where Generative Feedback Loops can autonomously create vast amounts of helpful content, adapted to the specific needs and interests of each person. By feeding this generated content back into the system, we can create an ever-growing knowledge base that is continuously updated and refined. Realizing this vision will require ongoing research and development in areas like prompt engineering, model optimization, and user experience design. It will also require careful consideration of cost efficiency, data quality, and ethical content generation.\n\n\n',author:"",category:["Blog"],summary:"",chunk_spans:[[22765,23628],[23479,24409],[4466,5323]]}],metadata:{collection_name:"weaviate_blogs",return_type:"document",output_type:"original",query_text:"Mark Robson",query_type:"semantic",code:{language:"python",title:"Query",text:'collection.query.near_text(query="Mark Robson", limit=5)'},chunked:!0},code:{language:"python",title:"Query",text:'collection.query.near_text(query="Mark Robson", limit=5)'}}},{type:"result",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",id:"res-2bab30c5-5233-48f2-b4f2-52402dc396fa",payload:{type:"document",objects:[{title:"Nv_queries Searches",date:"",content:'\nAs collections with named vectors can include multiple vectors, any vector or similarity search must specify a "target" vector.\n\nThis applies for `near_text` and `near_vector` searches, as well as multimodal searches such as `near_image` and so on. Let\'s explore a few examples here.\n\n##  Text searches\n\n###  Code\n\nHere, we look for entries in "MovieNVDemo" based on their similarity to the phrase `"A joyful holiday film"`. Note, however, that we show multiple versions of the same query, each with a different `target_vector` parameter:\n\n\n  \n    \n  \n  \n    \n  \n  \n    \n  \n\n\n###  Explain the code\n\nEach named vector here is based on a different property of the movie data.\n\nThe first search compares the meaning of the movie **title** with the query, the second search compares the **entire summary** (overview) with the query, and the third compares the **poster (and the title)** with the query.\n\nWeaviate also allows each named vector to be set with a different vectorizer. You will recall that the `poster_title` vector is created by the CLIP models, while the `title` and `overview` properties are created by the OpenAI model.\n\nAs a result, each named vector can be further specialized by using the right model for the right property.\n\n###  Explain the results\n\nThe results of each search are different, as they are based on different properties of the movie data.\n\n#### `title` vs `overview`\n\nNote that the search with the `overview` target vector includes titles like "Home Alone" and "Home Alone 2: Lost in New York", which are not included in the other searches.\n\nThis is because the plots of these movies are holiday-themed, even though the titles are not obviously joyful or holiday-related.\n\n#### `poster`\n\nThe search with `poster_title` target vector interestingly includes "Misery" - the Stephen King horror movie! This is very likely because the poster of the movie is a snowy scene. And since the CLIP vectorizer is trained to identify elements of images, it identifies this terrifying film as a result of the search.\n\nGiven the imagery of the poster only and no other context, you would have to say that the search isn\'t wrong, even though anyone who\'s read the book or watched the movie would agree.\n\n\n  \n\n```text\nHow the Grinch Stole Christmas 2000 8871\nDistance to query: 0.162\n\nThe Nightmare Before Christmas 1993 9479\nDistance to query: 0.177\n\nThe Pursuit of Happyness 2006 1402\nDistance to query: 0.182\n\nJingle All the Way 1996 9279\nDistance to query: 0.184\n\nMrs. Doubtfire 1993 788\nDistance to query: 0.189\n```\n\n  \n  \n\n```text\nHow the Grinch Stole Christmas 2000 8871\nDistance to query: 0.148\n\nHome Alone 1990 771\nDistance to query: 0.164\n\nOnward 2020 508439\nDistance to query: 0.172\n\nHome Alone 2: Lost in New York 1992 772\nDistance to query: 0.175\n\nLittle Miss Sunshine 2006 773\nDistance to query: 0.176\n```\n\n  \n  \n\nPosters for the top 5 matches:\n\n\n\n\n\n\n```text\nLife Is Beautiful 1997 637\nDistance to query: 0.621\n\nGroundhog Day 1993 137\nDistance to query: 0.623\n\nJingle All the Way 1996 9279\nDistance to query: 0.625\n\nTraining Day 2001 2034\nDistance to query: 0.627\n\nMisery 1990 1700\nDistance to query: 0.632\n```\n\n  \n\n\n##  Hybrid search\n\n###  Code\n\nThis example finds entries in "MovieNVDemo" with the highest hybrid search scores for the term "history", and prints out the title and release year of the top 5 matches.\n\n\n\n###  Explain the code\n\nHybrid search with named vectors works the same way as other vector searches with named vectors. You must provide a `target_vector` parameter to specify the named vector for the vector search component of the hybrid search.\n\n##  Keyword search\n\nAs named vectors affect the vector representations of objects, they do not affect keyword searches. You can perform keyword searches on named vector collections using the same syntax as you would for any other collections.\n\n##  Named vectors in search\n\nThe use of named vectors enables flexible search options that can be tailored to your needs.\n\nEach object can have as many named vectors as you would like, with any combinations of properties and vectorizers, or even multiple custom vectors provided by your own models.\n\nThis flexibility allows you to create databases with vector representations that are tailored to your specific use case, and to search for similar items based on any combination of properties.\n\n##  What about RAG?\n\nRAG, or retrieval augmented generation, queries with named vectors work the same way as with other vector searches with named vectors. You must provide a `target_vector` parameter to specify the named vector for the vector search component of the RAG query.\n\nThis, in turn, can improve the quality of the generation. Let\'s explore a few examples in the [next section](./20_use_cases.mdx).\n\n\n## Questions and feedback\n\n\n',author:"",category:["Documentation"],summary:"",chunk_spans:[[1831,2612]]},{title:"Named_vectors Nv_preparation",date:"",content:"\nThis course is self-contained. However, we recommend that you go through one of the 101-level courses, such as that for working with [text](../../starter_text_data/index.md), [your own vectors](../../starter_custom_vectors/index.md), or [multimodal data](../../starter_multimodal_data/index.md).\n\nThis page briefly covers the required resources and setup, including the Weaviate Python client library, and a Weaviate instance with the multi-modal vectorizer.\n\n##  Weaviate Python client library\n\nInstall the latest (`v4`, e.g. `4.5.0`) Weaviate Python client library with:\n\n```bash\npip install -U weaviate-client\n```\n\n##  Set up Weaviate\n\nInstall Docker on your machine. We recommend following the [official Docker installation guide](https://docs.docker.com/get-docker/).\n\nCreate a new directory and navigate to it in your terminal. Then, create a new file called `docker-compose.yml` and add the following content:\n\n```yaml\n---\nservices:\n  weaviate:\n    command:\n    - --host\n    - 0.0.0.0\n    - --port\n    - '8080'\n    - --scheme\n    - http\n    image: cr.weaviate.io/semitechnologies/weaviate:||site.weaviate_version||\n    ports:\n    - 8080:8080\n    - 50051:50051\n    volumes:\n    - weaviate_data:/var/lib/weaviate\n    restart: on-failure:0\n    environment:\n      CLIP_INFERENCE_API: 'http://multi2vec-clip:8080'\n      QUERY_DEFAULTS_LIMIT: 25\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n      ENABLE_MODULES: 'multi2vec-clip'\n      ENABLE_API_BASED_MODULES: 'true'\n      CLUSTER_HOSTNAME: 'node1'\n  multi2vec-clip:\n    image: cr.weaviate.io/semitechnologies/multi2vec-clip:sentence-transformers-clip-ViT-B-32-multilingual-v1\n    environment:\n      ENABLE_CUDA: '0'\nvolumes:\n  weaviate_data:\n...\n\n```\n\n###  Create a Weaviate instance\n\nRun the following command to start Weaviate:\n\n```bash\ndocker compose up\n```\n\n###  Your Weaviate instance details\n\nOnce the instance is created, you can access it at `http://localhost:8080`.\n\n##  Work with Weaviate\n\n###  Connect to your Weaviate instance\n\nTo connect to the Weaviate instance, use the `connect_to_local` function. We also provide API keys here for any inference APIs (e.g. OpenAI, Cohere, Google, AWS etc.) that Weaviate may use.\n\n\n\n###  Check Weaviate status\n\nYou can check whether the Weaviate instance is up using the `is_live` function.\n\n\n\n###  Retrieve server meta information\n\nYou can retrieve meta information about the Weaviate instance using the `meta` function.\n\n\n\nThis will print the server meta information to the console. The output will look similar to the following:\n\n\n  Example get_meta output\n\nNote that this output is a little longer due to the additional details from the CLIP models.\n\n\n\n\n###  Close the connection\n\nAfter you have finished using the Weaviate client, you should close the connection. This frees up resources and ensures that the connection is properly closed.\n\nWe suggest using a `try`-`finally` block as a best practice. For brevity, we will not include the `try`-`finally` blocks in the remaining code snippets.\n\n\n\n##  Source data\n\nWe are going to use a movie dataset sourced from [TMDB](https://www.themoviedb.org/). The dataset can be found in this [GitHub repository](https://raw.githubusercontent.com/weaviate-tutorials/edu-datasets/main/movies_data_1990_2024.json), and it contains bibliographic information on ~700 movies released between 1990 and 2024.\n\nAs a multimodal project, we'll also use [corresponding posters for each movie](https://raw.githubusercontent.com/weaviate-tutorials/edu-datasets/main/movies_data_1990_2024_posters.zip), which are available in the same repository.\n\n\n  See sample text data\n\n|    | backdrop_path                    | genre_ids       |   id | original_language   | original_title              | overview                                                                                                                                                                                                                                                                                                                                                                                                                        |   popularity | poster_path                      | release_date   | title                       | video   |   vote_average |   vote_count |\n|---:|:---------------------------------|:----------------|-----:|:--------------------|:----------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------:|:---------------------------------|:---------------|:----------------------------|:--------|---------------:|-------------:|\n|  0 | /3Nn5BOM1EVw1IYrv6MsbOS6N1Ol.jpg | [14, 18, 10749] |  162 | en                  | Edward Scissorhands         | A small suburban town receives a visit from a castaway unfinished science experiment named Edward.                                                                                                                                                                                                                                                                                                                              |       45.694 | /1RFIbuW9Z3eN9Oxw2KaQG5DfLmD.jpg | 1990-12-07     | Edward Scissorhands         | False   |            7.7 |        12305 |\n|  1 | /sw7mordbZxgITU877yTpZCud90M.jpg | [18, 80]        |  769 | en                  | GoodFellas                  | The true story of Henry Hill, a half-Irish, half-Sicilian Brooklyn kid who is adopted by neighbourhood gangsters at an early age and climbs the ranks of a Mafia family under the guidance of Jimmy Conway.                                                                                                                                                                                                                     |       57.228 | /aKuFiU82s5ISJpGZp7YkIr3kCUd.jpg | 1990-09-12     | GoodFellas                  | False   |            8.5 |        12106 |\n|  2 | /6uLhSLXzB1ooJ3522ydrBZ2Hh0W.jpg | [35, 10751]     |  771 | en                  | Home Alone                  | Eight-year-old Kevin McCallister makes the most of the situation after his family unwittingly leaves him behind when they go on Christmas vacation. But when a pair of bungling burglars set their sights on Kevin's house, the plucky kid stands ready to defend his territory. By planting booby traps galore, adorably mischievous Kevin stands his ground as his frantic mother attempts to race home before Christmas Day. |        3.538 | /onTSipZ8R3bliBdKfPtsDuHTdlL.jpg | 1990-11-16     | Home Alone                  | False   |            7.4 |        10599 |\n|  3 | /vKp3NvqBkcjHkCHSGi6EbcP7g4J.jpg | [12, 35, 878]   |  196 | en                  | Back to the Future Part III | The final installment of the Back to the Future trilogy finds Marty digging the trusty DeLorean out of a mineshaft and looking for Doc in the Wild West of 1885. But when their time machine breaks down, the travelers are stranded in a land of spurs. More problems arise when Doc falls for pretty schoolteacher Clara Clayton, and Marty tangles with Buford Tannen.                                                       |       28.896 | /crzoVQnMzIrRfHtQw0tLBirNfVg.jpg | 1990-05-25     | Back to the Future Part III | False   |            7.5 |         9918 |\n|  4 | /3tuWpnCTe14zZZPt6sI1W9ByOXx.jpg | [35, 10749]     |  114 | en                  | Pretty Woman                | When a millionaire wheeler-dealer enters a business contract with a Hollywood hooker Vivian Ward, he loses his heart in the bargain.                                                                                                                                                                                                                                                                                            |       97.953 | /hVHUfT801LQATGd26VPzhorIYza.jpg | 1990-03-23     | Pretty Woman                | False   |            7.5 |         7671 |\n\n\n\n## Questions and feedback\n\n\n",author:"",category:["Documentation"],summary:"",chunk_spans:[[5814,6464],[4252,5554],[5138,6230]]},{title:"Llm-frameworks Dspy",date:"",content:'[DSPy](https://github.com/stanfordnlp/dspy) from Stanford NLP is a framework for programming language models.\n\nDSPy introduces two key concepts, the **programming model** and **optimizers**.\n\n- **Programing model**: The programming model lets you define a series of components that make a language model request. Components include input fields, output fields, task descriptions, and calls to a vector database like Weaviate.\n\n- **Optimizers**: Optimizers compile your DSPy program to tune the language model prompt and/or the weights.\n\n## DSPy and Weaviate\n\nWeaviate is integrated with DSPy through the retriever model! \n\nConnect your Weaviate cluster (WCD or local instance) to DSPy, use the [retriever module](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/WeaviateRM) and pass in your collection:\n\n```python\nweaviate_client = weaviate.Client("http://localhost:8080") # or pass in your WCD cluster url\n\nretriever_module = WeaviateRM("WeaviateBlogChunk", # collection name\n                    weaviate_client=weaviate_client)\n```\n\n## Our Resources \nHere are a few resources from the Weaviate team on using DSPy!\n\nThe resources are broken into two categories: \n1. [**Hands on Learning**](#hands-on-learning): Build your technical understanding with end-to-end tutorials.\n\n2. [**Read and Listen**](#read-and-listen): Develop your conceptual understanding of these technologies.\n\n\n### Hands on Learning \n\n| Topic | Description | Resource | \n| --- | --- | --- |\n| Getting Started with RAG in DSPy | Learn about DSPy and how to build a program: Installation, settings, datasets, LLM metrics, DSPy programming model, and optimization. | [Notebook](https://github.com/weaviate/recipes/blob/main/integrations/llm-frameworks/dspy/1.Getting-Started-with-RAG-in-DSPy.ipynb), [Video](https://youtu.be/CEuUG4Umfxs?si=4Gp8gR9glmoMJNaU) |\n| DSPy + Weaviate for the Next Generation of LLM Apps | Build a 4-layer DSPy program for generating blog posts from queries. | [Notebook](https://github.com/weaviate/recipes/blob/main/integrations/llm-frameworks/dspy/2.Writing-Blog-Posts-with-DSPy.ipynb), [Video](https://youtu.be/ickqCzFxWj0?si=AxCbD9tq2cbAH6bB)|\n| RAG with Persona | Build a compound AI system with DSPy, Cohere, and Weaviate, where you\'ll add a persona to the language model. | [Notebook](https://github.com/weaviate/recipes/blob/main/integrations/llm-frameworks/dspy/fullstack-recipes/RAGwithPersona/4.RAG-with-Persona.ipynb), [Post](https://twitter.com/ecardenas300/status/1765444492348243976)|\n| Adding Depth to RAG Programs | Enhancing DSPy programs by integrating unique input-output examples and multiple LLMs. | [Notebook](https://github.com/weaviate/recipes/blob/main/integrations/llm-frameworks/dspy/3.Adding-Depth-to-RAG-Programs.ipynb), [Video](https://youtu.be/0c7Ksd6BG88?si=YUF2wm1ncUTkSuPQ) |\n| Hurricane: Writing Blog Posts with Generative Feedback Loops | Introduction to Hurricane, a web app for demonstrating generative feedback loops with blog posts. | [Notebook](https://github.com/weaviate-tutorials/Hurricane), [Blog](/blog/hurricane-generative-feedback-loops) |\n| Structured Outputs with DSPy | The three methods for structuring outputs in DSPy programs. | [Notebook](https://github.com/weaviate/recipes/blob/main/integrations/llm-frameworks/dspy/4.Structured-Outputs-with-DSPy.ipynb), [Video](https://youtu.be/tVw3CwrN5-8?si=P7fWeXzQ7p-2SFYF) |\n| Building RAG with Command R+ from Cohere, DSPy, and Weaviate | Overview of Command R+ with a quick RAG demo in DSPy. | [Notebook](https://github.com/weaviate/recipes/blob/main/integrations/llm-frameworks/dspy/llms/Command-R-Plus.ipynbb), [Video](https://youtu.be/6dgXALb_5Ag?si=nSX2AnmpbUau_2JF) |\n| Advanced Optimizers in DSPy | Dive into optimizing DSPy programs with various techniques. | [Notebook](https://github.com/weaviate/recipes/blob/main/integrations/llm-frameworks/dspy/5.Advanced-Optimizers.ipynb) |\n| Llama 3 RAG Demo with DSPy Optimization, Ollama, and Weaviate | Integrating Llama3 with DSPy and optimizing prompts with MIPRO. | [Notebook](https://github.com/weaviate/recipes/blob/main/integrations/llm-frameworks/dspy/llms/Llama3.ipynb), [Video](https://youtu.be/1h3_h8t3L14?si=G4d-aY5Ynpv8ckea)|\n| BigQuery and Weaviate orchestrated with DSPy | Build an end-to-end RAG pipeline that uses BigQuery and Weaviate using DSPy. | [Notebook](https://github.com/weaviate/recipes/blob/main/integrations/cloud-hyperscalers/google/bigquery/BigQuery-Weaviate-DSPy-RAG.ipynb)|\n\n\n\n### Read and Listen\n\n| Topic | Description | Resource | \n| --- | --- | --- |\n| DSPy and ColBERT with Omar Khattab! - Weaviate Podcast #85 | Omar Khattab joins the Weaviate podcast to discuss DSPy and ColBERT. | [Video](https://www.youtube.com/watch?v=CDung1LnLbY) |\n| DSPy Explained| Learn about the core concepts of DSPy. Walk through the introduction notebooks to compile a simple retrieve-then-read RAG program and Multi-Hop RAG Program. | [Video](https://youtu.be/41EfOY0Ldkc?si=sFieUeHc9rXRn6uk)|\n| XMC.dspy with Karel D\'Oosterlinck - Weaviate Podcast #87 | Karel D\'Oosterlinck joins the Weaviate podcast to discuss IReRa (Infer-Retrieve-Rank). | [Video](https://youtu.be/_ye26_8XPcs?si=ZBodgHbOcaq2Kwky)\n| Intro to DSPy: Goodbye Prompting, Hello Programming | Overview of DSPy and how it solves the fragility problem in LLM-based applications. | [Blog](https://towardsdatascience.com/intro-to-dspy-goodbye-prompting-hello-programming-4ca1c6ce3eb9)|\n| Fine-Tuning Cohere’s Reranker | Generate synthetic data with DSPy to fine-tune Cohere’s reranker model. |[Blog](/blog/fine-tuning-coheres-reranker)|\n| Your Language Model Deserves Better Prompting | Overview of the DSPy optimizers for prompt tuning. | [Blog](/blog/dspy-optimizers)|\n\n\n## Questions and feedback\n\n\n',author:"",category:["Documentation"],summary:"",chunk_spans:[[3197,3781]]}],metadata:{collection_name:"weaviate_documentation",return_type:"document",output_type:"original",query_text:"Mark Robson",query_type:"semantic",code:{language:"python",title:"Query",text:'collection.query.near_text(query="Mark Robson", limit=5)'},chunked:!0},code:{language:"python",title:"Query",text:'collection.query.near_text(query="Mark Robson", limit=5)'}}},{type:"text",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",id:"tex-54267a7b-4cc6-46e1-9dc0-d42e109c474a",payload:{type:"response",metadata:{},objects:[{text:"Unfortunately, I couldn't find any information about Mark Robson in the available collections."}]}},{type:"text",conversation_id:"f2e00824-bfb0-4f58-b897-3ae1ea52c6bb",query_id:"2cd41d8c-884b-4487-adac-2dc44192925e",id:"tex-c294ea60-6950-437c-a069-da620c69b0db",payload:{type:"response",metadata:{},objects:[{text:"I'm sorry, but I couldn't find any information about Mark Robson in the available knowledge bases.  Is there anything else I can help you with?"}]}}],finished:!0,query_start:new Date,query_end:new Date(new Date().getTime()+1e3),NER:{text:"What is Verba?",noun_spans:[],entity_spans:[[7,18]]},feedback:0,index:0},u={id:"12345",query:"When was the highest wind recorded?",messages:[{type:"User",id:"2dcc9a5d-d507-4d21-adf3-18d1687e1f67",query_id:"94570c95-1478-46de-acf2-324fdecff657",conversation_id:"368db6e7-849d-41a4-93e8-818dd368e9e0",payload:{type:"text",metadata:{},code:{language:"",title:"",text:""},objects:["When was the highest wind recorded?"]}},{type:"text",conversation_id:"368db6e7-849d-41a4-93e8-818dd368e9e0",query_id:"94570c95-1478-46de-acf2-324fdecff657",id:"tex-767fde45-e051-4b5a-bffa-19c23bf784e4",payload:{type:"response",metadata:{},objects:[{text:"I'm initiating a search in the weather collection to find details about the highest wind speed recorded."}]}},{type:"text",conversation_id:"368db6e7-849d-41a4-93e8-818dd368e9e0",query_id:"94570c95-1478-46de-acf2-324fdecff657",id:"tex-dd8257ec-93fc-4bcb-a81e-7532290d438b",payload:{type:"response",metadata:{},objects:[{text:"I'm using an aggregation function to find the maximum wind speed recorded in the dataset."}]}},{type:"text",conversation_id:"368db6e7-849d-41a4-93e8-818dd368e9e0",query_id:"94570c95-1478-46de-acf2-324fdecff657",id:"tex-d41dbb3a-69e9-401e-ba41-a9cbbd65615a",payload:{type:"response",metadata:{},objects:[{text:"I'm grouping the results by date to identify the specific date when the highest wind speed occurred."}]}},{type:"result",conversation_id:"368db6e7-849d-41a4-93e8-818dd368e9e0",query_id:"94570c95-1478-46de-acf2-324fdecff657",id:"res-ce60a756-1243-44bf-906d-96fb737ffaf2",payload:{type:"aggregation",objects:[{weather:{date:{type:"text",values:[{value:2,field:"2023-11-26",aggregation:"count"},{value:2,field:"2023-11-08",aggregation:"count"},{value:2,field:"2023-05-02",aggregation:"count"},{value:2,field:"2023-12-13",aggregation:"count"},{value:2,field:"2023-11-17",aggregation:"count"},{value:1,field:"2023-11-22",aggregation:"count"},{value:1,field:"2023-12-08",aggregation:"count"},{value:1,field:"2023-06-16",aggregation:"count"},{value:1,field:"2023-04-30",aggregation:"count"},{value:1,field:"2023-05-12",aggregation:"count"},{value:1,field:"2023-10-13",aggregation:"count"},{value:1,field:"2023-01-22",aggregation:"count"},{value:1,field:"2023-09-20",aggregation:"count"},{value:1,field:"2023-08-06",aggregation:"count"},{value:1,field:"2023-09-10",aggregation:"count"},{value:1,field:"2023-09-04",aggregation:"count"},{value:1,field:"2023-04-11",aggregation:"count"},{value:1,field:"2023-03-25",aggregation:"count"},{value:1,field:"2023-08-04",aggregation:"count"},{value:1,field:"2023-03-14",aggregation:"count"},{value:1,field:"2023-02-09",aggregation:"count"},{value:1,field:"2023-03-09",aggregation:"count"},{value:1,field:"2023-03-13",aggregation:"count"},{value:1,field:"2023-08-23",aggregation:"count"},{value:1,field:"2023-03-06",aggregation:"count"},{value:1,field:"2023-08-12",aggregation:"count"},{value:1,field:"2023-04-01",aggregation:"count"},{value:1,field:"2023-08-13",aggregation:"count"},{value:1,field:"2023-10-01",aggregation:"count"},{value:1,field:"2023-10-11",aggregation:"count"},{value:1,field:"2023-01-19",aggregation:"count"},{value:1,field:"2023-03-12",aggregation:"count"},{value:1,field:"2023-02-24",aggregation:"count"},{value:1,field:"2023-09-06",aggregation:"count"},{value:1,field:"2023-09-15",aggregation:"count"},{value:1,field:"2023-02-02",aggregation:"count"},{value:1,field:"2023-01-11",aggregation:"count"},{value:1,field:"2023-02-04",aggregation:"count"},{value:1,field:"2023-12-06",aggregation:"count"},{value:1,field:"2023-03-22",aggregation:"count"},{value:1,field:"2023-02-19",aggregation:"count"},{value:1,field:"2023-01-13",aggregation:"count"},{value:1,field:"2023-10-17",aggregation:"count"},{value:1,field:"2023-08-18",aggregation:"count"},{value:1,field:"2023-10-08",aggregation:"count"}],groups:{"2023-11-26":{wind_speed:{type:"number",values:[{value:89.6,field:null,aggregation:"maximum"}]}},"2023-11-08":{wind_speed:{type:"number",values:[{value:91.5,field:null,aggregation:"maximum"}]}},"2023-05-02":{wind_speed:{type:"number",values:[{value:93.5,field:null,aggregation:"maximum"}]}},"2023-12-13":{wind_speed:{type:"number",values:[{value:91.1,field:null,aggregation:"maximum"}]}},"2023-11-17":{wind_speed:{type:"number",values:[{value:12.6,field:null,aggregation:"maximum"}]}},"2023-11-22":{wind_speed:{type:"number",values:[{value:24.8,field:null,aggregation:"maximum"}]}},"2023-12-08":{wind_speed:{type:"number",values:[{value:12.2,field:null,aggregation:"maximum"}]}},"2023-06-16":{wind_speed:{type:"number",values:[{value:30,field:null,aggregation:"maximum"}]}},"2023-04-30":{wind_speed:{type:"number",values:[{value:14.6,field:null,aggregation:"maximum"}]}},"2023-05-12":{wind_speed:{type:"number",values:[{value:56.1,field:null,aggregation:"maximum"}]}},"2023-10-13":{wind_speed:{type:"number",values:[{value:53,field:null,aggregation:"maximum"}]}},"2023-01-22":{wind_speed:{type:"number",values:[{value:12.4,field:null,aggregation:"maximum"}]}},"2023-09-20":{wind_speed:{type:"number",values:[{value:30.9,field:null,aggregation:"maximum"}]}},"2023-08-06":{wind_speed:{type:"number",values:[{value:85.5,field:null,aggregation:"maximum"}]}},"2023-09-10":{wind_speed:{type:"number",values:[{value:36.2,field:null,aggregation:"maximum"}]}},"2023-09-04":{wind_speed:{type:"number",values:[{value:94.9,field:null,aggregation:"maximum"}]}},"2023-04-11":{wind_speed:{type:"number",values:[{value:34.2,field:null,aggregation:"maximum"}]}},"2023-03-25":{wind_speed:{type:"number",values:[{value:41.9,field:null,aggregation:"maximum"}]}},"2023-08-04":{wind_speed:{type:"number",values:[{value:44.5,field:null,aggregation:"maximum"}]}},"2023-03-14":{wind_speed:{type:"number",values:[{value:55.7,field:null,aggregation:"maximum"}]}},"2023-02-09":{wind_speed:{type:"number",values:[{value:19.6,field:null,aggregation:"maximum"}]}},"2023-03-09":{wind_speed:{type:"number",values:[{value:73.8,field:null,aggregation:"maximum"}]}},"2023-03-13":{wind_speed:{type:"number",values:[{value:72.9,field:null,aggregation:"maximum"}]}},"2023-08-23":{wind_speed:{type:"number",values:[{value:44.3,field:null,aggregation:"maximum"}]}},"2023-03-06":{wind_speed:{type:"number",values:[{value:38.4,field:null,aggregation:"maximum"}]}},"2023-08-12":{wind_speed:{type:"number",values:[{value:62.5,field:null,aggregation:"maximum"}]}},"2023-04-01":{wind_speed:{type:"number",values:[{value:72.7,field:null,aggregation:"maximum"}]}},"2023-08-13":{wind_speed:{type:"number",values:[{value:69.7,field:null,aggregation:"maximum"}]}},"2023-10-01":{wind_speed:{type:"number",values:[{value:35.8,field:null,aggregation:"maximum"}]}},"2023-10-11":{wind_speed:{type:"number",values:[{value:49.9,field:null,aggregation:"maximum"}]}},"2023-01-19":{wind_speed:{type:"number",values:[{value:26.7,field:null,aggregation:"maximum"}]}},"2023-03-12":{wind_speed:{type:"number",values:[{value:87.5,field:null,aggregation:"maximum"}]}},"2023-02-24":{wind_speed:{type:"number",values:[{value:73.2,field:null,aggregation:"maximum"}]}},"2023-09-06":{wind_speed:{type:"number",values:[{value:54.9,field:null,aggregation:"maximum"}]}},"2023-09-15":{wind_speed:{type:"number",values:[{value:84.7,field:null,aggregation:"maximum"}]}},"2023-02-02":{wind_speed:{type:"number",values:[{value:24.8,field:null,aggregation:"maximum"}]}},"2023-01-11":{wind_speed:{type:"number",values:[{value:23.9,field:null,aggregation:"maximum"}]}},"2023-02-04":{wind_speed:{type:"number",values:[{value:54.4,field:null,aggregation:"maximum"}]}},"2023-12-06":{wind_speed:{type:"number",values:[{value:69.3,field:null,aggregation:"maximum"}]}},"2023-03-22":{wind_speed:{type:"number",values:[{value:17.7,field:null,aggregation:"maximum"}]}},"2023-02-19":{wind_speed:{type:"number",values:[{value:93.5,field:null,aggregation:"maximum"}]}},"2023-01-13":{wind_speed:{type:"number",values:[{value:69.9,field:null,aggregation:"maximum"}]}},"2023-10-17":{wind_speed:{type:"number",values:[{value:30.4,field:null,aggregation:"maximum"}]}},"2023-08-18":{wind_speed:{type:"number",values:[{value:74.1,field:null,aggregation:"maximum"}]}},"2023-10-08":{wind_speed:{type:"number",values:[{value:36.6,field:null,aggregation:"maximum"}]}}}}}}],metadata:{collection_name:"weather",description:["Finding the maximum wind speed for each date in the weather data."],code:{language:"python",title:"Aggregation",text:'collection.aggregate.over_all(\n    group_by=GroupByAggregate(prop="date"),\n    return_metrics=Metrics("wind_speed").number(maximum=True),\n)'}},code:{language:"python",title:"Aggregation",text:'collection.aggregate.over_all(\n    group_by=GroupByAggregate(prop="date"),\n    return_metrics=Metrics("wind_speed").number(maximum=True),\n)'}}},{type:"text",conversation_id:"368db6e7-849d-41a4-93e8-818dd368e9e0",query_id:"94570c95-1478-46de-acf2-324fdecff657",id:"tex-99c85742-7f60-4386-9023-d41e1d68b9e0",payload:{type:"summary",metadata:{},objects:[{text:"The wind speed data reveals some remarkable meteorological moments in 2023. Among the recorded dates, the highest wind speed was **94.9 mph**, which occurred on **2023-09-04**. This represents an exceptionally strong wind event, likely categorized as a severe wind condition that could potentially cause significant environmental impact.",title:"Extreme Winds: Peak Velocity Revealed"}]}}],finished:!0,query_start:new Date,query_end:new Date(new Date().getTime()+1e3),NER:{text:"What is Elysia?",noun_spans:[],entity_spans:[[8,14]]},feedback:0,index:0},h={id:"12345",query:"What is Verba?",messages:[{type:"User",id:"34bbd9cc-dfde-45a2-bdfb-54b602ff4813",query_id:"b7d4b911-fbf4-42cd-8ce0-b4488d43f95a",conversation_id:"1ed93c26-dfe7-4a82-924b-bbfcac7abe9c",payload:{type:"text",metadata:{},code:{language:"",title:"",text:""},objects:["Give me  a summary of the verba tickets :)"]}},{type:"text",conversation_id:"1ed93c26-dfe7-4a82-924b-bbfcac7abe9c",query_id:"b7d4b911-fbf4-42cd-8ce0-b4488d43f95a",id:"tex-3d88ef9d-bb91-4fad-b644-31842a5c2391",payload:{type:"response",metadata:{},objects:[{text:'I am going to perform a search to gather the necessary information about the "verba tickets" for summarization.'}]}},{type:"text",conversation_id:"1ed93c26-dfe7-4a82-924b-bbfcac7abe9c",query_id:"b7d4b911-fbf4-42cd-8ce0-b4488d43f95a",id:"tex-41397142-4172-4eb2-8567-d06bd21cecce",payload:{type:"response",metadata:{},objects:[{text:'I am going to perform a query to retrieve the relevant information about the "verba tickets" for summarization.'}]}},{type:"text",conversation_id:"1ed93c26-dfe7-4a82-924b-bbfcac7abe9c",query_id:"b7d4b911-fbf4-42cd-8ce0-b4488d43f95a",id:"tex-425469f3-c1b0-4494-b99e-228b95f1fd2a",payload:{type:"response",metadata:{},objects:[{text:"I'll be querying the Verba GitHub issues to provide you with a summary of the tickets."}]}},{type:"text",conversation_id:"1ed93c26-dfe7-4a82-924b-bbfcac7abe9c",query_id:"b7d4b911-fbf4-42cd-8ce0-b4488d43f95a",id:"tex-1e1617d4-99e1-4c5e-a1f0-29c997296baf",payload:{type:"response",metadata:{},objects:[{text:"I've now created concise summaries for each ticket, highlighting key details like title, author, status, and a brief description."}]}},{type:"result",conversation_id:"1ed93c26-dfe7-4a82-924b-bbfcac7abe9c",query_id:"b7d4b911-fbf4-42cd-8ce0-b4488d43f95a",id:"res-d158e922-f548-4d77-b641-bba4981d1543",payload:{type:"ticket",objects:[{created_at:"2024-08-09T00:22:19Z",content:"instead of using the frontend ??? thanks!",tags:["enhancement"],id:2456876167,url:"https://github.com/weaviate/Verba/issues/254",comments:3,title:"can i use verba via api ?",subtitle:"",status:"open",author:"michabbb",updated_at:"2024-09-13T03:06:16Z",uuid:"dfc712329a735e6f8d2f3b3e5fe96288",summary:'Michabbb opened issue #254 (enhancement): "can i use verba via api ?" asking about API usage.  Last updated 2024-09-13.'},{created_at:"2024-05-17T15:20:52Z",content:"NaN",tags:[],id:2303012454,url:"https://github.com/weaviate/Verba/pull/163",comments:0,title:"Fix https://github.com/weaviate/Verba/issues/162",subtitle:"",status:"closed",author:"hholtmann",updated_at:"2024-05-17T16:27:17Z",uuid:"60c0be46eb9358318ebcb189c5452215",summary:'hholtmann closed pull request #163: "Fix https://github.com/weaviate/Verba/issues/162".'},{created_at:"2024-05-20T01:52:41Z",content:"NaN",tags:[],id:2304916794,url:"https://github.com/weaviate/Verba/issues/176",comments:0,title:"N",subtitle:"",status:"closed",author:"Jannice89",updated_at:"2024-05-20T18:52:38Z",uuid:"3301494c93625e2ea39234579274fe1f",summary:'Jannice89 closed issue #176: "N".'},{created_at:"2024-09-10T21:14:41Z",content:"NaN",tags:[],id:2517845518,url:"https://github.com/weaviate/Verba/pull/281",comments:1,title:"this is v2",subtitle:"",status:"closed",author:"RyanLisse",updated_at:"2024-09-12T18:43:16Z",uuid:"4d7c71af5dc95ee5a2d1b75a7d5447c6",summary:'RyanLisse closed pull request #281: "this is v2".'},{created_at:"2024-07-03T09:31:20Z",content:"NaN",tags:[],id:2388149512,url:"https://github.com/weaviate/Verba/pull/236",comments:0,title:"Call webhook  line",subtitle:"",status:"closed",author:"namvv-miichi",updated_at:"2024-07-03T09:32:20Z",uuid:"7861436f2b605005adde8060833d6056",summary:'namvv-miichi closed pull request #236: "Call webhook  line".'},{created_at:"2024-05-19T15:28:22Z",content:"ressource -> resource",tags:[],id:2304662325,url:"https://github.com/weaviate/Verba/pull/173",comments:0,title:"docs: update README.md",subtitle:"",status:"closed",author:"eltociear",updated_at:"2024-05-19T17:06:30Z",uuid:"15b089e3004d586f9701e0c2826c9b07",summary:'eltociear closed pull request #173: "docs: update README.md" correcting "ressource" to "resource".'},{created_at:"2023-10-20T06:54:17Z",content:"NaN",tags:[],id:1953657079,url:"https://github.com/weaviate/Verba/pull/27",comments:4,title:"Remove Unnececary imports",subtitle:"",status:"closed",author:"agi-dude",updated_at:"2023-11-13T09:22:55Z",uuid:"a72ff03d1015507b87b43207e3006121",summary:'agi-dude closed pull request #27: "Remove Unnececary imports".'},{created_at:"2023-09-27T07:19:56Z",content:"Please create a Google Colab notebook to demonstrate Verba's embedding and retrieval functions!",tags:["enhancement"],id:1914862458,url:"https://github.com/weaviate/Verba/issues/21",comments:4,title:"Suggesting: A demo usage on Google Colab.",subtitle:"",status:"open",author:"mikebilly",updated_at:"2023-11-29T10:11:15Z",uuid:"9e2e7804852f5040891b31f818c85389",summary:'mikebilly opened issue #21 (enhancement): "Suggesting: A demo usage on Google Colab." requesting a Google Colab demo. Last updated 2023-11-29.'},{created_at:"2024-09-04T11:04:46Z",content:"## Description\r\nA document search functionality would be a great feature for Verba.\r\nThe goal is to find relevant documents based on a text input.\r\n## Additional context\r\nhttps://qatalog.com/product/\r\nhttps://dash.dropbox.com/features/universal-search\r",tags:["enhancement"],id:2505018612,url:"https://github.com/weaviate/Verba/issues/269",comments:1,title:"Document search",subtitle:"",status:"closed",author:"danielhangan",updated_at:"2024-09-13T02:55:58Z",uuid:"b839b403f21d50db9e21a9014e6e59e6",summary:'danielhangan closed issue #269 (enhancement): "Document search" suggesting a document search feature. Last updated 2024-09-13.'},{created_at:"2024-07-16T12:29:20Z",content:"Test1",tags:[],id:2411016751,url:"https://github.com/weaviate/Verba/pull/245",comments:0,title:"Update index.html",subtitle:"",status:"closed",author:"javegasainz10",updated_at:"2024-07-16T20:58:24Z",uuid:"d25ff770cf8d5f3c8e6531c9e7715605",summary:'javegasainz10 closed pull request #245: "Update index.html".'}],metadata:{collection_name:"example_verba_github_issues",return_type:"ticket",output_type:"summary",query_text:"verba tickets",query_type:"semantic",code:{language:"python",title:"Query",text:'collection.query.near_text(query="verba tickets", limit=10)'}},code:{language:"python",title:"Query",text:'collection.query.near_text(query="verba tickets", limit=10)'}}},{type:"text",conversation_id:"1ed93c26-dfe7-4a82-924b-bbfcac7abe9c",query_id:"b7d4b911-fbf4-42cd-8ce0-b4488d43f95a",id:"tex-d46683dd-ff32-4c2b-8bb9-79ddf359d6d6",payload:{type:"summary",metadata:{},objects:[{text:"The Verba project has several active and closed tickets on GitHub, reflecting ongoing development and user contributions. Notable open issues include a request for API usage (issue #254) and a suggestion for a Google Colab demo (issue #21). Closed tickets include enhancements like a document search feature (issue #269) and various pull requests addressing minor updates and fixes. The tickets indicate a vibrant community actively engaging with the project, contributing ideas, and resolving issues.",title:"Overview of Verba Tickets"}]}}],finished:!0,query_start:new Date,query_end:new Date(new Date().getTime()+1e3),NER:{text:"What is Verba?",noun_spans:[],entity_spans:[[8,13]]},feedback:0,index:0},p={id:"12345",query:"What is Verba?",messages:[{type:"User",id:"8ae36566-66e6-4cef-baeb-d20b6aac4f5d",query_id:"711c2774-f296-4762-935b-d653062732ae",conversation_id:"0d750b45-9c13-4330-9a19-dde52fa36312",payload:{type:"text",metadata:{},code:{language:"",title:"",text:""},objects:["I'm looking for pink clothing items"]}},{type:"text",conversation_id:"0d750b45-9c13-4330-9a19-dde52fa36312",query_id:"711c2774-f296-4762-935b-d653062732ae",id:"tex-f22cfdb7-7cd4-4397-95c7-747f5dfd7b66",payload:{type:"response",metadata:{},objects:[{text:"I'm currently querying our ecommerce database to find all the pink clothing items available."}]}},{type:"text",conversation_id:"0d750b45-9c13-4330-9a19-dde52fa36312",query_id:"711c2774-f296-4762-935b-d653062732ae",id:"tex-17451ec2-f81d-4037-bc44-786325ef1aca",payload:{type:"response",metadata:{},objects:[{text:"I'm using a filter to specifically find items where pink is one of the available colors."}]}},{type:"result",conversation_id:"0d750b45-9c13-4330-9a19-dde52fa36312",query_id:"711c2774-f296-4762-935b-d653062732ae",id:"res-70d1ae5c-8e03-4c96-9d1b-79e05710cf26",payload:{type:"ecommerce",objects:[{description:"Embrace the charm of country living with this delightful tote. Crafted from soft, durable fabric, it's adorned with a soft pink hue and features a delicately embroidered cream floral pattern. Perfect for carrying your market finds or picnic necessities.",tags:[],collection:"Cottagecore",id:"001970d1-ae8b-4cd6-bdf9-25e61c3eeae4",url:"",price:58,rating:4.8,brand:"Canvas & Co.",subcategory:"Bags & Totes",name:"Garden Tea Party Tote",reviews:["Absolutely beautiful! Carries all my essentials for a sunny day out in the fields.","Sturdy and pretty, gets compliments everywhere I go!"],image:"https://d3o574pyao1sq3.cloudfront.net/fashion/001970d1-ae8b-4cd6-bdf9-25e61c3eeae4.png",category:"Accessories",colors:["pink","cream"],sizes:"",uuid:"eb8df81313195af283564da520c116e4",summary:""},{description:"Embrace your love for the countryside with our soft, hand-stitched loafers, perfect for quiet walks through the garden. Crafted with eco-friendly dyed soft pink leather and adorned with a subtle leaf embossing, these shoes are a testament to the beauty of understated simplicity.",tags:[],collection:"Cottagecore",id:"7860533c-7036-4268-9121-e98278b844b8",url:"",price:75,rating:4.8,brand:"Eko & Stitch",subcategory:"Dress Shoes",name:"Bramble Berry Loafers",reviews:["The perfect blend of comfort and beauty. I feel like I'm walking on air!","Absolutely love the subtle design, goes well with all my countryside outfits.","High-quality and sustainable, exactly what I was looking for."],image:"https://d3o574pyao1sq3.cloudfront.net/fashion/7860533c-7036-4268-9121-e98278b844b8.png",category:"Footwear",colors:["pink","green"],sizes:"",uuid:"060eaccd9ece533cac2b4aedb2fcb2ac",summary:""},{description:"Delve into a world of whimsy with this delicate blouse, featuring soft flutters of fabric and subtle hints of pastel blue and pink. Perfect for those who find magic in the details.",tags:["most loved"],collection:"Fairycore",id:"1acbf0f0-20e0-4a45-ac0e-dcb3f55452b8",url:"",price:58,rating:4.5,brand:"Nova Nest",subcategory:"Blouses & Shirts",name:"Blue Breeze Blouse",reviews:["Feels like a dream when worn. Soft and ethereal!","Perfect for my garden tea party. Got so many compliments."],image:"https://d3o574pyao1sq3.cloudfront.net/fashion/1acbf0f0-20e0-4a45-ac0e-dcb3f55452b8.png",category:"Tops",colors:["blue","pink"],sizes:"",uuid:"70add9bb062856f6957258a4a45110eb",summary:""},{description:"Delicately tailored for a touch of the magical and whimsical, these pants capture the essence of untold tales and mythical whispers. Crafted with light and airy fabric, they're dyed in shades of lavender and soft pink, featuring subtle floral embroidery that dances with every step.",tags:[],collection:"Cottagecore",id:"999bcc62-d106-4939-a727-7aaeb6687042",url:"",price:79.99,rating:4.7,brand:"Canvas & Co.",subcategory:"Jeans",name:"Pixie Dust Denims",reviews:["Absolutely enchanted by these jeans! They fit like a dream and feel so magical.","The embroidery detail is just exquisite. I feel like I'm in a fairytale!"],image:"https://d3o574pyao1sq3.cloudfront.net/fashion/999bcc62-d106-4939-a727-7aaeb6687042.png",category:"Bottoms",colors:["purple","pink"],sizes:"",uuid:"3ebe8886b1365dbc95833bc878bd6608",summary:""},{description:"Add a touch of early 2000s glamour to your wardrobe with this shimmering metallic cardigan. Featuring a delicate pink hue and a comfortable, figure-hugging fit, it's perfect for layering over your favorite mini skirt or low-rise jeans.",tags:[],collection:"Y2K",id:"3405575a-c932-4b3e-8040-e85ee6f2823d",url:"",price:45.99,rating:4.5,brand:"Vivid Verse",subcategory:"Sweaters & Cardigans",name:"Metallic Shimmer Cardigan",reviews:["Absolutely obsessed with the sparkle! Feels like I'm back in 2001.","Perfect fit and the color is exactly what I was looking for."],image:"https://d3o574pyao1sq3.cloudfront.net/fashion/3405575a-c932-4b3e-8040-e85ee6f2823d.png",category:"Outerwear",colors:["pink","silver"],sizes:"",uuid:"32a6dbf6ec445140a50b80d855b6316e",summary:""},{description:"Embrace the dawn of pastel hues with these ultra-comfy low-rise jeans, featuring a soft pink wash that's sure to brighten up your day.",tags:[],collection:"Y2K",id:"44936295-1af5-4440-ae21-09b144b6b8bf",url:"",price:58.99,rating:4.5,brand:"Vivid Verse",subcategory:"Jeans",name:"Pastel Dawn Jeans",reviews:["Super comfy and the color is just adorable!","Love the fit, feels like a throwback to a sunnier time."],image:"https://d3o574pyao1sq3.cloudfront.net/fashion/44936295-1af5-4440-ae21-09b144b6b8bf.png",category:"Bottoms",colors:["pink"],sizes:"",uuid:"b48e55a7987b5b75af660c91a6760dde",summary:""},{description:"Channel the vibe of early millennial fashion with this glossy pastel pink tank. Perfect for expressing your vibrant, playful side while keeping it effortlessly chic.",tags:["sale","most loved"],collection:"Y2K",id:"d99464b4-9bc9-4271-ae77-b502d1c696e6",url:"",price:24.99,rating:4.5,brand:"Vivid Verse",subcategory:"Tank Tops",name:"Bubblegum Bliss Tank",reviews:["Absolutely LOVE! Takes me right back but still feels fresh.","The color is perfect, just what I was looking for."],image:"https://d3o574pyao1sq3.cloudfront.net/fashion/d99464b4-9bc9-4271-ae77-b502d1c696e6.png",category:"Tops",colors:["pink"],sizes:"",uuid:"6c482ae93fec5050800b445930bf5a26",summary:""},{description:"Embrace early millennium vibes with this shiny pastel cardigan. Features a metallic sheen over baby blue and soft pink hues, perfect for adding a pop of color and nostalgia to any outfit.",tags:["most loved"],collection:"Y2K",id:"adde1e4f-69c4-4d52-87c2-860df3f9f99d",url:"",price:48.99,rating:4.5,brand:"Vivid Verse",subcategory:"Sweaters & Cardigans",name:"Metallic Pastel Dream Cardigan",reviews:["Absolutely in love with the colors and fit!","Brings back so many memories, plus it's super comfy."],image:"https://d3o574pyao1sq3.cloudfront.net/fashion/adde1e4f-69c4-4d52-87c2-860df3f9f99d.png",category:"Outerwear",colors:["blue","pink"],sizes:"",uuid:"b025754d51d959948d48938420bfd2ae",summary:""},{description:"A soft, stretchy cotton tee in pastel pink, adorned with a playful sprinkle of baby blue cloud graphics. Perfect for a casual, whimsical look.",tags:[],collection:"Y2K",id:"5286c4fa-e3a9-42ef-8e14-7d5e336b15af",url:"",price:25.99,rating:4.5,brand:"Vivid Verse",subcategory:"T-Shirts",name:"Candy Cloud Baby Tee",reviews:["Absolutely adorable! Feels like a throwback to my favorite era.","Soft fabric and the fit is just right. Love the pastel colors!"],image:"https://d3o574pyao1sq3.cloudfront.net/fashion/5286c4fa-e3a9-42ef-8e14-7d5e336b15af.png",category:"Tops",colors:["pink","blue"],sizes:"",uuid:"8a1f3dfb7ec65d248df0e532d9b28419",summary:""},{description:"Embrace the charm of early turn-of-the-century vibes with our soft cotton tee, featuring a vibrant pastel pink hue. Accentuated with a delicate baby blue trim, this tee is the epitome of throwback flair.",tags:[],collection:"Y2K",id:"dc3b967a-ef81-4f62-a3bc-67ecf2f07a0c",url:"",price:29.99,rating:4.8,brand:"Nova Nest",subcategory:"T-Shirts",name:"Pastel Prism Baby Tee",reviews:["Absolutely adore the nostalgic feel!","The colors are so vibrant. Love it!"],image:"https://d3o574pyao1sq3.cloudfront.net/fashion/dc3b967a-ef81-4f62-a3bc-67ecf2f07a0c.png",category:"Tops",colors:["pink","blue"],sizes:"",uuid:"ae6c39e837da5412b7f9ac3b0a43ebc8",summary:""}],metadata:{collection_name:"ecommerce",return_type:"ecommerce",output_type:"original",query_text:null,query_type:"fetch",code:{language:"python",title:"Query",text:'collection.query.fetch_objects(filters=Filter.by_property("colors").contains_any(["pink"]), limit=10)'}},code:{language:"python",title:"Query",text:'collection.query.fetch_objects(filters=Filter.by_property("colors").contains_any(["pink"]), limit=10)'}}},{type:"text",conversation_id:"0d750b45-9c13-4330-9a19-dde52fa36312",query_id:"711c2774-f296-4762-935b-d653062732ae",id:"tex-2d398b49-e1bf-4d36-a237-db9b6135cf71",payload:{type:"summary",metadata:{},objects:[{text:'Here\'s a summary of pink clothing items currently available:\n\n**Tops:** We have several pink tops, including the "Blue Breeze Blouse" (Fairycore collection, features pastel blue and pink), the "Bubblegum Bliss Tank" (Y2K collection, glossy pastel pink), and two pink and blue "baby tees" from Vivid Verse and Nova Nest, both with Y2K styles.\n\n**Bottoms:**  Pink bottoms include the "Pixie Dust Denims" (Cottagecore collection, lavender and pink with floral embroidery) and the "Pastel Dawn Jeans" (Y2K collection, soft pink wash).\n\n**Outerwear:**  Pink outerwear options include the "Metallic Shimmer Cardigan" (Y2K collection, delicate pink with a metallic sheen) and the "Metallic Pastel Dream Cardigan" (Y2K collection, baby blue and soft pink).\n\n**Footwear:** The "Bramble Berry Loafers" (Cottagecore collection) are available in pink and green, featuring eco-friendly leather.\n\n**Accessories:** The "Garden Tea Party Tote" (Cottagecore collection) is a soft pink tote bag with a cream floral pattern.\n\nCustomer reviews are generally positive, praising the comfort, style, and quality of the items.  Each item\'s specific details, including brand, price, and image, are available for your review.',title:"Pink Clothing Items Available"}]}}],finished:!0,query_start:new Date,query_end:new Date(new Date().getTime()+1e3),NER:{text:"What is Verba?",noun_spans:[],entity_spans:[[8,13]]},feedback:0,index:0},m={id:"12345",query:"What is Verba?",messages:[{type:"User",id:"1e15fdb3-2cc0-4e63-95d5-db626838aad2",query_id:"2238e46c-396c-4863-b46d-f319e5f56f8b",conversation_id:"ded92d5c-199c-451e-b8c0-49a372b084a0",payload:{type:"text",metadata:{},code:{language:"",title:"",text:""},objects:["Are there any email conversations about pdf ingestion in verba?"]}},{type:"text",conversation_id:"ded92d5c-199c-451e-b8c0-49a372b084a0",query_id:"2238e46c-396c-4863-b46d-f319e5f56f8b",id:"tex-96d96d83-5b32-4d26-b000-e51ccc1cacf3",payload:{type:"response",metadata:{},objects:[{text:'I am now searching the "example_verba_email_chains" collection for mentions of "pdf ingestion".'}]}},{type:"text",conversation_id:"ded92d5c-199c-451e-b8c0-49a372b084a0",query_id:"2238e46c-396c-4863-b46d-f319e5f56f8b",id:"tex-6e160c11-94a1-45b1-8ff3-2669ec1131dd",payload:{type:"response",metadata:{},objects:[{text:"I've selected the 'query' function to directly search for relevant email conversations containing the keywords \"pdf ingestion.\""}]}},{type:"text",conversation_id:"ded92d5c-199c-451e-b8c0-49a372b084a0",query_id:"2238e46c-396c-4863-b46d-f319e5f56f8b",id:"tex-cfc42c47-451b-47a8-bd7c-79f50ad4a5be",payload:{type:"response",metadata:{},objects:[{text:"I'm using a hybrid search to find the most relevant conversations about PDF ingestion in Verba, which will combine both semantic and keyword matching for better results."}]}},{type:"result",conversation_id:"ded92d5c-199c-451e-b8c0-49a372b084a0",query_id:"2238e46c-396c-4863-b46d-f319e5f56f8b",id:"res-6dd791f2-c9cd-4515-ace4-68fc76285bd2",payload:{type:"conversation",metadata:{collection_name:"example_verba_email_chains",return_type:"conversation",output_type:"original",query_text:"pdf ingestion",query_type:"hybrid",code:{language:"python",title:"Query",text:'collection.query.hybrid(\n    query="pdf ingestion",\n    limit=5\n)'}},objects:[{messages:[{conversation_id:394,content:"Hello Team,\n\nWe are excited to announce a new update to our data ingestion tool! The latest version includes improved error handling and better schema management capabilities. Make sure to check out the new documentation for best practices on setting up your client.\n\nBest regards,\nKerrigan",author:"Kerrigan",timestamp:"2023-10-10 09:15:00",message_id:0,uuid:"6c648d0564dc5bddbdb741aa2af054a8",relevant:!1},{conversation_id:394,content:"Hi Everyone,\n\nI wanted to share some tips on optimizing your data ingestion process. Remember to validate your files before loading them into the system to avoid potential hiccups. Our updated guide covers these points in detail!\n\nKind regards,\nDanny",author:"Danny",timestamp:"2023-10-10 10:45:32",message_id:1,uuid:"4c802cac4c8c5480aecbe28e34a6c154",relevant:!0},{conversation_id:394,content:"Dear Users,\n\nHave you explored our new features yet? The recent release also allows for batch processing of files, greatly enhancing efficiency. Don't miss out on utilizing these capabilities in your workflows.\n\nRegards,\nRavi",author:"Ravi",timestamp:"2023-10-10 12:30:47",message_id:2,uuid:"38bb6391c816518f98f010bb57d05e1e",relevant:!1},{conversation_id:394,content:"Hello All,\n\nJust a friendly reminder to regularly check for updates. Our team is constantly working to enhance the tool and your feedback is invaluable in this process. Let us know how we can assist you further!\n\nBest,\nKerrigan",author:"Kerrigan",timestamp:"2023-10-10 14:05:12",message_id:3,uuid:"de2d66df777550fdb79cec7697ad6bd7",relevant:!1},{conversation_id:394,content:"Hi Team,\n\nAs we continue to enhance our data processing tool, we encourage you to share your experiences and suggestions. Collaboration is key to our success!\n\nKind regards,\nDanny",author:"Danny",timestamp:"2023-10-10 15:30:25",message_id:4,uuid:"619e95978cc6531f9c3150f1e8f20a62",relevant:!1},{conversation_id:394,content:"Dear Team,\n\nWe appreciate your continued support! Stay tuned for our upcoming webinar where we will dive deeper into the new features and answer any questions you may have. Looking forward to seeing you there!\n\nWarm regards,\nRavi",author:"Ravi",timestamp:"2023-10-10 16:45:58",message_id:5,uuid:"d477eb70acee50e4a7af8b2ee0636abc",relevant:!1}],conversation_id:394},{messages:[{conversation_id:367,content:"Hello Support Team,\n\nI hope this message finds you well. I recently started using your PDFReader feature and had a couple of questions regarding its functionalities. Could you please clarify if there are any limitations on the file size for the PDFs I can upload?\n\nThank you for your assistance!\n\nBest regards,\nKaladin",author:"Kaladin",timestamp:"2023-10-01 09:15:23",message_id:0,uuid:"172e0ca06d8f50f1b99ac23834656213",relevant:!1},{conversation_id:367,content:"Dear Kaladin,\n\nThank you for reaching out to us! I'm glad to hear that you are using the PDFReader feature. There are no strict limitations on the file size, but we recommend keeping it under 100MB for optimal performance. If you encounter any issues with larger files, feel free to let us know.\n\nBest,\nDalinar",author:"Dalinar",timestamp:"2023-10-01 09:40:56",message_id:1,uuid:"d5678a488b4b5d39a01e3e496610f107",relevant:!1},{conversation_id:367,content:"Hi Dalinar,\n\nThank you for the quick response! That makes sense. I also wanted to ask if there are any specific formats or types of PDFs that the PDFReader struggles with?\n\nLooking forward to your reply.\n\nRegards,\nKaladin",author:"Kaladin",timestamp:"2023-10-01 10:02:14",message_id:2,uuid:"2c622bda4814500d8ad69d75c4d2f459",relevant:!1},{conversation_id:367,content:"Hello Kaladin,\n\nI appreciate your questions. The PDFReader is designed to handle most standard PDF formats without issues. However, very complex PDFs with heavy graphics or unusual encodings might occasionally lead to performance hiccups. If you experience any such issues, please let us know, and we can help troubleshoot!\n\nKind regards,\nRavi",author:"Ravi",timestamp:"2023-10-01 10:30:42",message_id:3,uuid:"ccb78b4a8e1656c99c815a0a6aa139ba",relevant:!0},{conversation_id:367,content:"Dear Ravi,\n\nThanks for the information! That’s helpful to know. I also noticed a feature that allows for chunking the PDF content. Can you explain how that works, and if there are any settings I should be aware of?\n\nBest wishes,\nKaladin",author:"Kaladin",timestamp:"2023-10-01 11:05:11",message_id:4,uuid:"f388a36d325758b7ba9b903ee1a4d1a9",relevant:!1},{conversation_id:367,content:"Hi Kaladin,\n\nGreat question! The TokenChunker feature automatically breaks down the text into manageable chunks for processing. You don’t need to adjust any settings; it works seamlessly in the background. But if you want to modify how the chunking is done, we can provide customization options upon request.\n\nRegards,\nDalinar",author:"Dalinar",timestamp:"2023-10-01 12:00:29",message_id:5,uuid:"94cdca4f66e05f31a5cd40c49430fd2a",relevant:!1},{conversation_id:367,content:"Hello Dalinar,\n\nThank you for the clarification! I appreciate the support. Lastly, are there any upcoming features or updates planned for the PDFReader that I should know about?\n\nThanks again!\nKaladin",author:"Kaladin",timestamp:"2023-10-01 13:15:48",message_id:6,uuid:"f92e6bc9f62a52428319a76836808b96",relevant:!1},{conversation_id:367,content:"Dear Kaladin,\n\nWe’re glad to hear that you're enjoying the PDFReader! We have a few enhancements in the pipeline, including improved error handling and additional format support, which we hope to roll out soon. Stay tuned for updates!\n\nBest regards,\nRavi",author:"Ravi",timestamp:"2023-10-01 14:30:05",message_id:7,uuid:"c524f1564c93563b9a8ab8aa7dadd78b",relevant:!1},{conversation_id:367,content:"Hi Ravi,\n\nThat sounds fantastic! I look forward to those updates. Thank you for all the help and information you've provided. It has really enhanced my experience with your product.\n\nWarm regards,\nKaladin",author:"Kaladin",timestamp:"2023-10-01 15:45:12",message_id:8,uuid:"2b23c6f40ffa5241939eb6b5e8b3d709",relevant:!1}],conversation_id:367},{messages:[{conversation_id:413,content:"Hi Support Team,\n\nI hope this message finds you well. I have been using your product for a while now, and I wanted to inquire about the data import features. Is there a way to streamline the process for importing large datasets? \n\nThank you for your assistance!\n\nBest regards,\nJaina",author:"Jaina",timestamp:"2023-10-01 09:15:32",message_id:0,uuid:"0ad9401803e85291b4526ae5452c0891",relevant:!1},{conversation_id:413,content:"Hello Jaina,\n\nThank you for reaching out! We appreciate your feedback. Currently, our product allows for batch imports, but we understand the need for more flexibility. We are working on features to allow for more efficient imports, including incremental data ingestion.\n\nIf you have any specific requirements or suggestions, please feel free to share!\n\nKind regards,\nFatima",author:"Fatima",timestamp:"2023-10-01 09:49:02",message_id:1,uuid:"a2344a6e16e85c3da6fff4330e1ec4cd",relevant:!0},{conversation_id:413,content:"Hi Fatima,\n\nThank you for your prompt response! Incremental ingestion sounds like a great improvement. Could you provide an estimated timeline for when this feature might be available? It would help me plan my data workflows better.\n\nLooking forward to your reply.\n\nBest,\nJaina",author:"Jaina",timestamp:"2023-10-01 10:05:18",message_id:2,uuid:"8140732c8a3650eb9042685b1b5d0d59",relevant:!1},{conversation_id:413,content:"Dear Jaina,\n\nThank you for your interest in our upcoming features! While I can't provide an exact timeline just yet, I can confirm that our development team is actively working on this enhancement. We aim to make it available in the next few months. I'll be sure to keep you updated on our progress.\n\nBest wishes,\nZara",author:"Zara",timestamp:"2023-10-01 11:14:47",message_id:3,uuid:"be64756a59045bee9dda114bf9352609",relevant:!1},{conversation_id:413,content:"Hello Zara,\n\nI appreciate the update! I understand that these developments take time. In the meantime, are there any workarounds you could suggest for optimizing my current data import processes? \n\nThank you for your help!\n\nWarm regards,\nJaina",author:"Jaina",timestamp:"2023-10-01 12:30:21",message_id:4,uuid:"292b71fac1ec5bbc80820959ce97af8a",relevant:!1},{conversation_id:413,content:"Hi Jaina,\n\nGreat question! One workaround could be to segment your data into smaller batches. This way, you can manage imports more efficiently while we finalize the incremental ingestion feature. Additionally, consider using our API for more control over the import process.\n\nLet me know if you need further assistance!\n\nBest,\nFatima",author:"Fatima",timestamp:"2023-10-01 13:45:33",message_id:5,uuid:"c2c0911863725980a869812ac558044d",relevant:!1},{conversation_id:413,content:"Hi Fatima,\n\nThank you for the suggestions! I'll start implementing those changes right away. It's reassuring to know that the incremental ingestion feature is on the way. \n\nIs there a way to receive notifications on updates regarding this feature?\n\nThanks again!\n\nBest,\nJaina",author:"Jaina",timestamp:"2023-10-01 14:02:55",message_id:6,uuid:"1862a3220bf0513f9c261594c99c2d1e",relevant:!1},{conversation_id:413,content:"Hello Jaina,\n\nYes, absolutely! We can add you to our mailing list for feature updates. You will receive notifications as soon as we have news regarding the incremental ingestion feature or any other relevant updates.\n\nBest regards,\nZara",author:"Zara",timestamp:"2023-10-01 14:30:12",message_id:7,uuid:"c36117796b815fc6b95a14014b8c1cf1",relevant:!1},{conversation_id:413,content:"Hi Zara,\n\nThank you for adding me to the list! I’m looking forward to the updates. Your support has been fantastic, and I appreciate your team's responsiveness. \n\nHave a great day!\n\nWarmest regards,\nJaina",author:"Jaina",timestamp:"2023-10-01 15:15:44",message_id:8,uuid:"0f85f72bde0f55a4b062f882fa079f5c",relevant:!1},{conversation_id:413,content:"Hi Jaina,\n\nThank you for your kind words! We're here to help, and we value your input. If you have any more questions or feedback, please don't hesitate to reach out. \n\nWishing you a productive day ahead!\n\nKind regards,\nFatima",author:"Fatima",timestamp:"2023-10-01 16:00:19",message_id:9,uuid:"37ec61396db95b71b60ecfc81cfe45a8",relevant:!1}],conversation_id:413},{messages:[{conversation_id:263,content:"Dear Support Team,\n\nI hope this message finds you well. I am currently exploring the features of your product and have some questions regarding its capabilities. Specifically, I would like to know if there are any limitations on file sizes for imports. I recently attempted to import a large PDF and encountered some difficulties. \n\nLooking forward to your guidance.\n\nBest regards,\nJohn",author:"John",timestamp:"2023-10-03 09:15:30",message_id:0,uuid:"b75db827654f550ab914b1e3b5a51253",relevant:!1},{conversation_id:263,content:"Hello John,\n\nThank you for reaching out! We appreciate your interest in our product. Regarding your question, there are indeed some size limitations based on the method of import. However, I would be more than happy to assist you in troubleshooting the issue you faced. Could you please provide more details about the file and the method you used?\n\nKind regards,\nFatima",author:"Fatima",timestamp:"2023-10-03 10:35:45",message_id:1,uuid:"7d0f510fd8c45d5f8091b17abd5639b0",relevant:!1},{conversation_id:263,content:"Hi Fatima,\n\nThank you for your prompt response. The PDF file I was trying to import is approximately 7.7MB. I used the Token Chunker feature with settings optimized for larger files. Despite following the setup instructions, I encountered a chunk mismatch error. Any suggestions on how to proceed?\n\nThanks again!\n\nBest,\nJohn",author:"John",timestamp:"2023-10-03 11:50:12",message_id:2,uuid:"9c0f2fb8c18f5873b7a3555c45cec8b5",relevant:!0},{conversation_id:263,content:"Dear John,\n\nThank you for the additional information. We recommend checking the specific settings for your chunking parameters. Sometimes, adjusting the overlap can yield better results. Additionally, our team is working on enhancing the import capabilities in upcoming updates, which should address such issues more effectively. \n\nPlease let us know if this helps or if you need further assistance.\n\nRegards,\nDanny",author:"Danny",timestamp:"2023-10-03 13:20:56",message_id:3,uuid:"451574de3b5d50f0b4d31d46b7748c4e",relevant:!1},{conversation_id:263,content:"Hi Danny,\n\nI appreciate your suggestions and will try adjusting the overlap settings. I’m also interested to hear more about the upcoming updates you mentioned. Are there any specific features we should be looking forward to that might improve our experience with file imports?\n\nThank you!\n\nSincerely,\nJohn",author:"John",timestamp:"2023-10-03 15:05:24",message_id:4,uuid:"b4805b10f2275b3bb80b143344789b8a",relevant:!1},{conversation_id:263,content:"Hello John,\n\nGreat to hear you’re willing to try the adjustments! Regarding the updates, we are planning to enhance not only the import capabilities but also the overall performance and user interface. We will definitely keep you informed as we roll out new features. \n\nStay tuned!\n\nBest,\nFatima",author:"Fatima",timestamp:"2023-10-03 16:45:37",message_id:5,uuid:"db9c4548a0fc5db592975a576ba49d05",relevant:!1},{conversation_id:263,content:"Hi Fatima,\n\nThat sounds exciting! I look forward to trying out the new features once they are released. In the meantime, if I encounter any more issues, I will reach out for assistance. Thank you for your support!\n\nBest regards,\nJohn",author:"John",timestamp:"2023-10-03 18:15:00",message_id:6,uuid:"8cd0ab58e296510b871371fb972d2097",relevant:!1},{conversation_id:263,content:"Dear John,\n\nWe’re here to help anytime! Don’t hesitate to reach out with any further questions or feedback. Your experience is very important to us, and we value your insights.\n\nHave a wonderful day!\n\nKind regards,\nDanny",author:"Danny",timestamp:"2023-10-03 19:30:10",message_id:7,uuid:"1305cce96971551aad8cd6499776cd8e",relevant:!1},{conversation_id:263,content:"Hello John,\n\nJust a quick note to say that we are always eager to hear your thoughts. Your feedback is crucial as we continue to improve our services. Thank you for being a valued customer!\n\nWarm regards,\nFatima",author:"Fatima",timestamp:"2023-10-03 21:00:45",message_id:8,uuid:"f817b7c41f73552dadd35476b2a36c4f",relevant:!1}],conversation_id:263},{messages:[{conversation_id:413,content:"Hi Support Team,\n\nI hope this message finds you well. I have been using your product for a while now, and I wanted to inquire about the data import features. Is there a way to streamline the process for importing large datasets? \n\nThank you for your assistance!\n\nBest regards,\nJaina",author:"Jaina",timestamp:"2023-10-01 09:15:32",message_id:0,uuid:"0ad9401803e85291b4526ae5452c0891",relevant:!1},{conversation_id:413,content:"Hello Jaina,\n\nThank you for reaching out! We appreciate your feedback. Currently, our product allows for batch imports, but we understand the need for more flexibility. We are working on features to allow for more efficient imports, including incremental data ingestion.\n\nIf you have any specific requirements or suggestions, please feel free to share!\n\nKind regards,\nFatima",author:"Fatima",timestamp:"2023-10-01 09:49:02",message_id:1,uuid:"a2344a6e16e85c3da6fff4330e1ec4cd",relevant:!1},{conversation_id:413,content:"Hi Fatima,\n\nThank you for your prompt response! Incremental ingestion sounds like a great improvement. Could you provide an estimated timeline for when this feature might be available? It would help me plan my data workflows better.\n\nLooking forward to your reply.\n\nBest,\nJaina",author:"Jaina",timestamp:"2023-10-01 10:05:18",message_id:2,uuid:"8140732c8a3650eb9042685b1b5d0d59",relevant:!1},{conversation_id:413,content:"Dear Jaina,\n\nThank you for your interest in our upcoming features! While I can't provide an exact timeline just yet, I can confirm that our development team is actively working on this enhancement. We aim to make it available in the next few months. I'll be sure to keep you updated on our progress.\n\nBest wishes,\nZara",author:"Zara",timestamp:"2023-10-01 11:14:47",message_id:3,uuid:"be64756a59045bee9dda114bf9352609",relevant:!1},{conversation_id:413,content:"Hello Zara,\n\nI appreciate the update! I understand that these developments take time. In the meantime, are there any workarounds you could suggest for optimizing my current data import processes? \n\nThank you for your help!\n\nWarm regards,\nJaina",author:"Jaina",timestamp:"2023-10-01 12:30:21",message_id:4,uuid:"292b71fac1ec5bbc80820959ce97af8a",relevant:!1},{conversation_id:413,content:"Hi Jaina,\n\nGreat question! One workaround could be to segment your data into smaller batches. This way, you can manage imports more efficiently while we finalize the incremental ingestion feature. Additionally, consider using our API for more control over the import process.\n\nLet me know if you need further assistance!\n\nBest,\nFatima",author:"Fatima",timestamp:"2023-10-01 13:45:33",message_id:5,uuid:"c2c0911863725980a869812ac558044d",relevant:!0},{conversation_id:413,content:"Hi Fatima,\n\nThank you for the suggestions! I'll start implementing those changes right away. It's reassuring to know that the incremental ingestion feature is on the way. \n\nIs there a way to receive notifications on updates regarding this feature?\n\nThanks again!\n\nBest,\nJaina",author:"Jaina",timestamp:"2023-10-01 14:02:55",message_id:6,uuid:"1862a3220bf0513f9c261594c99c2d1e",relevant:!1},{conversation_id:413,content:"Hello Jaina,\n\nYes, absolutely! We can add you to our mailing list for feature updates. You will receive notifications as soon as we have news regarding the incremental ingestion feature or any other relevant updates.\n\nBest regards,\nZara",author:"Zara",timestamp:"2023-10-01 14:30:12",message_id:7,uuid:"c36117796b815fc6b95a14014b8c1cf1",relevant:!1},{conversation_id:413,content:"Hi Zara,\n\nThank you for adding me to the list! I’m looking forward to the updates. Your support has been fantastic, and I appreciate your team's responsiveness. \n\nHave a great day!\n\nWarmest regards,\nJaina",author:"Jaina",timestamp:"2023-10-01 15:15:44",message_id:8,uuid:"0f85f72bde0f55a4b062f882fa079f5c",relevant:!1},{conversation_id:413,content:"Hi Jaina,\n\nThank you for your kind words! We're here to help, and we value your input. If you have any more questions or feedback, please don't hesitate to reach out. \n\nWishing you a productive day ahead!\n\nKind regards,\nFatima",author:"Fatima",timestamp:"2023-10-01 16:00:19",message_id:9,uuid:"37ec61396db95b71b60ecfc81cfe45a8",relevant:!1}],conversation_id:413}],code:{language:"python",title:"Query",text:'collection.query.hybrid(\n    query="pdf ingestion",\n    limit=5\n)'}}},{type:"text",conversation_id:"ded92d5c-199c-451e-b8c0-49a372b084a0",query_id:"2238e46c-396c-4863-b46d-f319e5f56f8b",id:"tex-de989adf-031c-430c-9bbd-db975f69d9bc",payload:{type:"summary",metadata:{},objects:[{text:'I\'ve reviewed several email conversations related to PDF ingestion within Verba.  Some conversations discuss general data ingestion updates and improvements, but others directly address user questions and issues concerning PDF handling.  One user reported a "chunk mismatch error" when importing a large PDF using the TokenChunker feature.  Support staff suggested checking the chunking parameters, specifically the overlap settings, as a potential solution.  Additionally, ongoing development efforts are focused on improving import capabilities and addressing similar issues in future updates.',title:"Relevant Verba Email Conversations Regarding PDF Ingestion"}]}}],finished:!1,query_start:new Date,query_end:new Date(new Date().getTime()+1e3),NER:{text:"What is Verba?",noun_spans:[],entity_spans:[[8,13]]},feedback:1,index:0};function g(){let e=(0,d.usePathname)(),t=(0,d.useSearchParams)(),a=(0,i.useRef)(null),g=()=>{},f=()=>{},y={text_response:[s],what_is_verba:[r],initial_response:[l],who_is_mark_robson:[c],highest_wind:[u],tickets:[h],ecommerce:[p],conversation:[m]},[b,v]=(0,i.useState)([s]);return(0,i.useEffect)(()=>{setTimeout(()=>{var e;null===(e=a.current)||void 0===e||e.scrollIntoView({behavior:"smooth"})},100)},[b]),(0,i.useEffect)(()=>{let e=t.get("type");e&&e in y&&v(y[e])},[e,t]),(0,n.jsx)("div",{className:"flex flex-col w-full overflow-scroll justify-center items-center",children:(0,n.jsxs)("div",{className:"flex flex-col w-full md:w-[60vw] lg:w-[40vw] h-[90vh] ",children:[" ",Object.entries(b).sort((e,t)=>e[1].index-t[1].index).map((e,t,i)=>{let[s,r]=e;return(0,n.jsx)(o.Z,{isLastQuery:t===i.length-1,handleSendQuery:()=>{},messages:r.messages,conversationID:"12345",queryID:s,finished:r.finished,query_start:r.query_start,query_end:r.query_end,_collapsed:t!==i.length-1,messagesEndRef:a,NER:r.NER,updateNER:g,feedback:r.feedback,updateFeedback:f,addDisplacement:()=>{},addDistortion:()=>{}},s)})]})})}}},function(e){e.O(0,[6051,5452,7699,5706,6305,522,7675,6957,8180,4436,1886,5247,4881,9268,6323,9467,5593,5525,1312,2971,2117,1744],function(){return e(e.s=73560)}),_N_E=e.O()}]);