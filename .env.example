# =============================================================================
# Weaviate Configuration
# =============================================================================
# For local Docker setup, use: http://weaviate:8080
# For Weaviate Cloud, use your cluster URL
WCD_URL=http://weaviate:8080
# WCD_API_KEY=

# Set to 'true' for local Docker Weaviate, 'false' for Weaviate Cloud
WEAVIATE_IS_LOCAL=true
# LOCAL_WEAVIATE_PORT=8080
# LOCAL_WEAVIATE_GRPC_PORT=50051

# =============================================================================
# LLM API Keys - Uncomment and add your keys
# =============================================================================
# OpenRouter (recommended - access to multiple models)
# OPENROUTER_API_KEY=

# OpenAI (GPT models)
# OPENAI_API_KEY=

# Anthropic (Claude models)
# ANTHROPIC_API_KEY=

# Other LLM Providers (optional)
# ANYSCALE_API_KEY=
# AWS_ACCESS_KEY=
# AWS_SECRET_KEY=
# COHERE_API_KEY=
# DATABRICKS_TOKEN=
# FRIENDLI_TOKEN=
# VERTEX_API_KEY=
# STUDIO_API_KEY=
# HUGGINGFACE_API_KEY=
# JINAAI_API_KEY=
# MISTRAL_API_KEY=
# NVIDIA_API_KEY=
# AZURE_API_KEY=
# VOYAGE_API_KEY=
# XAI_API_KEY=

# =============================================================================
# Model Configuration
# =============================================================================
# BASE_MODEL=gemini-2.0-flash-001
# COMPLEX_MODEL=gemini-2.0-flash-001
# BASE_PROVIDER=openrouter/google
# COMPLEX_PROVIDER=openrouter/google

# =============================================================================
# Application Settings
# =============================================================================
# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOGGING_LEVEL=INFO

# Enable/disable feedback collection
USE_FEEDBACK=False

# First start flag (automatically set)
FIRST_START_ELYSIA=0

# =============================================================================
# MCP (Model Context Protocol) Configuration
# =============================================================================
# MCP_AS_AGENT: Control MCP tool behavior
# True:  Each MCP server becomes ONE agent tool that takes natural language queries
#        Uses ReAct agent with langchain_mcp_adapters to execute tasks
# False: Each tool in the MCP server becomes an individual Elysia Tool
#        Tools are discovered and wrapped individually at startup
MCP_AS_AGENT=True

# =============================================================================
# Optional: Local LLM Configuration (Ollama)
# =============================================================================
# When using local models with Ollama
# OLLAMA_BASE_URL=http://ollama:11434
# OLLAMA_MODEL=llama2